{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResumeRecommendationReview\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design:\n",
    "- Peer Review:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ResumeRecommendationReview system is a comprehensive solution designed to simplify and enhance the job application process for individuals seeking corporate positions. The system is divided into two main components, each tailored to address key challenges faced by job seekers:\n",
    "\n",
    "1) Company Recommendation\n",
    "Using advanced matching algorithms, the system analyzes a user’s uploaded resume and compares it with job postings on LinkedIn. Based on this analysis, it identifies and recommends companies that align closely with the candidate’s qualifications, skills, and career aspirations.\n",
    "\n",
    "2) Resume Evaluation and Enhancement\n",
    "For the recommended companies, the system conducts a detailed evaluation of the user’s resume. It highlights strengths, identifies areas for improvement, and provides actionable suggestions for tailoring the resume to better fit the expectations of target roles. This ensures candidates can present their qualifications in the most impactful way possible.\n",
    "\n",
    "By integrating these two components, the ResumeRecommendationReview system streamlines the job application journey, empowering users to:\n",
    "\n",
    "- Discover job opportunities that best match their unique profile.\n",
    "- Optimize their resumes for maximum impact, increasing their chances of securing interviews and job offers.\n",
    "\n",
    "**Key Features**:\n",
    "\n",
    "- **CV/Resume Upload**: \n",
    "  Users begin by uploading their existing CV or resume in a supported file format (e.g., PDF)\n",
    "  The system extracts relevant keywords, experiences, and skill sets to build a user profile.\n",
    "\n",
    "- **Job Matching with LinkedIn Postings**: \n",
    "  The platform automatically scans LinkedIn job listings (and potentially other job boards) for roles that align with the user’s skill set and career interests.\n",
    "  A matching algorithm ranks and recommends a list of the most relevant companies and positions for the candidate to consider.\n",
    "\n",
    "- **Comparison & Evaluation (LLM-as-a-Judge)**  \n",
    "  The system leverages a Large Language Model (LLM) to analyze the uploaded resume and specific job requirements. \n",
    "  It evaluates the alignment between the user's experience and the job description, identifying strengths,  skill gaps, and areas in need of improvement.\n",
    "  Additionally, the system evaluates the recommendation performance using **cosine similarity** to measure the semantic alignment and **NDCG (Normalized Discounted Cumulative Gain)** to assess the ranking quality of the recommendations.\n",
    "  \n",
    "- **Automated Resume Enhancement**: \n",
    "  Based on the LLM evaluation, the system provides a detailed report highlighting sections that need modification.\n",
    "  Suggested edits may include restructuring experience points, emphasizing relevant skills, or adding keywords that match the job posting’s expectations.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Data Preparation and Preprocessing](#data-preparation-and-preprocessing)\n",
    "- [Setting Up ChromaDB and Storing Data](#Setting-Up-ChromaDB-and-Storing-Data)\n",
    "- [Company Recommendation System](#Company-Recommendation-System)\n",
    "- [LLM-Based Resume Evaluation System](#LLM-Based-Resume-Evaluation-System)\n",
    "- [LLM-Based Resume Revise System](#LLM-Based-Resume-Revise-System)\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"chromadb\",\n",
    "        \"langchain_chroma\",\n",
    "        \"langchain_openai\",\n",
    "        \"PyMuPDF\",\n",
    "        \"pydantic\",\n",
    "        \"pandas\",\n",
    "        \"kagglehub\",\n",
    "        \"langchain_community\",\n",
    "        \"numpy\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"ResumeRecommendationReview\",\n",
    "        \"UPSTAGE_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Preprocessing\n",
    "\n",
    "This section covers the data preparation and preprocessing steps required for the Resume Recommendation System. The key stages include:\n",
    "\n",
    "- **Processing resume data (PDF)**  \n",
    "- **Processing LinkedIn job postings**  \n",
    "\n",
    "For the LinkedIn job postings data, this tutorial uses the dataset available on Kaggle: [arshkon/linkedin-job-postings](https://www.kaggle.com/arshkon/linkedin-job-postings).  \n",
    "\n",
    "Using the raw data directly to build the recommendation system may lead to suboptimal performance. Therefore, the data is refined and preprocessed to focus specifically on recruitment-related information to enhance the accuracy and relevance of the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List\n",
    "import kagglehub\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Splitting Configuration\n",
    "\n",
    "Set up configurations to divide the extracted text into manageable sizes, ensuring smooth processing:\n",
    "\n",
    "Parameter Descriptions:\n",
    "- `chunk_size`: The maximum length of each text chunk, ensuring the text is divided into manageable sections.\n",
    "- `chunk_overlap`: The length of overlapping text between chunks, providing continuity and context for downstream tasks.\n",
    "- `separators`: The delimiters used to split the text, such as line breaks or punctuation, to optimize the splitting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Pydantic Model\n",
    "\n",
    "In this section, we define a structured data model using Pydantic, which ensures validation and consistency in the data extracted from resumes. This model is critical for organizing key sections of a resume into a format that the system can analyze effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pydantic model\n",
    "class ResumeSection(BaseModel):\n",
    "    skills: List[str] = Field(description=\"List of job-related technical skills\")\n",
    "    work_experience: List[Dict[str, str]] = Field(\n",
    "        description=\"Work experience (role, description)\"\n",
    "    )\n",
    "    projects: List[Dict[str, str]] = Field(\n",
    "        description=\"Project experience (name, description)\"\n",
    "    )\n",
    "    achievements: List[str] = Field(\n",
    "        description=\"List of major achievements and accomplishments\"\n",
    "    )\n",
    "    education: List[Dict[str, str]] = Field(\n",
    "        description=\"Education information (name, description)\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Configure the PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=ResumeSection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Interests in Resumes\n",
    "\n",
    "The `analyze_interests` function is designed to extract and summarize the key areas of interest and research focus from a resume. It uses a **Large Language Model (LLM)** to process the resume text and provide a concise summary, helping to identify the candidate's academic and professional interests effectively.\n",
    "\n",
    "Purpose\n",
    "- Extracts **main areas of interest** and **research focus** from the provided resume text.\n",
    "- Generates a **brief summary** (2-3 sentences) that highlights the candidate's academic and career patterns.\n",
    "- Focuses solely on interests and research areas to provide targeted insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "\n",
    "def analyze_interests(resume_text: str, llm) -> str:\n",
    "    \"\"\"Analyzes the complete resume text to identify key interest areas.\"\"\"\n",
    "    interest_prompt = \"\"\"Analysis this resume text and provide a brief summary (2-3 sentences) \n",
    "    of the person's main areas of interest and research focus. Focus on their academic interests, \n",
    "    research topics, and career patterns.\n",
    "\n",
    "    Resume Text:\n",
    "    {text}\n",
    "\n",
    "    Provide a concise summary focusing ONLY on their interests and research areas.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": interest_prompt.format(text=resume_text)}]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Career Fit in Resumes\n",
    "\n",
    "The `analyze_career_fit` function evaluates a candidate's resume to recommend the most suitable job roles along with their respective fit scores. By leveraging a **Large Language Model (LLM)**, this function identifies key areas of expertise and rates the candidate's suitability for various technical positions.\n",
    "\n",
    "Purpose\n",
    "- Recommends **job roles** based on the candidate's skills, research background, and career trajectory.\n",
    "- Assigns a **fit score** (0.0 to 1.0) for each role, reflecting the candidate's alignment with the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_career_fit(resume_text: str, llm) -> Dict[str, float]:\n",
    "    \"\"\"Analyzes the resume to recommend suitable job roles and their fit scores.\"\"\"\n",
    "    career_prompt = \"\"\"You are an expert technical recruiter. Analyze this resume and recommend the most suitable job roles.\n",
    "    Focus on the candidate's expertise, research background, and career trajectory.\n",
    "    \n",
    "    Resume Text:\n",
    "    {text}\n",
    "    \n",
    "    Based on their background, rate the candidate's fit (0.0 to 1.0) for different technical roles.\n",
    "    Consider:\n",
    "    - Technical expertise and depth\n",
    "    - Research contributions\n",
    "    - Project complexity\n",
    "    - Educational background\n",
    "    - Career progression\n",
    "    \n",
    "    Return ONLY a JSON object with role-fit pairs, like:\n",
    "    {{\"Research Scientist\": 0.95, \"Machine Learning Engineer\": 0.9, \"Algorithm Engineer\": 0.85}}\n",
    "    \n",
    "    Include only roles with fit score > 0.7. Focus on senior/research level positions if appropriate.\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": career_prompt.format(text=resume_text)}]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    try:\n",
    "        return json.loads(response.content.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Resumes to Extract Key Job-Related Information\n",
    "\n",
    "The `process_resume` function analyzes a resume file, extracting and processing key information relevant to job applications. It combines **text extraction**, **interest analysis**, and **career fit evaluation** to generate structured, weighted insights from the resume.\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "Purpose\n",
    "- Extract **key job-related information** from resumes in PDF format.\n",
    "- Use **LLM analysis** to evaluate the candidate's skills, experience, projects, achievements, and education.\n",
    "- Assign **weights** to each section based on relevance to the target job role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resume(file_path, target_job_title=None):\n",
    "    \"\"\"Analyze a resume to extract key job-related information.\"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    resume_text = \"\"\n",
    "    for page in doc:\n",
    "        resume_text += page.get_text()\n",
    "\n",
    "    # Get interest summary and career fit analysis\n",
    "    interest_summary = analyze_interests(resume_text, llm)\n",
    "    career_fit = analyze_career_fit(resume_text, llm)\n",
    "\n",
    "    prompt_template = \"\"\"You are a professional resume analyst specializing in research and technical roles.\n",
    "    Analyze the resume in detail, focusing on the candidate's expertise level and research background.\n",
    "    \n",
    "    Target Job Title: {target_job_title}\n",
    "    \n",
    "    Resume Content:\n",
    "    {resume_text}\n",
    "    \n",
    "    Extract the information in the following format:\n",
    "    {format_instructions}\n",
    "    \n",
    "    Focus on extracting information most relevant to research and technical roles.\n",
    "    Pay special attention to:\n",
    "    - Research contributions and impact\n",
    "    - Technical depth in each area\n",
    "    - Project complexity and leadership\n",
    "    - Academic achievements and specializations\"\"\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    # Format the messages\n",
    "    messages = prompt.format_messages(\n",
    "        target_job_title=target_job_title if target_job_title else \"Not specified\",\n",
    "        resume_text=resume_text,\n",
    "        format_instructions=parser.get_format_instructions(),\n",
    "    )\n",
    "\n",
    "    # Perform LLM analysis\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    try:\n",
    "        parsed_sections = parser.parse(response.content)\n",
    "        print(\"Resume analysis completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "        print(f\"LLM response: {response.content}\")\n",
    "        return []\n",
    "\n",
    "    # Apply weights based on job relevance\n",
    "    weighted_content = []\n",
    "\n",
    "    # Skills (Weight: 0.25)\n",
    "    if parsed_sections.skills:\n",
    "        skills_text = \" \".join(parsed_sections.skills)\n",
    "        weighted_content.append((skills_text, 0.25))\n",
    "\n",
    "    # Work Experience (Weight: 0.3)\n",
    "    if parsed_sections.work_experience:\n",
    "        experience_text = \"\\n\".join(\n",
    "            [\n",
    "                f\"{exp.get('role', '')}: {exp.get('description', '')}\"\n",
    "                for exp in parsed_sections.work_experience\n",
    "            ]\n",
    "        )\n",
    "        weighted_content.append((experience_text, 0.3))\n",
    "\n",
    "    # Projects (Weight: 0.2)\n",
    "    if parsed_sections.projects:\n",
    "        projects_text = \"\\n\".join(\n",
    "            [\n",
    "                f\"{proj.get('name', '')}: {proj.get('description', '')}\"\n",
    "                for proj in parsed_sections.projects\n",
    "            ]\n",
    "        )\n",
    "        weighted_content.append((projects_text, 0.2))\n",
    "\n",
    "    # Achievements (Weight: 0.1)\n",
    "    if parsed_sections.achievements:\n",
    "        achievements_text = \" \".join(parsed_sections.achievements)\n",
    "        weighted_content.append((achievements_text, 0.1))\n",
    "\n",
    "    # Education (Weight: 0.05)\n",
    "    if parsed_sections.education:\n",
    "        education_text = \"\\n\".join(\n",
    "            [\n",
    "                f\"{edu.get('name', '')}: {edu.get('description', '')}\"\n",
    "                for edu in parsed_sections.education\n",
    "            ]\n",
    "        )\n",
    "        weighted_content.append((education_text, 0.05))\n",
    "\n",
    "    # Add interest summary and career fit (combined weight: 0.1)\n",
    "    if interest_summary or career_fit:\n",
    "        analysis_text = (\n",
    "            \"Research Interests and Focus Areas: \" + interest_summary + \"\\n\\n\"\n",
    "        )\n",
    "        if career_fit:\n",
    "            analysis_text += \"Recommended Roles:\\n\"\n",
    "            for role, score in sorted(\n",
    "                career_fit.items(), key=lambda x: x[1], reverse=True\n",
    "            ):\n",
    "                analysis_text += f\"- {role}: {score:.2f}\\n\"\n",
    "\n",
    "        weighted_content.append((analysis_text, 0.1))\n",
    "\n",
    "        # Adjust other weights to maintain total of 1.0\n",
    "        weighted_content = [\n",
    "            (content, weight * 0.9) for content, weight in weighted_content[:-1]\n",
    "        ] + [weighted_content[-1]]\n",
    "\n",
    "    # Generate chunks for each section\n",
    "    processed_chunks = []\n",
    "    for content, weight in weighted_content:\n",
    "        if content.strip():  # Process only non-empty strings\n",
    "            chunks = text_splitter.split_text(content)\n",
    "            processed_chunks.extend([(chunk, weight) for chunk in chunks])\n",
    "\n",
    "    print(f\"Number of extracted chunks: {len(processed_chunks)}\")\n",
    "    print(\"\\nCareer Analysis Summary:\")\n",
    "    print(\"------------------------\")\n",
    "    print(\"Interests:\", interest_summary)\n",
    "    print(\"\\nRecommended Roles:\")\n",
    "    for role, score in sorted(career_fit.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- {role}: {score:.2f}\")\n",
    "\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume Processing Example\n",
    "\n",
    "Here's an example of how to use the `process_resume` function to extract structured data from a resume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume analysis completed.\n",
      "Number of extracted chunks: 8\n",
      "\n",
      "Career Analysis Summary:\n",
      "------------------------\n",
      "Interests: Joanna Drummond's primary academic interests and research focus lie in computer science, particularly in algorithms, artificial intelligence, and game theory. Her research has extensively explored stable matching problems, preference elicitation, and multi-agent systems, with a specific emphasis on developing algorithms for stable and approximately stable matches using partial information and multi-attribute preferences. Additionally, she has investigated the application of machine learning techniques in educational contexts, such as classifying student engagement and understanding in intelligent tutoring systems.\n",
      "\n",
      "Recommended Roles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Python Java Julia R Matlab Unix Shell Scripting (bash) Linux Mac OSX Windows LATEX Weka',\n",
       "  0.225),\n",
       " ('Research Intern: Microsoft Research, with Ian Kash and Peter Key, May 2016 to August 2016. Investigated simple pricing for cloud computing.\\nResearch Assistant: University of Toronto, Department of Computer Science, August 2011 to Present. Investigated Bayes-Nash and ex-post equilibria for matching games with imperfect information, stable and approximately stable matching using multi-attribute preference information, and elicitation schemes using multi-attribute based queries.\\nResearch Assistant: University of Pittsburgh Department of Computer Science, April 2008 to May 2011. Investigated the impact of different training set populations on accurately classifying student uncertainty while using a spoken intelligent physics tutor.\\nDirected Study: University of Pittsburgh Department of Computer Science, September 2010 to December 2010. Analyzed and proved properties about an algorithm for dividing n indivisible objects among 2 people.',\n",
       "  0.27),\n",
       " ('Research Assistant: DREU Program, Information Sciences Institute, University of Southern California, June 2010 to August 2010. Applied HMM’s and decision trees to students’ online forum data to categorize students’ posts.',\n",
       "  0.27),\n",
       " ('Stable Matching Problem Research: Investigated stable and approximately stable matching using multi-attribute preference information and elicitation schemes for the stable matching problem.\\nSpoken Intelligent Physics Tutor: Investigated the impact of different training set populations on accurately classifying student uncertainty and applied decision trees to classifying student zoning out.',\n",
       "  0.18000000000000002),\n",
       " ('Program Committee, CoopMAS 2017 Microsoft Research PhD Fellowship Program Finalist, 2016 Reviewer, Algorithmica, 2015 Reviewer, SAGT 2015 Reviewer, AAAI-15 Ontario Graduate Scholarship, 2014 Reviewer, COMSOC-2014 Microsoft Research Graduate Women’s Scholarship Recipient, 2012 Google Anita Borg Memorial Scholarship Finalist, 2012 Ontario Graduate Scholarship, 2012 Awardee of 2011 NSF Graduate Research Fellowship Program DREU Recipient, Chosen for Distributed Research Experience for Undergraduates Program Best Undergraduate Poster, University of Pittsburgh Department of Computer Science 10th Annual Computer Science Day',\n",
       "  0.09000000000000001),\n",
       " ('PhD Computer Science: University of Toronto, (expected) Spring 2017. Co-advisors: Allan Borodin, Kate Larson. Relevant Courses: Algorithms for Solving Propositional Theories; Intro to Graph Theory; Topics in Knowledge Representation and Reasoning; Advanced Microeconomic Theory I. GPA: 3.83\\nM.S. Computer Science: University of Toronto, Spring 2013. Advisor: Craig Boutilier. Relevant Courses: Decision Making under Uncertainty; Advanced Inference Algorithms; Algorithm Design, Analysis, and Theory. GPA: 3.93',\n",
       "  0.045000000000000005),\n",
       " ('B.S. Computer Science and Mathematics: University of Pittsburgh, December 2010. Research Advisor: Diane Litman. Minor: Theatre Arts. Honors: Graduated Magna Cum Laude with Departmental Honors; Dean’s List, Fall 2006 to Spring 2010; Dean’s Stars List, Spring 2007; Upsilon Pi Epsilon, Member. Relevant Courses: Human Language Technologies; Intro to Artificial Intelligence; Advanced Topics in Artificial Intelligence: Speech and Natural Language Processing for Educational Applications (Graduate Course); Algorithm Design; Machine Learning (Graduate Course); Intro to Theory of Computation. GPA: 3.73',\n",
       "  0.045000000000000005),\n",
       " (\"Research Interests and Focus Areas: Joanna Drummond's primary academic interests and research focus lie in computer science, particularly in algorithms, artificial intelligence, and game theory. Her research has extensively explored stable matching problems, preference elicitation, and multi-agent systems, with a specific emphasis on developing algorithms for stable and approximately stable matches using partial information and multi-attribute preferences. Additionally, she has investigated the application of machine learning techniques in educational contexts, such as classifying student engagement and understanding in intelligent tutoring systems.\",\n",
       "  0.1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_resume(\"../data/joannadrummond-cv.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinkedIn Data Preprocessing\n",
    "\n",
    "This step involves loading job posting data and extracting only the necessary details. The dataset used for this tutorial is sourced from **Kaggle**: [arshkon/linkedin-job-postings](https://www.kaggle.com/arshkon/linkedin-job-postings).\n",
    "\n",
    "- `company_name`: The name of the company offering the job posting.\n",
    "- `title`: The title of the job being offered.\n",
    "- `description`: A detailed description of the job, including responsibilities, qualifications, and expectations.\n",
    "- `max_salary`: The maximum salary offered for the position.\n",
    "- `med_salary`: The median salary for the position, providing an average range for the offered pay.\n",
    "- `min_salary`: The minimum salary offered for the position.\n",
    "- `skills_desc`: A list or summary of the required or preferred skills for the position.\n",
    "- `work_type`: The type of work arrangement, such as full-time, part-time, remote, or hybrid.\n",
    "\n",
    "Purpose of These Columns\n",
    "These selected columns are essential for processing job posting data. They allow the system to:\n",
    "\n",
    "- Extract relevant metadata for recommendation and filtering.\n",
    "- Match resumes to job postings based on skills, and job details.\n",
    "- Provide users with clear and actionable job-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 14: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m path \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marshkon/linkedin-job-postings\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpostings.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m selected_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwork_type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m ]\n\u001b[0;32m     15\u001b[0m linkedin_df \u001b[38;5;241m=\u001b[39m df[selected_columns]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rhkre\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 14: invalid start byte"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"arshkon/linkedin-job-postings\", path=\"postings.csv\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "selected_columns = [\n",
    "    \"company_name\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"max_salary\",\n",
    "    \"med_salary\",\n",
    "    \"min_salary\",\n",
    "    \"skills_desc\",\n",
    "    \"work_type\",\n",
    "]\n",
    "linkedin_df = df[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning Function\n",
    "\n",
    "Here’s a utility function designed to clean and preprocess text data for better consistency and quality:\n",
    "\n",
    "If there are any `null` values in the company name field, those entries are excluded. (While other fields may also have `null` values, this step focuses only on excluding records with `null` in the company name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linkedin_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Remove rows where company_name is empty\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m linkedin_df \u001b[38;5;241m=\u001b[39m \u001b[43mlinkedin_df\u001b[49m\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Alternative using boolean indexing:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# linkedin_df = linkedin_df[linkedin_df['company_name'].notna()]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Clean text data\u001b[39;00m\n\u001b[0;32m     17\u001b[0m linkedin_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linkedin_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linkedin_df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", str(text))\n",
    "    # Remove consecutive whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Remove rows where company_name is empty\n",
    "linkedin_df = linkedin_df.dropna(subset=[\"company_name\"])\n",
    "# Alternative using boolean indexing:\n",
    "# linkedin_df = linkedin_df[linkedin_df['company_name'].notna()]\n",
    "\n",
    "# Clean text data\n",
    "linkedin_df[\"description\"] = linkedin_df[\"description\"].apply(clean_text)\n",
    "linkedin_df[\"skills_desc\"] = linkedin_df[\"skills_desc\"].apply(clean_text)\n",
    "linkedin_df[\"title\"] = linkedin_df[\"title\"].apply(clean_text)\n",
    "\n",
    "# Process salary information\n",
    "for col in [\"max_salary\", \"med_salary\", \"min_salary\"]:\n",
    "    linkedin_df[col] = pd.to_numeric(linkedin_df[col], errors=\"coerce\")\n",
    "\n",
    "# Handle missing values\n",
    "linkedin_df[\"work_type\"] = linkedin_df[\"work_type\"].fillna(\"Not specified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Job Postings Data\n",
    "\n",
    "The `process_job_postings` function integrates and processes job information from a LinkedIn dataset to create structured documents for analysis or recommendation purposes.\n",
    "\n",
    "This function takes a DataFrame of LinkedIn job postings and processes each entry into a standardized format, combining relevant details like company name, job title, required skills, and salary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linkedin_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m job_documents\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m job_documents \u001b[38;5;241m=\u001b[39m process_job_postings(\u001b[43mlinkedin_df\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linkedin_df' is not defined"
     ]
    }
   ],
   "source": [
    "def process_job_postings(linkedin_df):\n",
    "    \"\"\"Process and integrate job information\"\"\"\n",
    "    job_documents = []\n",
    "\n",
    "    # Integrate information for each job\n",
    "    for _, row in linkedin_df.iterrows():\n",
    "        # Format salary information\n",
    "        salary_info = \"No salary information\"\n",
    "        if pd.notna(row[\"min_salary\"]) and pd.notna(row[\"max_salary\"]):\n",
    "            salary_info = f\"{row['min_salary']:,.0f} - {row['max_salary']:,.0f}\"\n",
    "        elif pd.notna(row[\"med_salary\"]):\n",
    "            salary_info = f\"Average {row['med_salary']:,.0f}\"\n",
    "\n",
    "        # Integrate job information\n",
    "        job_text = f\"\"\"\n",
    "        Company: {row['company_name']}\n",
    "        Position: {row['title']}\n",
    "        Work Type: {row['work_type']}\n",
    "        Salary: {salary_info}\n",
    "        \n",
    "        Required Skills:\n",
    "        {row['skills_desc']}\n",
    "        \n",
    "        Job Description:\n",
    "        {row['description']}\n",
    "        \"\"\"\n",
    "\n",
    "        # Store with metadata\n",
    "        job_documents.append(\n",
    "            {\n",
    "                \"content\": job_text,\n",
    "                \"metadata\": {\n",
    "                    \"company\": row[\"company_name\"],\n",
    "                    \"title\": row[\"title\"],\n",
    "                    \"work_type\": row[\"work_type\"],\n",
    "                    \"salary\": salary_info,\n",
    "                    \"skills\": row[\"skills_desc\"],\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return job_documents\n",
    "\n",
    "\n",
    "# Usage example\n",
    "job_documents = process_job_postings(linkedin_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up ChromaDB and Storing Data\n",
    "\n",
    "Using ChromaDB for Storing and Retrieving Resume and Job Posting Data\n",
    "In this section, we will explore how to use ChromaDB to store resume and job posting data as vector representations and perform similarity-based searches.\n",
    "\n",
    "What is `ChromaDB`?\n",
    "\n",
    "`ChromaDB` is a vector database that allows text data to be stored as embeddings, enabling efficient similarity-based searches. In our Resume Recommendation System, ChromaDB is used for the following purposes:\n",
    "\n",
    "- Vectorizing Text: Converting resume and job posting text into vector representations.\n",
    "- Efficient Similarity Search: Performing fast searches based on the similarity of embeddings.\n",
    "- Metadata-Based Search and Filtering: Enhancing search results with filters like job title, or company name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Steps\n",
    "Preparing Required Libraries\n",
    "\n",
    "Before starting, import the necessary libraries:\n",
    "\n",
    "Roles of Each Library:\n",
    "\n",
    "- `langchain_community.vectorstores`: Provides integration with ChromaDB.\n",
    "- `langchain_openai`: Enables the use of OpenAI embedding models.\n",
    "- `chromadb`: Provides vector database functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing ChromaDB\n",
    "\n",
    "Set up ChromaDB and create collections:\n",
    "\n",
    "Why Use PersistentClient?\n",
    "- `Permanent Data Storage`: Ensures that data is not lost when the application or session ends.\n",
    "- `Data Persistence Across Sessions`: Allows the system to retain data for use in future queries without requiring re-upload or re-processing.\n",
    "- `Ease of Backup and Recovery`: Provides a reliable way to save and restore data for robustness and fault tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"../data/chromadb\")\n",
    "\n",
    "# Create separate collections for resumes and job postings\n",
    "resume_collection = client.create_collection(\"resumes\")\n",
    "job_collection = client.create_collection(\"jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Data\n",
    "This step involves saving resume and job posting data into ChromaDB for efficient querying and management.\n",
    "Origin data has too many data, so we use only 500 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume analysis completed.\n",
      "Number of extracted chunks: 8\n",
      "\n",
      "Career Analysis Summary:\n",
      "------------------------\n",
      "Interests: Joanna Drummond's main areas of interest and research focus are in computer science, particularly in algorithms, artificial intelligence, and game theory. Her research has concentrated on stable matching problems, preference elicitation, and decision-making under uncertainty, with applications in multi-agent systems and educational technologies. She has also explored topics related to dialogue systems and student engagement in educational settings.\n",
      "\n",
      "Recommended Roles:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'job_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m resume_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(resume_chunks))]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# origin data has too many data, so we use only 500 data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m job_documents_ \u001b[38;5;241m=\u001b[39m \u001b[43mjob_documents\u001b[49m[:\u001b[38;5;241m500\u001b[39m]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Prepare job posting data (same as before)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m job_texts \u001b[38;5;241m=\u001b[39m [doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m job_documents_]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job_documents' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare resume data\n",
    "resume_file_path = \"../data/joannadrummond-cv.pdf\"  # Path to the resume PDF file\n",
    "resume_chunks = process_resume(\n",
    "    resume_file_path\n",
    ")  # Using the previously defined process_resume function\n",
    "\n",
    "resume_texts = [chunk[0] for chunk in resume_chunks]\n",
    "resume_metadatas = [\n",
    "    {\"source\": \"resume\", \"type\": \"text\", \"weight\": chunk[1]} for chunk in resume_chunks\n",
    "]\n",
    "\n",
    "resume_ids = [f\"resume_chunk_{i}\" for i in range(len(resume_chunks))]\n",
    "\n",
    "# origin data has too many data, so we use only 500 data\n",
    "job_documents_ = job_documents[:500]\n",
    "\n",
    "# Prepare job posting data (same as before)\n",
    "job_texts = [doc[\"content\"] for doc in job_documents_]\n",
    "job_metadatas = [doc[\"metadata\"] for doc in job_documents_]\n",
    "job_ids = [f\"job_{i}\" for i in range(len(job_documents_))]\n",
    "\n",
    "# Generate and store embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Resume embeddings\n",
    "resume_embeddings = embeddings.embed_documents(resume_texts)\n",
    "resume_collection.add(\n",
    "    embeddings=resume_embeddings,\n",
    "    documents=resume_texts,\n",
    "    metadatas=resume_metadatas,\n",
    "    ids=resume_ids,\n",
    ")\n",
    "\n",
    "# Job posting embeddings\n",
    "job_embeddings = embeddings.embed_documents(job_texts)\n",
    "job_collection.add(\n",
    "    embeddings=job_embeddings, documents=job_texts, metadatas=job_metadatas, ids=job_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Job_documents_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'job_documents_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mjob_documents_\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job_documents_' is not defined"
     ]
    }
   ],
   "source": [
    "job_documents_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Recommendation System\n",
    "\n",
    "This section focuses on recommending companies that align with the candidate's resume and evaluates the recommendations using two key metrics:\n",
    "\n",
    "1. **Cosine Similarity for Recommendation Evaluation**:  \n",
    "   - Measures the similarity between the candidate's resume and the job posting.  \n",
    "   - A higher cosine similarity score indicates a stronger match between the candidate's profile and the company's job requirements.\n",
    "\n",
    "2. **NDCG (Normalized Discounted Cumulative Gain) for Recommendation Evaluation**:  \n",
    "   - Assesses the quality of the ranking of recommended companies.  \n",
    "   - A higher NDCG score signifies that the most relevant companies appear at the top of the recommendation list, reflecting better ranking performance.\n",
    "\n",
    "### Understanding the Scores\n",
    "- **High Scores**:  \n",
    "   - Indicate a strong alignment between the resume and the recommended companies (Cosine Similarity).  \n",
    "   - Demonstrate that the ranking system effectively prioritizes the most relevant companies (NDCG).  \n",
    "- **Low Scores**:  \n",
    "   - Suggest weaker matches between the resume and job postings or suboptimal ranking of recommendations.  \n",
    "\n",
    "The goal is to achieve high scores in both metrics, ensuring accurate and effective company recommendations for the candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Job Recommendation System with Weighted Similarity Search\n",
    "\n",
    "This implementation utilizes a **Job Recommendation System** to match resumes with the most relevant job postings. By combining **cosine similarity** and **weighted scoring**, the system ensures accurate and tailored recommendations.\n",
    "\n",
    "\n",
    "---\n",
    "- **Personalized Matching**: Matches resumes to job postings with high accuracy.\n",
    "- **Flexible Scoring**: Incorporates weighted factors to prioritize specific job attributes.\n",
    "- **Enhanced Readability**: Formats job descriptions for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weighted Recommendations ===\n",
      "\n",
      "=== Similar Job Posting Search Results ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "# Retrieve resume and job posting collections\n",
    "resume_collection = client.get_collection(\"resumes\")\n",
    "job_collection = client.get_collection(\"jobs\")\n",
    "\n",
    "# Retrieve stored resume data from ChromaDB\n",
    "resume_results = resume_collection.get(\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")  # Use the get() method to fetch all data\n",
    "\n",
    "# Combine resume texts\n",
    "full_resume_text = \" \".join(resume_results[\"documents\"])\n",
    "\n",
    "# Configure embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Convert the resume text into a query vector\n",
    "query_embedding = embeddings.embed_query(full_resume_text)\n",
    "\n",
    "# Use ChromaDB to search for the top 5 most similar job postings\n",
    "job_results = job_collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    ")\n",
    "\n",
    "# List to store recommended jobs\n",
    "recommended_jobs = []\n",
    "\n",
    "\n",
    "class JobRecommender:\n",
    "    def __init__(self, resume_collection, job_collection):\n",
    "        self.resume_collection = resume_collection\n",
    "        self.job_collection = job_collection\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    def get_resume_text(self) -> str:\n",
    "        \"\"\"Get combined resume text from collection\"\"\"\n",
    "        resume_results = self.resume_collection.get(include=[\"documents\", \"metadatas\"])\n",
    "        return \" \".join(resume_results[\"documents\"])\n",
    "\n",
    "    def get_query_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Convert text to embedding vector\"\"\"\n",
    "        return self.embeddings.embed_query(text)\n",
    "\n",
    "    def weighted_similarity_search(\n",
    "        self, query_embedding: List[float], method: str = \"cosine\", n_results: int = 5\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search jobs using weighted similarity\n",
    "\n",
    "        Args:\n",
    "            query_embedding: The query embedding vector\n",
    "            method: Similarity method ('cosine' or 'distance')\n",
    "            n_results: Number of results to return\n",
    "        \"\"\"\n",
    "        include_params = [\"documents\", \"metadatas\"]\n",
    "        include_params.append(\"embeddings\" if method == \"cosine\" else \"distances\")\n",
    "\n",
    "        results = self.job_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results * 2,  # Get more results for reranking\n",
    "            include=include_params,\n",
    "        )\n",
    "\n",
    "        weighted_results = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            weight = results[\"metadatas\"][0][i].get(\"weight\", 1.0)\n",
    "\n",
    "            if method == \"cosine\":\n",
    "                doc_embedding = results[\"embeddings\"][0][i]\n",
    "                similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "                )\n",
    "            else:  # distance\n",
    "                distance = results[\"distances\"][0][i]\n",
    "                similarity = 1 - distance\n",
    "\n",
    "            weighted_score = similarity * weight\n",
    "            job_desc = self._clean_job_description(results[\"documents\"][0][i])\n",
    "\n",
    "            # Ensure consistent dictionary structure with search_jobs_by_distance\n",
    "            weighted_results.append(\n",
    "                {\n",
    "                    \"company\": results[\"metadatas\"][0][i].get(\"company\", \"Unknown\"),\n",
    "                    \"title\": results[\"metadatas\"][0][i].get(\"title\", \"Unknown\"),\n",
    "                    \"description\": job_desc,\n",
    "                    \"similarity\": weighted_score,  # Use weighted_score as the similarity\n",
    "                    \"metadata\": results[\"metadatas\"][0][i],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Sort by weighted score and get top results\n",
    "        weighted_results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        return weighted_results[:n_results]\n",
    "\n",
    "    def _clean_job_description(self, description: str) -> str:\n",
    "        \"\"\"Clean job description text\"\"\"\n",
    "        return description.strip().replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "    def print_recommendations(self, recommendations: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Print job recommendations and return the results\"\"\"\n",
    "        print(\"\\n=== Similar Job Posting Search Results ===\")  # Changed from Korean\n",
    "        results = []\n",
    "\n",
    "        for i, job in enumerate(recommendations, 1):\n",
    "            print(f\"\\n\\nJob Posting #{i}\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Company: {job['company']}\")\n",
    "            print(f\"Position: {job['title']}\")\n",
    "            print(f\"Similarity Score: {job['similarity']:.2f}\")\n",
    "            print(\"\\n[Job Description]\")\n",
    "\n",
    "            desc_lines = [\n",
    "                line.strip() for line in job[\"description\"].split(\"\\n\") if line.strip()\n",
    "            ]\n",
    "            for line in desc_lines:\n",
    "                print(line)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            # Add results to list  # Changed from Korean\n",
    "            results.append(\n",
    "                {\n",
    "                    \"company\": job[\"company\"],\n",
    "                    \"title\": job[\"title\"],\n",
    "                    \"description\": job[\"description\"],\n",
    "                    \"similarity\": job[\"similarity\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Execution section modification  # Changed from Korean\n",
    "# Initialize collections\n",
    "resume_collection = client.get_collection(\"resumes\")\n",
    "job_collection = client.get_collection(\"jobs\")\n",
    "\n",
    "# Create recommender instance\n",
    "recommender = JobRecommender(resume_collection, job_collection)\n",
    "\n",
    "# Get resume text and create query embedding\n",
    "resume_text = recommender.get_resume_text()\n",
    "query_embedding = recommender.get_query_embedding(resume_text)\n",
    "\n",
    "# Get recommendations using different methods\n",
    "weighted_recommendations = recommender.weighted_similarity_search(\n",
    "    query_embedding, method=\"cosine\"\n",
    ")\n",
    "\n",
    "# Print results and store them\n",
    "print(\"\\n=== Weighted Recommendations ===\")\n",
    "recommended_jobs = recommender.print_recommendations(weighted_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resume and Job Recommendation Evaluation System\n",
    "\n",
    "This implementation introduces a comprehensive evaluation system for job recommendations based on resumes.\n",
    "\n",
    " The system leverages **Discounted Cumulative Gain (DCG)** and **Normalized Discounted Cumulative Gain (NDCG)** to measure the quality of recommendations. Additionally, precision and recall metrics are calculated for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class ResumeProcessor:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "    def process_resume(self, resume_texts: List[str]) -> str:\n",
    "        \"\"\"Process text using already refined resume_texts\"\"\"\n",
    "        return \" \".join(resume_texts)\n",
    "\n",
    "\n",
    "class NDCGEvaluator:\n",
    "    def __init__(self, model_name=\"gpt-4\", temperature=0.2):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "\n",
    "        # Ground truth generation prompt\n",
    "        self.relevance_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "        As an expert recruiter, evaluate the relevance between this resume and job posting.\n",
    "        Consider technical skills, experience level, and overall fit.\n",
    "\n",
    "        Resume:\n",
    "        {resume_text}\n",
    "\n",
    "        Job Posting:\n",
    "        {job_text}\n",
    "\n",
    "        Rate the relevance on a scale of 0 to 1, where:\n",
    "        1.0: Perfect match\n",
    "        0.8: Very good match\n",
    "        0.6: Good match\n",
    "        0.4: Moderate match\n",
    "        0.2: Poor match\n",
    "        0.0: No match\n",
    "\n",
    "        Provide only the numerical score, nothing else.\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    def calculate_dcg(self, relevance_scores: List[float], k: int = None) -> float:\n",
    "        \"\"\"Calculate Discounted Cumulative Gain\n",
    "        Formula: DCG = sum(rel_i / log2(i + 2)) where rel_i is the relevance of item i\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = len(relevance_scores)\n",
    "        else:\n",
    "            k = min(k, len(relevance_scores))\n",
    "\n",
    "        dcg = 0.0\n",
    "        for i in range(k):\n",
    "            # 2^rel - 1 is commonly used for NDCG calculation to emphasize relevant items\n",
    "            rel = 2 ** relevance_scores[i] - 1\n",
    "            dcg += rel / math.log2(i + 2)\n",
    "        return dcg\n",
    "\n",
    "    def calculate_ndcg(\n",
    "        self, predicted_scores: List[float], ideal_scores: List[float], k: int = None\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate Normalized Discounted Cumulative Gain\n",
    "        NDCG = DCG / IDCG where IDCG is DCG of ideal ordering\n",
    "        \"\"\"\n",
    "        if not predicted_scores or not ideal_scores:\n",
    "            return 0.0\n",
    "\n",
    "        if k is None:\n",
    "            k = len(predicted_scores)\n",
    "        k = min(k, len(predicted_scores))\n",
    "\n",
    "        # Sort ideal scores in descending order\n",
    "        ideal_scores_sorted = sorted(ideal_scores, reverse=True)\n",
    "\n",
    "        dcg = self.calculate_dcg(predicted_scores[:k], k)\n",
    "        idcg = self.calculate_dcg(ideal_scores_sorted[:k], k)\n",
    "\n",
    "        if idcg == 0:\n",
    "            return 0.0\n",
    "\n",
    "        ndcg = dcg / idcg\n",
    "        # Ensure NDCG is between 0 and 1\n",
    "        return max(0.0, min(1.0, ndcg))\n",
    "\n",
    "    def generate_ground_truth(\n",
    "        self, resume_text: str, job_postings: List[Dict]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Generate ground truth relevance scores using LLM\"\"\"\n",
    "        ground_truth = {}\n",
    "\n",
    "        for job in job_postings:\n",
    "            if job[\"company\"] == \"Unknown\":\n",
    "                continue\n",
    "\n",
    "            messages = self.relevance_prompt.format_messages(\n",
    "                resume_text=resume_text, job_text=job[\"description\"]\n",
    "            )\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "            try:\n",
    "                score = float(response.content.strip())\n",
    "                ground_truth[job[\"company\"]] = score\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing score for company {job['company']}\")\n",
    "                ground_truth[job[\"company\"]] = 0.0\n",
    "\n",
    "        return ground_truth\n",
    "\n",
    "    def normalize_scores(self, scores: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize scores to 0-1 range\"\"\"\n",
    "        if not scores:\n",
    "            return scores\n",
    "\n",
    "        min_score = min(scores)\n",
    "        max_score = max(scores)\n",
    "\n",
    "        if max_score == min_score:\n",
    "            return [1.0 for _ in scores]\n",
    "\n",
    "        return [(score - min_score) / (max_score - min_score) for score in scores]\n",
    "\n",
    "    def evaluate_recommendations(\n",
    "        self, resume_text: str, recommended_jobs: List[Dict], k: int = None\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate recommendations using NDCG\"\"\"\n",
    "        # Filter out Unknown companies\n",
    "        valid_jobs = [job for job in recommended_jobs if job[\"company\"] != \"Unknown\"]\n",
    "\n",
    "        # Generate ground truth scores\n",
    "        ground_truth = self.generate_ground_truth(resume_text, valid_jobs)\n",
    "\n",
    "        # Get predicted scores and normalize them\n",
    "        predicted_scores = [job[\"similarity\"] for job in valid_jobs]\n",
    "        predicted_scores = self.normalize_scores(predicted_scores)\n",
    "\n",
    "        # Get ideal scores in the same order as predictions\n",
    "        ideal_scores = [ground_truth[job[\"company\"]] for job in valid_jobs]\n",
    "\n",
    "        # Calculate NDCG\n",
    "        ndcg_score = self.calculate_ndcg(predicted_scores, ideal_scores, k)\n",
    "\n",
    "        # Additional metrics\n",
    "        if k is None:\n",
    "            k = len(valid_jobs)\n",
    "\n",
    "        # Calculate precision and recall using threshold of 0.6 for relevance\n",
    "        relevant_recommended = sum(1 for score in ideal_scores[:k] if score >= 0.6)\n",
    "        total_relevant = sum(1 for score in ground_truth.values() if score >= 0.6)\n",
    "\n",
    "        precision_at_k = relevant_recommended / k if k > 0 else 0\n",
    "        recall_at_k = relevant_recommended / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "        return {\n",
    "            \"ndcg\": ndcg_score,\n",
    "            \"precision@k\": precision_at_k,\n",
    "            \"recall@k\": recall_at_k,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"normalized_predictions\": dict(\n",
    "                zip([job[\"company\"] for job in valid_jobs], predicted_scores)\n",
    "            ),\n",
    "        }\n",
    "\n",
    "\n",
    "def print_evaluation_results(metrics: Dict[str, float], recommended_jobs: List[Dict]):\n",
    "    \"\"\"Print detailed evaluation results\"\"\"\n",
    "    print(\"\\n=== Recommendation Evaluation Results ===\")\n",
    "    print(f\"NDCG Score: {metrics['ndcg']:.3f}\")\n",
    "    print(f\"Precision@k: {metrics['precision@k']:.3f}\")\n",
    "    print(f\"Recall@k: {metrics['recall@k']:.3f}\")\n",
    "\n",
    "    print(\"\\nDetailed Company Scores:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Company':<30} {'Original':<10} {'Normalized':<10} {'Ground Truth':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for job in recommended_jobs:\n",
    "        company = job[\"company\"]\n",
    "        if company == \"Unknown\":\n",
    "            continue\n",
    "\n",
    "        original = job[\"similarity\"]\n",
    "        normalized = metrics[\"normalized_predictions\"].get(company, 0.0)\n",
    "        ground_truth = metrics[\"ground_truth\"].get(company, 0.0)\n",
    "        print(\n",
    "            f\"{company:<30} {original:<10.3f} {normalized:<10.3f} {ground_truth:<10.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recommendation Evaluation Results ===\n",
      "NDCG Score: 0.000\n",
      "Precision@k: 0.000\n",
      "Recall@k: 0.000\n",
      "\n",
      "Detailed Company Scores:\n",
      "================================================================================\n",
      "Company                        Original   Normalized Ground Truth\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Use existing processed resume_texts\n",
    "resume_processor = ResumeProcessor()\n",
    "resume_text = resume_processor.process_resume(resume_texts)\n",
    "\n",
    "evaluator = NDCGEvaluator()\n",
    "metrics = evaluator.evaluate_recommendations(\n",
    "    resume_text=resume_text,\n",
    "    recommended_jobs=recommended_jobs,\n",
    "    k=5,  # Evaluate top-5 recommendations\n",
    ")\n",
    "\n",
    "# Print evaluation results\n",
    "print_evaluation_results(metrics, recommended_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-Based Resume Evaluation System\n",
    "\n",
    "In this section, we implement a system that utilizes a `Large Language Model (LLM)` to compare and analyze resumes against job requirements.\n",
    "\n",
    "---\n",
    "\n",
    "What is `LLM-as-a-Judge`?\n",
    "\n",
    "The `LLM-as-a-Judge` system leverages the advanced reasoning and natural language understanding capabilities of an LLM to serve as an impartial evaluator in the hiring process. By acting as a \"judge,\" the LLM compares a candidate’s resume to job requirements, evaluates their alignment, and provides actionable feedback.\n",
    "\n",
    "Key features of the `LLM-as-a-Judge` system include:\n",
    "- `Contextual Understanding`: It comprehends detailed job descriptions and resumes beyond simple keyword matching, enabling nuanced evaluations.  \n",
    "- `Feedback Generation`: Provides insights into the candidate's strengths and areas for improvement.  \n",
    "- `Decision Support`: Assists hiring managers or applicants by generating a recommendation on the candidate's suitability for the role.\n",
    "\n",
    "This system bridges the gap between human evaluation and automated analysis, ensuring more accurate and tailored results in the recruitment process.\n",
    "\n",
    "---\n",
    "\n",
    "Functionalities\n",
    "\n",
    "The `LLM-as-a-Judge` system provides the following functionalities:\n",
    "\n",
    "- `Detailed Analysis`: Analyzes resumes and job requirements in detail, identifying key qualifications and expectations.  \n",
    "- `Alignment Evaluation`: Assesses how well the candidate's skills and experiences match the job requirements.  \n",
    "- `Strengths and Improvement Areas`: Identifies the candidate's strengths and offers suggestions for improvement.  \n",
    "- `Role Suitability Recommendation`: Provides a final recommendation on whether the candidate is a good fit for the role.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-Based Resume Evaluation System\n",
    "\n",
    "This system leverages a **Large Language Model (LLM)** to evaluate resumes against job descriptions systematically. It provides detailed feedback based on predefined evaluation criteria, helping candidates understand their strengths, areas for improvement, and overall suitability for specific roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define Pydantic Models\n",
    "class CriterionEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation result for individual criteria\"\"\"\n",
    "\n",
    "    score: int = Field(description=\"Evaluation score (1-5)\")\n",
    "    reasoning: str = Field(description=\"Reasoning behind the score\")\n",
    "    evidence: List[str] = Field(description=\"Evidence found in the resume\")\n",
    "    suggestions: List[str] = Field(description=\"Suggestions for improvement\")\n",
    "\n",
    "\n",
    "class DetailedEvaluation(BaseModel):\n",
    "    \"\"\"Detailed evaluation results\"\"\"\n",
    "\n",
    "    technical_fit: CriterionEvaluation\n",
    "    experience_relevance: CriterionEvaluation\n",
    "    industry_knowledge: CriterionEvaluation\n",
    "    education_qualification: CriterionEvaluation\n",
    "    soft_skills: CriterionEvaluation\n",
    "    overall_score: int = Field(description=\"Overall score (0-100)\")\n",
    "    key_strengths: List[str] = Field(description=\"Key strengths\")\n",
    "    improvement_areas: List[str] = Field(description=\"Areas for improvement\")\n",
    "    final_recommendation: str = Field(description=\"Final recommendation\")\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, model_name=\"gpt-4o\", temperature=0.1):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=DetailedEvaluation)\n",
    "\n",
    "        # Define evaluation criteria\n",
    "        self.evaluation_criteria = {\n",
    "            \"technical_fit\": {\n",
    "                \"weight\": 30,\n",
    "                \"description\": \"Evaluation of technical fit\",\n",
    "                \"subcriteria\": [\n",
    "                    \"required_skills_match\",\n",
    "                    \"tech_stack_relevance\",\n",
    "                    \"skill_proficiency\",\n",
    "                ],\n",
    "            },\n",
    "            \"experience_relevance\": {\n",
    "                \"weight\": 25,\n",
    "                \"description\": \"Evaluation of experience relevance\",\n",
    "                \"subcriteria\": [\"role_similarity\", \"impact_scale\", \"problem_solving\"],\n",
    "            },\n",
    "            \"industry_knowledge\": {\n",
    "                \"weight\": 15,\n",
    "                \"description\": \"Evaluation of industry knowledge\",\n",
    "                \"subcriteria\": [\n",
    "                    \"domain_expertise\",\n",
    "                    \"trend_awareness\",\n",
    "                    \"industry_exposure\",\n",
    "                ],\n",
    "            },\n",
    "            \"education_qualification\": {\n",
    "                \"weight\": 15,\n",
    "                \"description\": \"Evaluation of education and qualifications\",\n",
    "                \"subcriteria\": [\n",
    "                    \"degree_relevance\",\n",
    "                    \"certifications\",\n",
    "                    \"continuous_learning\",\n",
    "                ],\n",
    "            },\n",
    "            \"soft_skills\": {\n",
    "                \"weight\": 15,\n",
    "                \"description\": \"Evaluation of soft skills\",\n",
    "                \"subcriteria\": [\n",
    "                    \"leadership_teamwork\",\n",
    "                    \"communication\",\n",
    "                    \"problem_approach\",\n",
    "                ],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Evaluation prompt template\n",
    "        self.prompt_template = \"\"\"You are a professional hiring evaluator.\n",
    "        Evaluate the provided resume objectively and fairly based on the following criteria.\n",
    "\n",
    "        Job Information:\n",
    "        Company: {company_name}\n",
    "        Position: {position}\n",
    "        Job Description: {job_description}\n",
    "\n",
    "        Resume Content:\n",
    "        {resume_text}\n",
    "\n",
    "        Evaluation Criteria:\n",
    "        {evaluation_criteria}\n",
    "\n",
    "        Guidelines for Evaluation:\n",
    "        1. Assign a score from 1-5 for each evaluation area and provide detailed reasoning.\n",
    "        2. Scoring criteria:\n",
    "           5: Outstanding - Exceeds expectations significantly\n",
    "           4: Excellent - Meets and slightly exceeds expectations\n",
    "           3: Adequate - Meets expectations\n",
    "           2: Needs Improvement - Falls slightly short of expectations\n",
    "           1: Poor - Falls significantly short of expectations\n",
    "        3. Provide specific evidence found in the resume for each area.\n",
    "        4. Offer concrete suggestions for improvement.\n",
    "\n",
    "        Provide the evaluation results in the following format:\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            template=self.prompt_template,\n",
    "            partial_variables={\n",
    "                \"format_instructions\": self.parser.get_format_instructions(),\n",
    "                \"evaluation_criteria\": json.dumps(\n",
    "                    self.evaluation_criteria, indent=2, ensure_ascii=False\n",
    "                ),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def evaluate(self, resume_text: str, job_info: dict) -> DetailedEvaluation:\n",
    "        \"\"\"Perform resume evaluation\"\"\"\n",
    "        try:\n",
    "            messages = self.prompt.format_messages(\n",
    "                company_name=job_info.get(\"company\", \"Unknown\"),\n",
    "                position=job_info.get(\"position\", \"Unknown\"),\n",
    "                job_description=job_info.get(\"description\", \"\"),\n",
    "                resume_text=resume_text,\n",
    "            )\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "            evaluation = self.parser.parse(response.content)\n",
    "            return evaluation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class ResumeEvaluationSystem:\n",
    "    def __init__(self):\n",
    "        self.resume_processor = ResumeProcessor()\n",
    "        self.judge = LLMJudge()\n",
    "\n",
    "    def evaluate_with_recommendations(\n",
    "        self, resume_path: str, recommended_jobs: List[dict], top_n: int = 3\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Evaluate the resume for the recommended jobs\"\"\"\n",
    "        # Extract resume text\n",
    "        resume_text = self.resume_processor.process_resume(resume_path)\n",
    "\n",
    "        # Select top N jobs\n",
    "        sorted_jobs = sorted(\n",
    "            recommended_jobs, key=lambda x: x[\"similarity\"], reverse=True\n",
    "        )[:top_n]\n",
    "        evaluations = []\n",
    "\n",
    "        for job in sorted_jobs:\n",
    "            job_info = {\n",
    "                \"company\": job[\"company\"],\n",
    "                \"position\": job[\"title\"],\n",
    "                \"description\": job[\"description\"],\n",
    "                \"similarity_score\": job[\"similarity\"],\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Perform evaluation\n",
    "                evaluation = self.judge.evaluate(resume_text, job_info)\n",
    "\n",
    "                # Generate evaluation report\n",
    "                report = format_evaluation_report(evaluation)\n",
    "\n",
    "                evaluations.append(\n",
    "                    {\"job_info\": job_info, \"evaluation\": evaluation, \"report\": report}\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating for {job_info['company']}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return evaluations\n",
    "\n",
    "\n",
    "def format_evaluation_report(evaluation: DetailedEvaluation) -> str:\n",
    "    \"\"\"Format evaluation results into a report\"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\n📊 Resume Evaluation Report\")\n",
    "    output.append(\"=\" * 50)\n",
    "\n",
    "    output.append(f\"\\n💡 Overall Score: {evaluation.overall_score}/100\\n\")\n",
    "\n",
    "    # Evaluation by criteria\n",
    "    criteria_items = [\n",
    "        (\"🔧 Technical Fit (30%)\", evaluation.technical_fit),\n",
    "        (\"👔 Experience Relevance (25%)\", evaluation.experience_relevance),\n",
    "        (\"🎯 Industry Knowledge (15%)\", evaluation.industry_knowledge),\n",
    "        (\"📚 Education Qualification (15%)\", evaluation.education_qualification),\n",
    "        (\"🤝 Soft Skills (15%)\", evaluation.soft_skills),\n",
    "    ]\n",
    "\n",
    "    for title, criterion in criteria_items:\n",
    "        output.append(f\"\\n{title}\")\n",
    "        output.append(f\"Score: {criterion.score}/5\")\n",
    "        output.append(f\"Reasoning: {criterion.reasoning}\")\n",
    "        output.append(\"Evidence Found:\")\n",
    "        for evidence in criterion.evidence:\n",
    "            output.append(f\"  • {evidence}\")\n",
    "        output.append(\"Suggestions:\")\n",
    "        for suggestion in criterion.suggestions:\n",
    "            output.append(f\"  • {suggestion}\")\n",
    "\n",
    "    # Overall evaluation\n",
    "    output.append(\"\\n📋 Overall Evaluation\")\n",
    "    output.append(\"-\" * 30)\n",
    "\n",
    "    output.append(\"\\n💪 Key Strengths:\")\n",
    "    for strength in evaluation.key_strengths:\n",
    "        output.append(f\"  • {strength}\")\n",
    "\n",
    "    output.append(\"\\n📈 Areas for Improvement:\")\n",
    "    for area in evaluation.improvement_areas:\n",
    "        output.append(f\"  • {area}\")\n",
    "\n",
    "    output.append(\"\\n🎯 Final Recommendation:\")\n",
    "    output.append(f\"{evaluation.final_recommendation}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "def print_comprehensive_report(evaluations: List[Dict]):\n",
    "    \"\"\"Display the complete evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📋 Comprehensive Resume Evaluation Report\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for idx, eval_result in enumerate(evaluations, 1):\n",
    "        job_info = eval_result[\"job_info\"]\n",
    "        evaluation = eval_result[\"evaluation\"]\n",
    "\n",
    "        print(f\"\\n{idx}. {job_info['company']} - {job_info['position']}\")\n",
    "        print(f\"Recommendation Similarity Score: {job_info['similarity_score']:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(eval_result[\"report\"])\n",
    "        print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume analysis completed.\n",
      "Number of extracted chunks: 6\n",
      "\n",
      "Career Analysis Summary:\n",
      "------------------------\n",
      "Interests: Joanna Drummond's primary academic interests and research focus lie in computer science, particularly in algorithms, artificial intelligence, and game theory. Her research has extensively explored stable matching problems, preference elicitation, and multi-agent systems, with a strong emphasis on decision-making under uncertainty and the application of machine learning techniques to educational technologies. Her career pattern reflects a consistent engagement with theoretical and applied aspects of computer science, evidenced by her work on stable matchings and dialogue systems, as well as her involvement in teaching and research internships.\n",
      "\n",
      "Recommended Roles:\n",
      "Evaluating resume...\n",
      "\n",
      "================================================================================\n",
      "📋 Comprehensive Resume Evaluation Report\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Resume file path\n",
    "resume_path = \"../data/joannadrummond-cv.pdf\"\n",
    "\n",
    "# Initialize evaluation system\n",
    "evaluation_system = ResumeEvaluationSystem()\n",
    "\n",
    "# First, get the resume text\n",
    "resume_chunks = process_resume(resume_path)\n",
    "resume_text = \" \".join([chunk[0] for chunk in resume_chunks])\n",
    "\n",
    "# Perform resume evaluation\n",
    "print(\"Evaluating resume...\")\n",
    "evaluations = evaluation_system.evaluate_with_recommendations(\n",
    "    resume_text,  # Pass the actual resume text instead of the path\n",
    "    recommended_jobs=recommended_jobs,\n",
    "    top_n=1,\n",
    ")\n",
    "\n",
    "# Print comprehensive report\n",
    "print_comprehensive_report(evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-Based Resume Revise System\n",
    "\n",
    "This tutorial demonstrates how to create a system that evaluates and improves resumes using a **Large Language Model (LLM)**. \n",
    "\n",
    "The system provides actionable suggestions to optimize resumes for specific job descriptions, enhancing the candidate’s chances of securing a role.\n",
    "\n",
    "---\n",
    "\n",
    "Key Components\n",
    "\n",
    "1. **EnhancementSuggestion Model**\n",
    "The `EnhancementSuggestion` model defines the structure for improvement suggestions:\n",
    "- **`section`**: The specific resume section being improved (e.g., \"Skills\" or \"Work Experience\").\n",
    "- **`current_content`**: The original content of the section.\n",
    "- **`improved_content`**: The suggested improvement for the section.\n",
    "- **`explanation`**: A detailed explanation of why the improvement is recommended.\n",
    "\n",
    "---\n",
    "\n",
    "2. **ResumeEnhancement Model**\n",
    "The `ResumeEnhancement` model provides a holistic improvement report:\n",
    "- **`improvements`**: A list of section-specific suggestions.\n",
    "- **`keyword_optimization`**: Suggested keywords to include in the resume for optimization.\n",
    "- **`general_suggestions`**: Overall suggestions for structure and presentation.\n",
    "- **`action_items`**: Practical, actionable items for the candidate to implement.\n",
    "\n",
    "---\n",
    "\n",
    "3. **ResumeEnhancementSystem**\n",
    "This class integrates LLM-based analysis to generate detailed improvement suggestions for resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "class EnhancementSuggestion(BaseModel):\n",
    "    \"\"\"Suggestions for improvement for each resume section\"\"\"\n",
    "\n",
    "    section: str = Field(description=\"Resume section\")\n",
    "    current_content: str = Field(description=\"Current content\")\n",
    "    improved_content: str = Field(description=\"Suggested improvement\")\n",
    "    explanation: str = Field(description=\"Reason for the improvement and explanation\")\n",
    "\n",
    "\n",
    "class ResumeEnhancement(BaseModel):\n",
    "    \"\"\"Overall suggestions for resume improvement\"\"\"\n",
    "\n",
    "    improvements: List[EnhancementSuggestion] = Field(\n",
    "        description=\"Suggestions for each section\"\n",
    "    )\n",
    "    keyword_optimization: List[str] = Field(description=\"Keywords to optimize\")\n",
    "    general_suggestions: List[str] = Field(description=\"General suggestions\")\n",
    "    action_items: List[str] = Field(description=\"Actionable items\")\n",
    "\n",
    "\n",
    "class ResumeEnhancementSystem:\n",
    "    def __init__(self, model_name=\"gpt-4o\", temperature=0.1):\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ResumeEnhancement)\n",
    "\n",
    "        # Prompt template for generating improvement suggestions\n",
    "        self.prompt_template = \"\"\"You are a professional resume consultant.\n",
    "        Based on the provided evaluation results, offer detailed and actionable suggestions for improving the resume.\n",
    "\n",
    "        Current Resume:\n",
    "        {resume_text}\n",
    "\n",
    "        Evaluation Results:\n",
    "        {evaluation_results}\n",
    "\n",
    "        Job Information:\n",
    "        {job_info}\n",
    "\n",
    "        Please include the following considerations when making your suggestions:\n",
    "        1. Specific improvement suggestions for each section\n",
    "        2. Key job-related keywords\n",
    "        3. General structural and expression improvements\n",
    "        4. Short-term and long-term actionable items\n",
    "\n",
    "        Pay particular attention to the following:\n",
    "        - Emphasize areas with high scores\n",
    "        - Provide concrete solutions for areas with low scores\n",
    "        - Tailor suggestions to the characteristics of the job\n",
    "        - Ensure realistic and actionable recommendations\n",
    "\n",
    "        Provide the improvement suggestions in the following format:\n",
    "        {format_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_template(\n",
    "            template=self.prompt_template,\n",
    "            partial_variables={\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def generate_improvements(\n",
    "        self, resume_text: str, evaluation_results: List[Dict], job_info: Dict\n",
    "    ) -> ResumeEnhancement:\n",
    "        \"\"\"Generate improvement suggestions based on the evaluation results\"\"\"\n",
    "        try:\n",
    "            # Serialize evaluation_results (if DetailedEvaluation objects are included)\n",
    "            evaluation_data = [\n",
    "                (\n",
    "                    eval_result.model_dump()\n",
    "                    if hasattr(eval_result, \"model_dump\")\n",
    "                    else eval_result\n",
    "                )\n",
    "                for eval_result in evaluation_results\n",
    "            ]\n",
    "\n",
    "            messages = self.prompt.format_messages(\n",
    "                resume_text=resume_text,\n",
    "                evaluation_results=json.dumps(\n",
    "                    evaluation_data, ensure_ascii=False, indent=2\n",
    "                ),\n",
    "                job_info=json.dumps(job_info, ensure_ascii=False, indent=2),\n",
    "            )\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "            suggestions = self.parser.parse(response.content)\n",
    "            return suggestions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while generating improvement suggestions: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def format_enhancement_report(enhancement: ResumeEnhancement) -> str:\n",
    "    \"\"\"Format the improvement suggestions into a report\"\"\"\n",
    "    output = []\n",
    "    output.append(\"\\n📝 Resume Improvement Report\")\n",
    "    output.append(\"=\" * 50)\n",
    "\n",
    "    # Section-specific suggestions\n",
    "    output.append(\"\\n📋 Section-Specific Improvements\")\n",
    "    output.append(\"-\" * 30)\n",
    "    for improvement in enhancement.improvements:\n",
    "        output.append(f\"\\n[{improvement.section}]\")\n",
    "        output.append(\"Current:\")\n",
    "        output.append(f\"  {improvement.current_content}\")\n",
    "        output.append(\"Improved:\")\n",
    "        output.append(f\"  {improvement.improved_content}\")\n",
    "        output.append(\"Reason:\")\n",
    "        output.append(f\"  {improvement.explanation}\")\n",
    "\n",
    "    # Keyword optimization\n",
    "    output.append(\"\\n🔍 Recommended Keywords\")\n",
    "    output.append(\"-\" * 30)\n",
    "    for keyword in enhancement.keyword_optimization:\n",
    "        output.append(f\"• {keyword}\")\n",
    "\n",
    "    # General suggestions\n",
    "    output.append(\"\\n💡 General Suggestions\")\n",
    "    output.append(\"-\" * 30)\n",
    "    for suggestion in enhancement.general_suggestions:\n",
    "        output.append(f\"• {suggestion}\")\n",
    "\n",
    "    # Action items\n",
    "    output.append(\"\\n✅ Actionable Steps\")\n",
    "    output.append(\"-\" * 30)\n",
    "    for item in enhancement.action_items:\n",
    "        output.append(f\"• {item}\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "class IntegratedResumeSystem:\n",
    "    \"\"\"A system combining evaluation and improvement\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.evaluation_system = ResumeEvaluationSystem()\n",
    "        self.enhancement_system = ResumeEnhancementSystem()\n",
    "\n",
    "    def analyze_and_improve(\n",
    "        self, resume_path: str, recommended_jobs: List[dict], top_n: int = 3\n",
    "    ):\n",
    "        \"\"\"Perform integrated resume evaluation and improvement suggestions\"\"\"\n",
    "        try:\n",
    "            # First, process the resume to get the text content\n",
    "            resume_chunks = process_resume(resume_path)\n",
    "            resume_text = \" \".join([chunk[0] for chunk in resume_chunks])\n",
    "\n",
    "            # 1. Perform resume evaluation\n",
    "            print(\"Evaluating the resume...\")\n",
    "            evaluations = self.evaluation_system.evaluate_with_recommendations(\n",
    "                resume_text,  # Pass the processed text instead of path\n",
    "                recommended_jobs=recommended_jobs,\n",
    "                top_n=top_n,\n",
    "            )\n",
    "\n",
    "            # 2. Generate improvement suggestions for each recommended job\n",
    "            print(\"Generating improvement suggestions...\")\n",
    "            improvements = []\n",
    "\n",
    "            for eval_result in evaluations:\n",
    "                job_info = eval_result[\"job_info\"]\n",
    "                evaluation = eval_result[\"evaluation\"]\n",
    "\n",
    "                # Generate improvement suggestions using the already processed resume text\n",
    "                enhancement = self.enhancement_system.generate_improvements(\n",
    "                    resume_text=resume_text,  # Use the processed text\n",
    "                    evaluation_results=[evaluation.model_dump()],\n",
    "                    job_info=job_info,\n",
    "                )\n",
    "\n",
    "                improvements.append(\n",
    "                    {\n",
    "                        \"job_info\": job_info,\n",
    "                        \"evaluation\": evaluation,\n",
    "                        \"enhancement\": enhancement,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return improvements\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis and improvement: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excute Evaluation\n",
    "\n",
    "you can choose how many jobs you want to evaluate by changing the `top_n` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume analysis completed.\n",
      "Number of extracted chunks: 8\n",
      "\n",
      "Career Analysis Summary:\n",
      "------------------------\n",
      "Interests: Joanna Drummond's main areas of interest and research focus are in computer science, specifically in algorithms, artificial intelligence, and stable matching problems. Her research has extensively explored topics such as stable and approximately stable matching using multi-attribute preference information, preference elicitation, and the application of machine learning techniques to educational technologies. She has also investigated decision-making under uncertainty and the impact of dialogue systems on learning, demonstrating a strong interest in the intersection of computational methods and human-centered applications.\n",
      "\n",
      "Recommended Roles:\n",
      "Evaluating the resume...\n",
      "Generating improvement suggestions...\n"
     ]
    }
   ],
   "source": [
    "# Resume file path\n",
    "resume_path = \"../data/joannadrummond-cv.pdf\"\n",
    "\n",
    "# Initialize the integrated system\n",
    "system = IntegratedResumeSystem()\n",
    "\n",
    "# Perform analysis and improvements\n",
    "results = system.analyze_and_improve(\n",
    "    resume_path=resume_path, recommended_jobs=recommended_jobs, top_n=3\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "for result in results:\n",
    "    print(f\"\\nJob: {result['job_info']['position']} @ {result['job_info']['company']}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n[Evaluation Results]\")\n",
    "    print(result[\"evaluation\"])\n",
    "    print(\"\\n[Improvement Suggestions]\")\n",
    "    print(format_enhancement_report(result[\"enhancement\"]))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
