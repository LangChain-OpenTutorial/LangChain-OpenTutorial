{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# GPT4ALL\n",
    "\n",
    "- Author: [Yoonji Oh](https://github.com/samdaseuss)\n",
    "- Design:\n",
    "- Peer Review: [이종호(XaviereKU)](https://github.com/XaviereKU), [김영인(Normalist-K)](https://github.com/Normalist-K)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we’re exploring `GPT4ALL` together! From picking the perfect model for your hardware to running it on your own, we’ll walk you through the process **step by step**. \n",
    "\n",
    "Ready? Let’s dive in and have some fun along the way! \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Installation](#installation)\n",
    "- [What is GPT4ALL](#what-is-gpt4all)\n",
    "- [Choosing a Model](#choosing-a-model)\n",
    "- [Downloading a Model](#downloading-a-model)\n",
    "- [Running GPT4ALL Models](#running-gpt4all-models)\n",
    "- [Summary](#summary)\n",
    "\n",
    "### References\n",
    "\n",
    "- [GPT4All FAQ](https://docs.gpt4all.io/old/gpt4all_faq.html)\n",
    "- [Download Models](https://docs.gpt4all.io/gpt4all_desktop/models.html#connect-model-apis)\n",
    "- [GPT4All Python SDK](https://docs.gpt4all.io/gpt4all_python/home.html)\n",
    "- [GPT4ALL docs](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.gpt4all.GPT4All.html#langchain_community.llms.gpt4all.GPT4All.backend)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"GPT4ALL\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad47c55",
   "metadata": {},
   "source": [
    "You can also create and use a `.env` file in the root directory as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0765061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f3aa3",
   "metadata": {},
   "source": [
    "## Installation\n",
    "1. First, visit the official website, download the installation file, and complete the installation.\n",
    "2. Go to the [gpt4all](https://gpt4all.io/index.html) website\n",
    "3. Install the Python package.\n",
    "4. Installation using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e62b554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8fdc4",
   "metadata": {},
   "source": [
    "## What is GPT4ALL\n",
    "\n",
    "`GitHub:nomic-ai/gpt4all` is an open-source chatbot ecosystem trained on a large amount of data, including code and chat-form conversations. \n",
    "\n",
    "In this example, we will explain how to interact with the GPT4All model using LangChain.\n",
    "\n",
    "![](./assets/10-GPT4ALL-gpt4all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f3a31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a93f76",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "\n",
    "It's the most crucial and decision-making time. Before diving into writing code, it's time to decide which model to use. Below, we explore popular models and help you choose the right one based on GPT4All's [Python Documentation](https://docs.gpt4all.io/gpt4all_python/home.html#load-llm).\n",
    "\n",
    "---\n",
    "\n",
    "### Model Selection Criteria\n",
    "| **Model Name**                          | **Filesize** | **RAM Required** | **Parameters** | **Quantization** | **Developer**          | **License**           | **MD5 Sum (Unique Hash)**            |\n",
    "|-----------------------------------------|--------------|------------------|----------------|------------------|------------------------|-----------------------|--------------------------------------|\n",
    "| **Meta-Llama-3-8B-Instruct.Q4_0.gguf**  | 4.66 GB      | 8 GB            | 8 Billion      | q4_0             | Meta                   | Llama 3 License       | c87ad09e1e4c8f9c35a5fcef52b6f1c9    |\n",
    "| **Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf** | 4.11 GB   | 8 GB            | 7 Billion      | q4_0             | Mistral & Nous Research | Apache 2.0           | Coa5f6b4eabd3992da4d7fb7f020f921eb  |\n",
    "| **Phi-3-mini-4k-instruct.Q4_0.gguf**    | 2.18 GB      | 4 GB            | 3.8 Billion    | q4_0             | Microsoft              | MIT                   | f8347badde9bfc2efbe89124d78ddaf5    |\n",
    "| **orca-mini-3b-gguf2-q4_0.gguf**        | 1.98 GB      | 4 GB            | 3 Billion      | q4_0             | Microsoft              | CC-BY-NC-SA-4.0       | 0e769317b90ac30d6e09486d61fefa26    |\n",
    "| **gpt4all-13b-snoozy-q4_0.gguf**        | 7.37 GB      | 16 GB           | 13 Billion     | q4_0             | Nomic AI               | GPL                   | 40388eb2f8d16bb5d08c96fdfaac6b2c    |\n",
    "\n",
    "#### Based on Use Case\n",
    "Choose your model depending on the tasks you plan to perform:\n",
    "- **Lightweight tasks** (e.g., simple conversation): `orca-mini-3b-gguf2-q4_0.gguf` or `Phi-3-mini-4k-instruct.Q4_0.gguf`.  \n",
    "- **Moderate tasks** (e.g., summarization or grammar correction): `Meta-Llama-3-8B-Instruct.Q4_0.gguf` or `Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf`.  \n",
    "- **Advanced tasks** (e.g., long text generation, research): `gpt4all-13b-snoozy-q4_0.gguf`.\n",
    "\n",
    "#### Based on System Specifications\n",
    "Select a model based on your available hardware:\n",
    "- For **4GB RAM or less**, use `orca-mini-3b-gguf2-q4_0.gguf` or `Phi-3-mini-4k-instruct.Q4_0.gguf`.  \n",
    "- For **8GB RAM or more**, use `Meta-Llama-3-8B-Instruct.Q4_0.gguf` or `Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf`.  \n",
    "- For **16GB RAM or more**, use `gpt4all-13b-snoozy-q4_0.gguf`.\n",
    "\n",
    "\n",
    "#### [NOTE]  \n",
    "- **`GGML`**: CPU-friendly and low memory usage.  \n",
    "- **`GGUF`**: Latest format with GPU acceleration support.  \n",
    "- **`q4_0 Quantization`**: Efficient for both CPU and GPU workloads, with reduced memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01060432",
   "metadata": {},
   "source": [
    "## Downloading a Model\n",
    "In this tutorial, we will be using Microsoft's `Phi-3-Mini-4K-Instruct` model.\n",
    "\n",
    "1. **Download Models**: Visit the [Phi-3-Mini-4K-Instruct model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main)(2.2 GB) to download the required model.\n",
    "2. **Load Models in Python**: After downloading the model, create a folder named `models` and place the downloaded file in that folder. \n",
    "- Assign the local file path (e.g., `Phi-3-mini-4k-instruct-q4.gguf`) to the `local_path` variable.\n",
    "- You can replace this path with any local file path you prefer.\n",
    "\n",
    "Use the [Python Documentation](https://docs.gpt4all.io/gpt4all_python/home.html#load-llm) to load and run your selected model in your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./models/Phi-3-mini-4k-instruct-q4.gguf\" # Replace with your desired local file path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e1409",
   "metadata": {},
   "source": [
    "## Running GPT4ALL Models\n",
    "\n",
    "`GPT4All` is a `powerful large-scale language model`, similar to `GPT-3`, designed to support a variety of natural language processing tasks.  \n",
    "\n",
    "This module allows you to `easily load the GPT4All model` and perform inference seamlessly.  \n",
    "\n",
    "In the following example, we demonstrate how to `load the GPT4All model` and utilize it to `answer a question` by leveraging a `custom prompt` and `inference pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5befc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d6f15",
   "metadata": {},
   "source": [
    "[NOTE]\n",
    "\n",
    "Due to structural changes, in version `0.3.13`, you need to replace `from langchain.prompts import ChatPromptTemplate` with `from langchain_core.prompts import ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4fcd8",
   "metadata": {},
   "source": [
    "### Creating a Prompt and Checking the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0da2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
      "<s>Human: where is the capital of United States?</s>\n",
      "<s>Assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "result = prompt.invoke({\"question\": \"where is the capital of United States?\"})\n",
    "\n",
    "print(result.messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab9e5e",
   "metadata": {},
   "source": [
    "The `ChatPromptTemplate` is responsible for creating prompt templates in LangChain and dynamically substituting variables. Without using the `invoke()` method, you can utilize the class's template methods to generate prompts. In this case, the template can be returned as a string using the `format` method.\n",
    "\n",
    "```python\n",
    "# Using format() instead of invoke()\n",
    "result = prompt.format(question=\"What is the capital of United States?\")\n",
    "```\n",
    "\n",
    "In a nutshell, the `invoke()` method is great for `chain-based tasks`, while the `format()` method is perfect for `returning simple strings`. \n",
    "\n",
    "|       | **.invoke()** | **.format()**  |\n",
    "|-------|---------------|----------------|\n",
    "| **Purpose**      | Creates a structured object for chain execution | Returns the template as a simple string |\n",
    "| **Return Value** | ChatPromptValue object (structured LangChain object) | String                              |\n",
    "| **Use Case**     | Used in LangChain chain operations          | Used for simple prompt generation and testing |\n",
    "| **Complexity**   | Relatively complex                          | Simple and intuitive                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4e5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
      "<s>Human: where is the capital of United States?</s>\n",
      "<s>Assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = prompt.format(question=\"where is the capital of United States?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec19bd",
   "metadata": {},
   "source": [
    "You might notice that `Human:` is automatically added to the output. If you'd like to avoid this behavior, you can use LangChain's `PromptTemplate` class instead of `ChatPromptTemplate`. The `PromptTemplate` class doesn’t add any extra prefixes like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873ea75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
      "<s>Human: Where is the capital of the United States?</s>\n",
      "<s>Assistant:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "formatted_prompt = prompt.format(question=\"Where is the capital of the United States?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf53bc7",
   "metadata": {},
   "source": [
    "We'll be using invoke for `chain-based tasks`, so go ahead and forget about the format method for now! 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3adc4f",
   "metadata": {},
   "source": [
    "### Using Chains to Display Results in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7551b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "The capital of the United States is Washington, D.C., which stands for District of Columbia. It was established by the Constitution along with a federal district that would serve as the nation's seat of government and be under its exclusive jurisdiction. The city itself lies on the east bank of the Potomac River near its fall point where it empties into Chesapeake Bay, but Washington is not part of any U.S. state; instead, it has a special federal district status as outlined in Article I, Section 8 of the Constitution and further defined by the Residence Act of 1790 signed by President George Washington.\n",
      "\n",
      "Washington D.C.'s location was chosen to be near the nation's capital at that time—Philadelphia, Pennsylvania—and it also holds symbolic significance as a neutral ground for both northern and southern states during their early years in America. The city is home to many iconic landmarks such as the U.S. Capitol Building where Congress meets, the White House (the residence of the President), Supreme Court buildings, numerous museums like the Smithsonian Institution's National Museum of American History or Natural History and Air & Space, among others"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\" \n",
    "    <s>A chat between a user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "    <s>Human: {question}</s>\n",
    "    <s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "model = GPT4All(\n",
    "    model=local_path,\n",
    "    n_threads=8,\n",
    "    backend=\"gpu\",\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"question\": \"where is the capital of United States?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb0841",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Today, we explored GPT4ALL together! **We didn’t just run models** — we took part in the decision-making process, from selecting a model to suit our needs to choosing the right methods based on our desired outcomes or execution direction. Along the way, we compared the performance of popular models and even ran the code ourselves.\n",
    "\n",
    "Next time, we’ll dive into `Video Q&A LLM (Gemini)`. Until then, try running today’s code with different models and see how they perform. See you soon! 😊"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
