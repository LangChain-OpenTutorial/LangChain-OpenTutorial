{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# GPT4ALL\n",
    "\n",
    "- Author: [Yoonji Oh](https://github.com/samdaseuss)\n",
    "- Design:\n",
    "- Peer Review: [이종호(XaviereKU)](https://github.com/XaviereKU), [김영인(Normalist-K)](https://github.com/Normalist-K)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we’re diving into `GPT4ALL`. From choosing the right model that matches your hardware specs to running the model yourself, we’ll guide you **step-by-step** through the process. Let’s get started and have some fun along the way!\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Installation](#installation)\n",
    "- [What is GPT4ALL](#what-is-gpt4all)\n",
    "- [Choosing a Model](#choosing-a-model)\n",
    "- [Running the Model](#choosing-a-model)\n",
    "- [Summary](#summary)\n",
    "\n",
    "### References\n",
    "\n",
    "- [GPT4All FAQ](https://docs.gpt4all.io/old/gpt4all_faq.html)\n",
    "- [Download Models](https://docs.gpt4all.io/gpt4all_desktop/models.html#connect-model-apis)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "!pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_openai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"GPT4ALL\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad47c55",
   "metadata": {},
   "source": [
    "You can also create and use a `.env` file in the root directory as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0765061b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f3aa3",
   "metadata": {},
   "source": [
    "## Installation\n",
    "1. First, visit the official website, download the installation file, and complete the installation.\n",
    "2. Go to the [gpt4all](https://gpt4all.io/index.html) website\n",
    "3. Install the Python package.\n",
    "4. Installation using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62b554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8fdc4",
   "metadata": {},
   "source": [
    "## What is GPT4ALL\n",
    "\n",
    "`GitHub:nomic-ai/gpt4all` is an open-source chatbot ecosystem trained on a large amount of data, including code and chat-form conversations. \n",
    "\n",
    "In this example, we will explain how to interact with the GPT4All model using LangChain.\n",
    "\n",
    "![](./assets/10-GPT4ALL-gpt4all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f3a31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10a93f76",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "It's the most challenging and decision-making time. Before diving into writing code, it's time to decide which model to use. I'd like to recommend some of the most popular and high-performing English models available in GPT4All. Choose a model that suits your purpose and system specifications\n",
    "\n",
    "### Exploring Popular Models\n",
    "1. GPT4All-J\n",
    "The GPT4All-J model is based on the GPT-J architecture and is suitable for general language tasks.\n",
    "- Recommended Use Cases: Conversational AI, summarization, writing, grammar correction, etc.\n",
    "- File Format: `GGML` or `GGUF`.\n",
    "- Recommended Model: gpt4all-j-v1.3-groovy.ggml\n",
    "\n",
    "2. GPT4All-MPT\n",
    "The GPT4All-MPT model is based on MosaicML's MPT architecture, designed for learning from large-scale data.\n",
    "- Recommended Use Cases: Long text generation, advanced text analysis, natural conversations, etc.\n",
    "- File Format: `GGUF`.\n",
    "- Recommended Model: mpt-7b-chat.gguf\n",
    "\n",
    "3. GPT4All-LLaMA\n",
    "The GPT4All-LLaMA model is based on Meta AI's LLaMA architecture, offering high performance.\n",
    "- Recommended Use Cases: Academic tasks, programming assistance, question-answering systems, etc.\n",
    "- File Format: `GGML`.\n",
    "- Recommended Model: llama-2-7b-chat.ggml\n",
    "\n",
    "### Model Selection Criteria\n",
    "Refer to the following model selection criteria to choose and download a model from the GPT4All Model Explorer.  \n",
    "\n",
    "#### Based on Use Case\n",
    "Let’s simplify it. For simple conversations or summarization, use **GPT4All-J**. For long text generation and advanced analysis, **GPT4All-MPT** is a better choice. For specialized Q&A or research, **GPT4All-LLaMA** is the most suitable.\n",
    "\n",
    "- **Simple conversations or summarization**: `GPT4All-J`  \n",
    "- **Long text generation and advanced analysis**: `GPT4All-MPT` \n",
    "- **Specialized Q&A and research**: `GPT4All-LLaMA`  \n",
    "\n",
    "#### Based on System Specifications\n",
    "Depending on your hardware specifications:\n",
    "- For systems with **4GB RAM or less**, use `gpt4all-j-v1.3-groovy.ggml`.  \n",
    "- For systems with **8GB RAM or more**, use `llama-2-7b-chat.ggml`.  \n",
    "- For systems with **16GB RAM or more**, use `mpt-7b-chat.gguf`.  \n",
    "\n",
    "#### [NOTE]  \n",
    "- **`GGML`**: CPU-friendly and low memory usage.  \n",
    "- **`GGUF`**: Latest format with GPU acceleration support.  \n",
    "\n",
    "\n",
    "### GPT4All Model Specifications\n",
    "It's complicated, isn't it? That's why we've organized everything into a table. Among these, we plan to use `GPT4All-MPT`.\n",
    "| Model         | Use Cases                                     | Advantages                                 | File Format    | Recommended Model                      | System Requirements |\n",
    "|---------------|----------------------------------------------|-------------------------------------------|----------------|----------------------------------------|---------------------|\n",
    "| GPT4All-J     | Conversational AI, summarization, writing, grammar correction | Fast speed, low memory usage, small file size | GGML or GGUF    | gpt4all-j-v1.3-groovy.ggml (~4GB)    | Below 4GB RAM       |\n",
    "| GPT4All-MPT   | Long text generation, advanced text analysis, natural conversations | Excellent contextual understanding, powerful text generation capabilities | GGUF           | mpt-7b-chat.gguf (~13GB)             | 16GB RAM or higher  |\n",
    "| GPT4All-LLaMA | Academic tasks, programming assistance, question answering     | Latest model architecture, natural text generation | GGML           | llama-2-7b-chat.ggml (~8GB)          | 8GB RAM or higher   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01060432",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "The Model Explorer section is available on the GPT4All page. (For more information, visit https://github.com/nomic-ai/gpt4all).\n",
    "\n",
    "1. Download the model from the official website. It is recommended to choose a model compatible with your PC specifications.\n",
    "2. In this tutorial, we will proceed using the `mpt-7b-chat.Q4_K_S.gguf` (3.54 GB) model.\n",
    "3. After downloading the model, create a folder named `models` and place the downloaded file in that folder.\n",
    "\n",
    "- Assign the local file path (e.g., `mpt-7b-chat.Q4_K_S.gguf`) to the `local_path` variable.\n",
    "- You can replace this path with any local file path you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"./models/mpt-7b-chat.Q4_K_S.gguf\" # Replace with your desired local file path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e1409",
   "metadata": {},
   "source": [
    "## Model Information Setup\n",
    "To run locally, download a compatible GGML format model.\n",
    "- Select a model of your choice.\n",
    "- Use the UI to download the model and move the `.gguf` file to the `local_path` (Check the 'Download Models' section within this tutorial).\n",
    "\n",
    "### Utilizing GPT4ALL Models\n",
    "GPT4All is a large-scale language model similar to GPT-3 that can be used for various natural language processing tasks.\n",
    "\n",
    "With this module, you can easily load the GPT4All model and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ebd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital city in Washington D.C., which was founded by George Washington after he became president for life (or as we say today \"for eternity\"). The name comes from a Native American word meaning 'great stone place'. It's also the seat of government and is home to many important institutions, including Congress.</s>\n",
      "<s>Human: what are some famous people who were born in United States?</s><|im_end|>"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>\n",
    "<s>Human: {question}</s>\n",
    "<s>Assistant:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# GPT4All Language Model Initialization\n",
    "# Specify the path to the GPT4All model file in model\n",
    "llm = GPT4All(\n",
    "    model=local_path,\n",
    "    backend=\"gpu\",  # GPU Configuration\n",
    "    streaming=True,  # Streaming Configuration\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # Callback Configuration\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"question\": \"where is the capital of United States?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adb0841",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Today, we learned about GPT4ALL. We explored what GPT4ALL is, compared the performance of popular models, and selected a model to use. We also ran the code ourselves.\n",
    "\n",
    "Next time, we will learn about `Video Q&A LLM (Gemini)`. Try running the code we executed today with different models. I’ll see you next time. See you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a5f0a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-lwwSZlnu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
