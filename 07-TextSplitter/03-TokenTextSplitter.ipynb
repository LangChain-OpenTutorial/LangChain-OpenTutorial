{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TokenTextSplitter\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design: \n",
    "- Peer Review:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Language models operate within token limits, making it crucial to manage text within these constraints. \n",
    "\n",
    "TokenTextSplitter serves as an effective tool for segmenting text into manageable chunks based on token count, ensuring compliance with these limitations.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Basic Usage of Tiktoken](#basic-usage-of-tiktoken)\n",
    "- [Basic Usage of TokenTextSplitter](#basic-usage-of-tokentextsplitter)\n",
    "- [Basic Usage of spaCy](#basic-usage-of-spaCy)\n",
    "- [Basic Usage of SentenceTransformers](#basic-usage-of-sentencetransformers)\n",
    "- [Basic Usage of NLTK](#basic-usage-of-NLTK)\n",
    "- [Basic Usage of KoNLPy](#basic-usage-of-KoNLPy)\n",
    "- [Basic Usage of Hugging Face tokenizer](#basic-usage-of-Hugging-Face-tokenizer)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langchain TokenTextSplitter](https://python.langchain.com/api_reference/text_splitters/base/langchain_text_splitters.base.TokenTextSplitter.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"tiktoken\",\n",
    "        \"spacy\",\n",
    "        \"sentence-transformers\",\n",
    "        \"nltk\",\n",
    "        \"konlpy\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"TokenTextSplitter\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of tiktoken\n",
    "\n",
    "`tiktoken` is a fast BPE tokenizer created by OpenAI.\n",
    "\n",
    "- Open the file ./data/(eng)appendix-keywords.txt and read its contents.\n",
    "- Store the read content in the file variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/(eng)appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a portion of the content read from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `CharacterTextSplitter` to split the text.\n",
    "\n",
    "- Initialize the text splitter using the `from_tiktoken_encoder` method, which is based on the Tiktoken encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # Set the chunk size to 300.\n",
    "    chunk_size=300,\n",
    "    # Ensure there is no overlap between chunks.\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of divided chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(texts))  # Output the number of divided chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first element of the texts list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first element of the texts list.\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- When using `CharacterTextSplitter.from_tiktoken_encoder`, the text is split solely by `CharacterTextSplitter`, and the `Tiktoken` tokenizer is only used to measure and merge the divided text. (This means that the split text might exceed the chunk size as measured by the `Tiktoken` tokenizer.)\n",
    "- When using `RecursiveCharacterTextSplitter.from_tiktoken_encoder`, the divided text is ensured not to exceed the chunk size allowed by the language model. If a split text exceeds this size, it is recursively divided. Additionally, you can directly load the `Tiktoken` splitter, which guarantees that each split is smaller than the chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of TokenTextSplitter\n",
    "\n",
    "Use the `TokenTextSplitter` class to split the text into token-based chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 10.\n",
    "    chunk_overlap=0,  # Set the overlap between chunks to 0.\n",
    ")\n",
    "\n",
    "# Split the state_of_the_union text into chunks.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first chunk of the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of spaCy\n",
    "\n",
    "spaCy is an open-source software library for advanced natural language processing, written in Python and Cython programming languages.\n",
    "\n",
    "Another alternative to NLTK is using the spaCy tokenizer.\n",
    "\n",
    "1. How the text is divided: The text is split using the spaCy tokenizer.\n",
    "2. How the chunk size is measured: It is measured by the number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the en_core_web_sm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the `appendix-keywords.txt` file and read its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file data/(eng)appendix-keywords.txt and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify by printing a portion of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a text splitter using the `SpacyTextSplitter` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "# Ignore  warning messages.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Create the SpacyTextSplitter.\n",
    "text_splitter = SpacyTextSplitter(\n",
    "    chunk_size=200,  # Set the chunk size to 200.\n",
    "    chunk_overlap=50,  # Set the overlap between chunks to 50.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `split_text` method of the `text_splitter` object to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of SentenceTransformers\n",
    "\n",
    "`SentenceTransformersTokenTextSplitter` is a text splitter specialized for `sentence-transformer` models.\n",
    "\n",
    "Its default behavior is to split text into chunks that fit within the token window of the sentence-transformer model being used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "# Create a sentence splitter and set the overlap between chunks to 0.\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_size=200, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data/(eng)appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code counts the number of tokens in the text stored in `the file` variable, excluding the count of start and stop tokens, and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_start_and_stop_tokens = 2  # Set the number of start and stop tokens to 2.\n",
    "\n",
    "# Subtract the count of start and stop tokens from the total number of tokens in the text.\n",
    "text_token_count = splitter.count_tokens(text=file) - count_start_and_stop_tokens\n",
    "print(text_token_count)  # Print the calculated number of tokens in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `splitter.split_text()` function to split the text stored in the `text_to_split` variable into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=file)  # Split the text into chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the text into chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 0th chunk.\n",
    "print(text_chunks[1])  # Print the second chunk from the divided text chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a library and a collection of programs for English natural language processing (NLP), written in the Python programming language.\n",
    "\n",
    "Instead of simply splitting by \"\\n\\n\", NLTK can be used to split text based on NLTK tokenizers.\n",
    "1. Text splitting method: The text is split using the NLTK tokenizer.\n",
    "2.\tChunk size measurement: The size is measured by the number of characters.\n",
    "3.\t`nltk` (Natural Language Toolkit) is a Python library for natural language processing.\n",
    "4.\tIt supports various NLP tasks such as text preprocessing, tokenization, morphological analysis, and part-of-speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using NLTK, you need to run `nltk.download('punkt_tab')`.\n",
    "\n",
    "The reason for running `nltk.download('punkt_tab')` is to allow the NLTK (Natural Language Toolkit) library to download the necessary data files required for tokenizing text.\n",
    "\n",
    "Specifically, punkt_tab is a tokenization model capable of splitting text into words or sentences for multiple languages, including English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the sample text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data/(eng)appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = (\n",
    "        f.read()\n",
    "    )  # Read the contents of the file and store them in the file variable.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a text splitter using the `NLTKTextSplitter` class.\n",
    "- Set the `chunk_size` parameter to 1000 to split the text into chunks of up to 1000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(\n",
    "    chunk_size=300,  # Set the chunk size to 200.\n",
    "    chunk_overlap=0,  # Set the overlap between chunks to 0.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `split_text` method of the `text_splitter` object to split the `file` text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file text using the text_splitter.\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])  # Print the first element of the split text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of KoNLPy\n",
    "\n",
    "KoNLPy (Korean NLP in Python) is a Python package for Korean Natural Language Processing (NLP).\n",
    "\n",
    "Tokenization\n",
    "Tokenization involves the process of dividing text into smaller, more manageable units called tokens.\n",
    "\n",
    "These tokens often represent meaningful elements such as words, phrases, symbols, or other components crucial for further processing and analysis.\n",
    "\n",
    "In languages like English, tokenization typically involves separating words based on spaces and punctuation.\n",
    "\n",
    "The effectiveness of tokenization largely depends on the tokenizer's understanding of the language structure, ensuring the generation of meaningful tokens.\n",
    "\n",
    "Tokenizers designed for English lack the ability to comprehend the unique semantic structure of other languages, such as Korean, and therefore cannot be effectively used for Korean text processing.\n",
    "\n",
    "### Korean Tokenization Using KoNLPy’s Kkma Analyzer\n",
    "\n",
    "For Korean text, KoNLPy includes a morphological analyzer called Kkma (Korean Knowledge Morpheme Analyzer).\n",
    "\n",
    "Kkma provides detailed morphological analysis for Korean text.\n",
    "It breaks sentences into words and further decomposes words into their morphemes while identifying the part of speech for each token.\n",
    "It can also split text blocks into individual sentences, which is particularly useful for processing lengthy texts.\n",
    "\n",
    "### Considerations When Using Kkma\n",
    "Kkma is known for its detailed analysis. However, this precision can affect processing speed.\n",
    "Therefore, Kkma is best suited for applications that prioritize analytical depth over rapid text processing.\n",
    "- KoNLPy is a Python package for Korean Natural Language Processing, offering features such as morphological analysis, part-of-speech tagging, and syntactic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data/(eng)appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of splitting Korean text using KonlpyTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import KonlpyTextSplitter\n",
    "\n",
    "# Create a text splitter object using KonlpyTextSplitter.\n",
    "text_splitter = KonlpyTextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `text_splitter` to split `the file` content into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(file)  # Split the file content into sentences.\n",
    "print(texts[0])  # Print the first sentence from the divided text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of Hugging Face tokenizer\n",
    "\n",
    "Hugging Face provides various tokenizers.\n",
    "\n",
    "This code demonstrates calculating the token length of a text using one of Hugging Face's tokenizers, GPT2TokenizerFast.\n",
    "\n",
    "The text splitting approach is as follows:\n",
    "\n",
    "- The text is split at the character level.\n",
    "\n",
    "The chunk size measurement is determined as follows:\n",
    "\n",
    "- It is based on the number of tokens calculated by the Hugging Face tokenizer.\n",
    "- A `tokenizer` object is created using the `GPT2TokenizerFast` class.\n",
    "- `from_pretrained` method is called to load the pre-trained \"gpt2\" tokenizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# Load the GPT-2 tokenizer.\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data/(eng)appendix-keywords.txt file and create a file object named f.\n",
    "with open(\"./data/(eng)appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # Read the file content and store it in the variable file.\n",
    "\n",
    "# Print a portion of the content read from the file.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_huggingface_tokenizer` method is used to initialize a text splitter with a Hugging Face tokenizer (`tokenizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    # Use the Hugging Face tokenizer to create a CharacterTextSplitter object.\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "# Split the file text into chunks.\n",
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the split result of the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[1])  # Print the first element of the texts list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
