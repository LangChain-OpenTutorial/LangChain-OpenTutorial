{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Caching\n",
    "\n",
    "- Author: [PangPangGod](https://github.com/pangpanggod)\n",
    "- Design: []()\n",
    "- Peer Review : []()\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/12-RunnableRetry.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/13-LangChain-Expression-Language/12-RunnableRetry.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Prompt caching is a powerful feature that optimizes API usage by enabling resumption from specific prefixes in your prompts.  \n",
    "This method greatly reduces processing time and costs for repetitive tasks or prompts with consistent components.\n",
    "\n",
    "Prompt Caching is especially useful for this situations:\n",
    "\n",
    "- Prompts with many examples\n",
    "- Large amounts of context or background information\n",
    "- Repetitive tasks with consistent instructions\n",
    "- Long multi-turn conversations\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](##overview)\n",
    "- [Fetch Data](##fetch-data)\n",
    "- [OpenAI](##OpenAI)\n",
    "- [Anthropic](##anthropic)\n",
    "\n",
    "### References\n",
    "\n",
    "- [OpenAI Prompt Caching Documentation](https://platform.openai.com/docs/guides/prompt-caching)\n",
    "- [Anthropic Prompt Caching Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain-core\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain-anthropic\",\n",
    "        \"langchain-google-genai\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set environment variables\n",
    "# from langchain_opentutorial import set_env\n",
    "\n",
    "# set_env(\n",
    "#     {   \n",
    "#         \"OPENAI_API_KEY\": \"\",\n",
    "#         \"ANTHROPIC_API_KEY\": \"\",\n",
    "#         \"GOOGLE_API_KEY\": \"\",\n",
    "#         \"LANGCHAIN_API_KEY\": \"\",\n",
    "#         \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "#         \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "#         \"LANGCHAIN_PROJECT\": \"Prompt-Caching\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "\n",
    "The easiest way to verify prompt caching is by including large amounts of context or background information.  \n",
    "To demonstrate this, I have provided a simple example using a long document retrieved from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def fetch_wikipedia_page(title: str, lang: str = \"en\"):\n",
    "    \"\"\"\n",
    "    Fetch the content of a Wikipedia page using the Wikipedia API.\n",
    "    \n",
    "    Args:\n",
    "        title (str): The title of the Wikipedia page to fetch.\n",
    "        lang (str): The language code for the Wikipedia (default: \"en\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The plain text content of the Wikipedia page.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    endpoint = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    # Query parameters\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"titles\": title,\n",
    "        \"explaintext\": True\n",
    "    }\n",
    "    \n",
    "    # Encode the parameters and create the URL\n",
    "    url = f\"{endpoint}?{urllib.parse.urlencode(params)}\"\n",
    "    \n",
    "    # Send the request and read the response\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        data = json.load(response)\n",
    "    \n",
    "    # Extract page content\n",
    "    pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    for page_id, page in pages.items():\n",
    "        if \"extract\" in page:\n",
    "            return page[\"extract\"]\n",
    "    \n",
    "    return \"No content found for the given title.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from wikipedia\n",
    "title = \"World War II\"\n",
    "content = fetch_wikipedia_page(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "OpenAI Prompt Caching works automatically on all your API requests (no code changes required) and has no additional fees associated with it.  \n",
    "This can reduce latency by up to **80%** and costs by **50%** for long prompts. Caching is available for prompts containing 1024 tokens or more.\n",
    "\n",
    "### Models Supporting Prompt Caching\n",
    "\n",
    "| Model                                    | Text Input Cost | Audio Input Cost |\n",
    "|------------------------------------------|-----------------|------------------|\n",
    "| gpt-4o (excludes gpt-4o-2024-05-13 and chatgpt-4o-latest) | 50% less         | n/a              |\n",
    "| gpt-4o-mini                              | 50% less         | n/a              |\n",
    "| gpt-4o-realtime-preview                  | 50% less         | 80% less         |\n",
    "| o1-preview                               | 50% less         | n/a              |\n",
    "| o1-mini                                  | 50% less         | n/a              |\n",
    "\n",
    "for detailed reference, please check link below.  \n",
    "[OpenAI Prompt caching](https://platform.openai.com/docs/guides/prompt-caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Australia and New Zealand joined World War II shortly after the outbreak of the war in Europe. On 3 September 1939, the United Kingdom declared war on Germany following Germany's invasion of Poland. Consequently, Australia and New Zealand, as part of the British Empire, also declared war on Germany on the same day.\n",
      "Token Usage: {'token_usage': {'completion_tokens': 65, 'prompt_tokens': 17389, 'total_tokens': 17454, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "Caching Answer: The first battle between Australia, New Zealand, and Japan took place at the Battle of the Coral Sea in May 1942.\n",
      "Token Usage: {'token_usage': {'completion_tokens': 27, 'prompt_tokens': 17395, 'total_tokens': 17422, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 17152}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"{question}\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "first_response = chain.invoke({\"content\": content,\"question\":\"When did Australia and New Zealand join the war?\"})\n",
    "second_response = chain.invoke({\"content\": content,\"question\":\"Where did the first battle between Australia, New Zealand, and Japan take place?\"})\n",
    "\n",
    "# You can see only cache read in 'prompt_tokens_details' -> 'cached_tokens' in langchain 0.3.29 OpenAI calls.\n",
    "print(f\"Answer: {first_response.content}\")\n",
    "print(f\"Token Usage: {first_response.response_metadata}\")\n",
    "print()\n",
    "print(f\"Caching Answer: {second_response.content}\")\n",
    "print(f\"Token Usage: {second_response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anthropic\n",
    "\n",
    "Anthropic Prompt Caching provides the following token limits for caching:\n",
    "- **1024 tokens** for Claude 3.5 Sonnet and Claude 3 Opus\n",
    "- **2048 tokens** for Claude 3.5 Haiku and Claude 3 Haiku\n",
    "\n",
    "**Important Notes:**\n",
    "- Shorter prompts cannot be cached, even if marked with `cache_control`.\n",
    "- The cache has a **5-minute time to live (TTL)**. Currently, \"ephemeral\" is the only supported cache type, corresponding to this 5-minute lifetime.\n",
    "\n",
    "### Models Supporting Prompt Caching\n",
    "- Claude 3.5 Sonnet\n",
    "- Claude 3.5 Haiku\n",
    "- Claude 3 Haiku\n",
    "- Claude 3 Opus\n",
    "\n",
    "While it has the drawback of requiring adherence to the Anthropic Message Style, a key advantage of Anthropic Prompt Caching is that it enables caching with fewer tokens.  \n",
    "\n",
    "for detailed reference, please check link below.   \n",
    "[Anthropic Prompt Caching Documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the provided document, there is no specific detailed explanation for why Yugoslavia was invaded. The text only briefly mentions that in late March 1941, Bulgaria and Yugoslavia signed the Tripartite Pact. However, two days later, the Yugoslav government was overthrown by pro-British nationalists. In response, Germany and Italy simultaneously invaded both Yugoslavia and Greece on April 6, 1941, with both nations forced to surrender within a month.\n",
      "\n",
      "The document does not provide the deeper political or strategic motivations behind the invasion. It simply notes the sequence of events: the pact signing, the government overthrow, and the subsequent Axis invasion.\n",
      "Token Usage: {'id': 'msg_016dMiPeQ3iJCC8zRSxsLBXQ', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 18837, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 139}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model = \"claude-3-5-haiku-latest\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"text\": f\"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\", \n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Why was Yugoslavia invaded?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "first_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Answer: {first_response.content}\")\n",
    "# You can see cache read in 'input_token_details' -> 'cache_creation_tokens' or 'cache_read_input_tokens'.\n",
    "print(f\"Token Usage: {first_response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: According to the document, after Yugoslavia, Greece was invaded. Specifically, the text states: \"Germany and Italy responded with simultaneous invasions of both Yugoslavia and Greece, commencing on 6 April 1941; both nations were forced to surrender within the month.\"\n",
      "Token Usage: {'id': 'msg_01KEnBPDjA2w26i6iQYiKzX5', 'model': 'claude-3-5-haiku-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 18837, 'input_tokens': 13, 'output_tokens': 59}}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\n",
    "            \"type\": \"text\",\n",
    "            #The {content} is sourced from the Wikipedia article mentioned above.\n",
    "            \"text\": f\"You are an assistant who answers questions based on the provided document.\\n<document>{content}</document>\", \n",
    "            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "        }]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Where was invaded after Yugoslavia?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "second_response = llm.invoke(messages)\n",
    "\n",
    "print(f\"Answer: {second_response.content}\")\n",
    "# You can see cache read in 'input_token_details' -> 'cache_creation_tokens' or 'cache_read_input_tokens'.\n",
    "print(f\"Token Usage: {second_response.response_metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
