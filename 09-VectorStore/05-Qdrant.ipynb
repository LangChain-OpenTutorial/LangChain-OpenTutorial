{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant\n",
    "\n",
    "- Author: [HyeonJong Moon](https://github.com/hj0302)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to utilize the features related to the `Qdrant` vector database.\n",
    "\n",
    "[`Qdrant`](https://python.langchain.com/docs/integrations/vectorstores/qdrant/) is an open-source vector similarity search engine designed to store, search, and manage high-dimensional vectors with additional payloads. It offers a production-ready service with a user-friendly API, suitable for applications such as semantic search, recommendation systems, and more.\n",
    "\n",
    "Qdrant's architecture is optimized for efficient vector similarity searches, employing advanced indexing techniques like Hierarchical Navigable Small World (HNSW) graphs to enable fast and scalable retrieval of relevant data.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Credentials](#credentials)\n",
    "- [Installation](#installation)\n",
    "- [Initialization](#initialization)\n",
    "- [Manage Vector Store](#manage-vector-store)\n",
    "  - [Create a Collection](#create-a-collection)\n",
    "  - [List Collections](#list-collections)\n",
    "  - [Delete a Collection](#delete-a-collection)\n",
    "  - [Add Items to the Vector Store](#add-items-to-the-vector-store)\n",
    "  - [Delete Items from the Vector Store](#delete-items-from-the-vector-store)\n",
    "  - [Upsert Items to Vector Store (Parallel)](#upsert-items-to-vector-store-parallel)\n",
    "- [Query Vector Store](#query-vector-store)\n",
    "  - [Query Directly](#query-directly)\n",
    "  - [Similarity Search with Score](#similarity-search-with-score)\n",
    "  - [Query by Turning into Retriever](#query-by-turning-into-retriever)\n",
    "  - [Search with Filtering](#search-with-filtering)\n",
    "  - [Delete with Filtering](#delete-with-filtering)\n",
    "  - [Filtering and Updating Records](#filtering-and-updating-records)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain Qdrant Reference](https://python.langchain.com/docs/integrations/vectorstores/qdrant/)\n",
    "- [Qdrant Official Reference](https://qdrant.tech/documentation/frameworks/langchain/)\n",
    "- [Qdrant Install Reference](https://qdrant.tech/documentation/guides/installation/)\n",
    "- [Qdrant Cloud Reference](https://cloud.qdrant.io)\n",
    "- [Qdrant Cloud Quickstart Reference](https://qdrant.tech/documentation/quickstart-cloud/)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to Environment Setup for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_qdrant\",\n",
    "        \"qdrant_client\",\n",
    "        \"langchain_core\",\n",
    "        \"fastembed\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPEN_API_KEY\": \"\",\n",
    "        \"QDRANT_API_KEY\": \"\",\n",
    "        \"QDRANT_URL\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Qdrant\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "**[Note]** If you are using a `.env` file, proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "\n",
    "Create a new account or sign in to your existing one, and generate an API key for use in this notebook.\n",
    "\n",
    "1. **Log in to Qdrant Cloud** : Go to the [Qdrant Cloud](https://cloud.qdrant.io) website and log in using your email, Google account, or GitHub account.\n",
    "\n",
    "2. **Create a Cluster** : After logging in, navigate to the `\"Clusters\"` section and click the `\"Create\"` button. Choose your desired configurations and region, then click `\"Create\"` to start building your cluster. Once the cluster is created, an API key will be generated for you.\n",
    "\n",
    "3. **Retrieve and Store Your API Key** : When your cluster is created, you will receive an API key. Ensure you save this key in a secure location, as you will need it later. If you lose it, you will have to generate a new one.\n",
    "\n",
    "4. **Manage API Keys** : To create additional API keys or manage existing ones, go to the `\"Access Management\"` section in the Qdrant Cloud dashboard and select `\"Qdrant Cloud API Keys\"` Here, you can create new keys or delete existing ones.\n",
    "\n",
    "```\n",
    "QDRANT_API_KEY=\"YOUR_QDRANT_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "There are several main options for initializing and using the Qdrant vector store:\n",
    "\n",
    "- **Local Mode** : This mode doesn't require a separate server.\n",
    "    - **In-memory storage** (data is not persisted)\n",
    "    - **On-disk storage** (data is saved to your local machine)\n",
    "- **Docker Deployments** : You can run Qdrant using Docker.\n",
    "- **Qdrant Cloud** : Use Qdrant as a managed cloud service.\n",
    "\n",
    "For detailed instructions, see the [installation instructions](https://qdrant.tech/documentation/guides/installation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory\n",
    "\n",
    "For simple tests or quick experiments, you might choose to store data directly in memory. This means the data is automatically removed when your client terminates, typically at the end of your script or notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'demo_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'demo_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with in-memory storage\n",
    "db = QdrantDocumentManager(\n",
    "    location=\":memory:\",  # Use in-memory database for temporary storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Disk Storage\n",
    "\n",
    "With on-disk storage, you can store your vectors directly on your hard drive without requiring a Qdrant server. This ensures that your data persists even when you restart the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'demo_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'demo_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the path for Qdrant storage\n",
    "qdrant_path = \"./qdrant_memory\"\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    path=qdrant_path,  # Specify the path for Qdrant storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Deployments\n",
    "\n",
    "You can deploy `Qdrant` in a production environment using [Docker](https://qdrant.tech/documentation/guides/installation/#docker) and [Docker Compose](https://qdrant.tech/documentation/guides/installation/#docker-compose). Refer to the Docker and Docker Compose setup instructions in the development section for detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the URL for Qdrant server\n",
    "url = \"http://localhost:6333\"\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=url,  # Specify the path for Qdrant storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant Cloud\n",
    "\n",
    "For a production environment, you can use [Qdrant Cloud](https://cloud.qdrant.io/). It offers fully managed `Qdrant` databases with features such as horizontal and vertical scaling, one-click setup and upgrades, monitoring, logging, backups, and disaster recovery. For more information, refer to the [Qdrant Cloud documentation](https://qdrant.tech/documentation/cloud/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Fetch the Qdrant server URL from environment variables or prompt for input\n",
    "if not os.getenv(\"QDRANT_URL\"):\n",
    "    os.environ[\"QDRANT_URL\"] = getpass.getpass(\"Enter your Qdrant Cloud URL key: \")\n",
    "QDRANT_URL = os.environ.get(\"QDRANT_URL\")\n",
    "\n",
    "# Fetch the Qdrant API key from environment variables or prompt for input\n",
    "if not os.getenv(\"QDRANT_API_KEY\"):\n",
    "    os.environ[\"QDRANT_API_KEY\"] = getpass.getpass(\"Enter your Qdrant API key: \")\n",
    "QDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'demo_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'demo_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Once you've established your vector store, you'll likely need to manage the collections within it. Here are some common operations you can perform:\n",
    "\n",
    "- Create a collection\n",
    "- List collections\n",
    "- Delete a collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Collection\n",
    "\n",
    " The `QdrantDocumentManager` class allows you to create a new collection in Qdrant. It can automatically create a collection if it doesn't exist or if you want to recreate it. You can specify configurations for dense and sparse vectors to meet different search needs. Use the `_ensure_collection_exists` method for automatic creation or call `create_collection` directly when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'test_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'test_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client.http.models import Distance\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    "    metric=Distance.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Collections\n",
    "\n",
    "The `QdrantDocumentManager` class lets you list all collections in your Qdrant instance using the `get_collections` method. This retrieves and displays the names of all existing collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Name: test_collection\n",
      "Collection Name: demo_collection\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of collections from the Qdrant client\n",
    "collections = db.client.get_collections()\n",
    "\n",
    "# Iterate over each collection and print its details\n",
    "for collection in collections.collections:\n",
    "    print(f\"Collection Name: {collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a Collection\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to delete a collection using the `delete_collection` method. This method removes the specified collection from your Qdrant instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'test_collection' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Define collection name\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "# Delete the collection\n",
    "if db.client.delete_collection(collection_name=collection_name):\n",
    "    print(f\"Collection '{collection_name}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage VectorStore\n",
    "\n",
    "After you've created your vector store, you can interact with it by adding or deleting items. Here are some common operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Items to the Vector Store\n",
    "\n",
    "The `QdrantDocumentManager` class lets you add items to your vector store using the `upsert` method. This method updates existing documents with new data if their IDs already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from uuid import uuid4\n",
    "\n",
    "# Load the text file\n",
    "loader = TextLoader(\"./data/the_little_prince.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600, chunk_overlap=100, length_function=len\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate unique IDs for documents\n",
    "uuids = [str(uuid4()) for _ in split_docs[:30]]\n",
    "page_contents = [doc.page_content for doc in split_docs[:30]]\n",
    "metadatas = [doc.metadata for doc in split_docs[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4c38e582-bf13-473d-8878-720ea6fe33d2',\n",
       " '6f8d264f-1408-426c-a0d9-0ba007d4df8a',\n",
       " '83f6383c-6871-4731-8610-d128df5b3b9d',\n",
       " 'f93cd8e8-99e5-4646-a8cc-8813abf4d242',\n",
       " '23612cc9-8e71-41fe-b686-76e95c351b4f',\n",
       " '4d597d0c-a969-4e12-a054-f0c39fc59cfe',\n",
       " '9a065986-b60b-4e35-bedb-2775bfd65a77',\n",
       " '12e0ade9-1dcd-4fd7-b647-e98e1ac6ec16',\n",
       " '333d233d-4887-4329-8562-250c94a5f798',\n",
       " '49a716c0-490c-4d01-a843-a23844a81c9f',\n",
       " '480a41eb-ed28-4019-8c6b-f957f4227cc3',\n",
       " '52104bf1-ba19-4623-80d1-717ce0e0166a',\n",
       " 'de662d15-e5d5-468c-b697-5802f7a6c7b9',\n",
       " '06cb5630-55ab-46cb-bd5a-39a48ef4744d',\n",
       " 'c58e0281-aa5e-4a9e-93d7-03b1e1082077',\n",
       " 'b61cc1a4-2298-4bc7-baea-9114d1f93f2e',\n",
       " '0aa66408-db7e-4588-a2f1-6907887f656d',\n",
       " 'ae87a13f-03fb-4cf9-ac2b-70c44498c5c8',\n",
       " '9e1afbfd-f549-4f09-8ad1-e151694741bc',\n",
       " 'f1af56ba-8c3c-4cc4-8eda-4e73e9d20151',\n",
       " '986bbb57-e61b-44ec-b043-b64b70cf35d6',\n",
       " 'fcae7694-8d48-49ca-8320-50df1dc93cf8',\n",
       " '3fc036b7-97a0-4fec-b61f-68ccb0fca786',\n",
       " 'a1b56a7e-cb74-40df-ad48-2163387f6205',\n",
       " 'c4babef1-0788-400a-b98c-1b897cb9d474',\n",
       " '89a2211a-228e-4e17-97eb-c2e0ecad2912',\n",
       " '1c58e76e-46c5-49ea-9975-3c30237fdd52',\n",
       " '40f64d41-455e-481a-a8a5-566cbfefd1d7',\n",
       " '8a70c555-2196-41bf-b72e-69d7e4526943',\n",
       " '470f8d53-9530-456d-9f54-2ebf0e1af1e1']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.qdrant_interface import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "db.upsert(texts=page_contents, metadatas=metadatas, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Items from the Vector Store\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to delete items from your vector store using the `delete` method. You can specify items to delete by providing IDs or filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_ids = [uuids[0]]\n",
    "\n",
    "db.delete(ids=delete_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert items to vector store (parallel)\n",
    "\n",
    "The `QdrantDocumentManager` class supports parallel upserts using the `upsert_parallel` method. This efficiently adds or updates multiple items with unique IDs, data, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6623fd13-cd54-4181-b657-90911b22de3a',\n",
       " '2cae00f3-a540-4217-89e8-ceaff6bb7a50',\n",
       " 'bd101408-17d4-4dc9-bc71-e8cc3d2c76b5',\n",
       " 'bc5075eb-f872-4fc5-a889-c6233853630f',\n",
       " 'db065742-d85c-4a3f-ba98-95eaff06c7dc',\n",
       " '80873271-0293-4281-97ad-71b228fe1a39',\n",
       " '62d809cf-d7a3-49d5-b34e-bae921995812',\n",
       " '22b47ea5-fd70-412b-9722-31b4ea544784',\n",
       " 'f1658894-3e9d-4416-8187-712f4368746c',\n",
       " '192af61b-3da0-4dd9-9138-d441cd07df38',\n",
       " '02703d74-7f10-4805-ab8a-67cc1e04bee5',\n",
       " '35393884-9987-4fea-ae27-f0fbe01d4cb7',\n",
       " '25d6f448-cbe9-4f22-bab4-be345a3f7190',\n",
       " '81ab337a-b5fd-4dc5-b97b-09c3373ac900',\n",
       " '1e1e2d77-31c2-468d-bb05-b34162216da0',\n",
       " 'e4b79469-582b-42d3-b315-c6f9b6483d33',\n",
       " 'e761c8fc-cd3e-4c31-98c4-20958ad6c256',\n",
       " 'b1010b30-4bbc-4872-af41-49ea7152a81e',\n",
       " '29089ae7-25cc-47b9-aa57-2e5484befbe0',\n",
       " '08470c7f-5c7b-48b6-8bc4-8606f959059e',\n",
       " 'fc2685a0-cc0b-46ef-8872-b5bb75f2fb7f',\n",
       " '5c487bdf-b145-4157-a6d9-55aae753e97e',\n",
       " '9069c953-a9f5-4146-9293-569430998ae6',\n",
       " '06cb50bc-77d5-4d2f-af61-628cde8f5695',\n",
       " '3aa4f987-77f5-4ec0-a398-945d7c8073bf',\n",
       " 'befda6cd-2dee-4d43-a57f-785f1427721c',\n",
       " 'c58ae7a5-281b-4c58-9ac9-defac63db7b8',\n",
       " 'a00d5bd5-08de-444c-8a05-6ef232d1e688',\n",
       " 'a1a78a7c-0064-4a9c-a2ec-d7c48c2f9028',\n",
       " '8f0c0142-10c6-4893-bd78-114805a2601b']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate unique IDs for documents\n",
    "uuids = [str(uuid4()) for _ in split_docs[30:60]]\n",
    "page_contents = [doc.page_content for doc in split_docs[30:60]]\n",
    "metadatas = [doc.metadata for doc in split_docs[30:60]]\n",
    "\n",
    "db.upsert_parallel(\n",
    "    texts=page_contents,\n",
    "    metadatas=metadatas,\n",
    "    ids=uuids,\n",
    "    batch_size=32,\n",
    "    workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query VectorStore\n",
    "\n",
    "Once your vector store has been created and the relevant documents have been added you will most likely wish to query it during the running of your chain or agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query directly\n",
    "\n",
    "The `QdrantDocumentManager` class allows direct querying using the `search` method. It performs similarity searches by converting queries into vector embeddings to find similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets-- good plants and bad plants. In consequence, there were good seeds from good plants, and bad seeds fro\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* [ Chapter 7 ]\n",
      "- the narrator learns about the secret of the little prince‘s life \n",
      "On the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed \n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "response = db.search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in response:\n",
    "    payload = res[\"payload\"]\n",
    "    print(f\"* {payload['page_content'][:200]}\\n [{payload['metadata']}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search with score\n",
    "\n",
    "The `QdrantDocumentManager` class enables similarity searches with scores using the `search` method. This provides a relevance score for each document found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.527] for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* [SIM=0.499] Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets-- good plants and bad plants. In consequence, there were good seeds from good plants, and bad seeds fro\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* [SIM=0.478] [ Chapter 7 ]\n",
      "- the narrator learns about the secret of the little prince‘s life \n",
      "On the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed \n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query to search in the database\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Perform the search with the specified query and number of results\n",
    "response = db.search(query=query, k=3)\n",
    "\n",
    "for res in response:\n",
    "    payload = res[\"payload\"]\n",
    "    score = res[\"score\"]\n",
    "    print(\n",
    "        f\"* [SIM={score:.3f}] {payload['page_content'][:200]}\\n [{payload['metadata']}]\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by turning into retreiver\n",
    "\n",
    "The `QdrantDocumentManager` class can transform the vector store into a `retriever`. This allows for easier integration into workflows or chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': '18db9172-aa34-4e48-a580-ada0d9a78fd1', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n",
      "* Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets-- good plants and bad plants. In consequence, there were good seeds from good plants, and bad seeds fro\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'b61cc1a4-2298-4bc7-baea-9114d1f93f2e', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n",
      "* [ Chapter 7 ]\n",
      "- the narrator learns about the secret of the little prince‘s life \n",
      "On the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed \n",
      " [{'source': './data/the_little_prince.txt', '_id': '1c58e76e-46c5-49ea-9975-3c30237fdd52', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Initialize QdrantVectorStore with the client, collection name, and embedding\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=db.client, collection_name=db.collection_name, embedding=db.embedding\n",
    ")\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Transform the vector store into a retriever with specific search parameters\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3},\n",
    ")\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with Filtering\n",
    "\n",
    "The `QdrantDocumentManager` class allows searching with filters to retrieve records based on specific metadata values. This is done using the `scroll` method with a defined filter query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(id='1c58e76e-46c5-49ea-9975-3c30237fdd52', payload={'page_content': '[ Chapter 7 ]\\n- the narrator learns about the secret of the little prince‘s life \\nOn the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed to me. Abruptly, without anything to lead up to it, and as if the question had been born of long and silent meditation on his problem, he demanded: \\n\"A sheep-- if it eats little bushes, does it eat flowers, too?\"\\n\"A sheep,\" I answered, \"eats anything it finds in its reach.\"\\n\"Even flowers that have thorns?\"\\n\"Yes, even flowers that have thorns.\" \\n\"Then the thorns-- what use are they?\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='25d6f448-cbe9-4f22-bab4-be345a3f7190', payload={'page_content': '[ Chapter 5 ]\\n- we are warned as to the dangers of the baobabs\\nAs each day passed I would learn, in our talk, something about the little prince‘s planet, his departure from it, his journey. The information would come very slowly, as it might chance to fall from his thoughts. It was in this way that I heard, on the third day, about the catastrophe of the baobabs.\\nThis time, once more, I had the sheep to thank for it. For the little prince asked me abruptly-- as if seized by a grave doubt-- \"It is true, isn‘t it, that sheep eat little bushes?\" \\n\"Yes, that is true.\" \\n\"Ah! I am glad!\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='3fc036b7-97a0-4fec-b61f-68ccb0fca786', payload={'page_content': '[ Chapter 6 ]\\n- the little prince and the narrator talk about sunsets\\nOh, little prince! Bit by bit I came to understand the secrets of your sad little life... For a long time you had found your only entertainment in the quiet pleasure of looking at the sunset. I learned that new detail on the morning of the fourth day, when you said to me: \\n\"I am very fond of sunsets. Come, let us go look at a sunset now.\" \\n\"But we must wait,\" I said. \\n\"Wait? For what?\" \\n\"For the sunset. We must wait until it is time.\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='6623fd13-cd54-4181-b657-90911b22de3a', payload={'page_content': '[ Chapter 4 ]\\n- the narrator speculates as to which asteroid from which the little prince came\\u3000\\u3000\\nI had thus learned a second fact of great importance: this was that the planet the little prince came from was scarcely any larger than a house!', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='9069c953-a9f5-4146-9293-569430998ae6', payload={'page_content': '[ Chapter 6 ]\\n- the little prince and the narrator talk about sunsets\\nOh, little prince! Bit by bit I came to understand the secrets of your sad little life... For a long time you had found your only entertainment in the quiet pleasure of looking at the sunset. I learned that new detail on the morning of the fourth day, when you said to me: \\n\"I am very fond of sunsets. Come, let us go look at a sunset now.\" \\n\"But we must wait,\" I said. \\n\"Wait? For what?\" \\n\"For the sunset. We must wait until it is time.\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='c58ae7a5-281b-4c58-9ac9-defac63db7b8', payload={'page_content': '[ Chapter 7 ]\\n- the narrator learns about the secret of the little prince‘s life \\nOn the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed to me. Abruptly, without anything to lead up to it, and as if the question had been born of long and silent meditation on his problem, he demanded: \\n\"A sheep-- if it eats little bushes, does it eat flowers, too?\"\\n\"A sheep,\" I answered, \"eats anything it finds in its reach.\"\\n\"Even flowers that have thorns?\"\\n\"Yes, even flowers that have thorns.\" \\n\"Then the thorns-- what use are they?\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='de662d15-e5d5-468c-b697-5802f7a6c7b9', payload={'page_content': '[ Chapter 5 ]\\n- we are warned as to the dangers of the baobabs\\nAs each day passed I would learn, in our talk, something about the little prince‘s planet, his departure from it, his journey. The information would come very slowly, as it might chance to fall from his thoughts. It was in this way that I heard, on the third day, about the catastrophe of the baobabs.\\nThis time, once more, I had the sheep to thank for it. For the little prince asked me abruptly-- as if seized by a grave doubt-- \"It is true, isn‘t it, that sheep eat little bushes?\" \\n\"Yes, that is true.\" \\n\"Ah! I am glad!\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Define a filter query to match documents containing the text \"Chapter\" in the page content\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"page_content\",\n",
    "            match=models.MatchText(text=\"Chapter\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve records from the collection that match the filter query\n",
    "db.scroll(\n",
    "    scroll_filter=filter_query,\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete with Filtering\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to delete records using filters based on specific metadata values. This is achieved with the `delete` method and a filter query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=7, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, MatchText\n",
    "\n",
    "# Define a filter query to match documents containing the text \"Chapter\" in the page content\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"page_content\",\n",
    "            match=models.MatchText(text=\"Chapter\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Delete records from the collection that match the filter query\n",
    "db.client.delete(collection_name=db.collection_name, points_selector=filter_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Updating Records\n",
    "\n",
    "The `QdrantDocumentManager` class supports filtering and updating records based on specific metadata values. This is done by retrieving records with filters and updating them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Define a filter query to match documents with a specific metadata source\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"metadata.source\",\n",
    "            match=models.MatchValue(value=\"./data/the_little_prince.txt\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve records matching the filter query, including their vectors\n",
    "response = db.scroll(scroll_filter=filter_query, k=10, with_vectors=True)\n",
    "new_source = \"the_little_prince.txt\"\n",
    "\n",
    "# Update the point IDs and set new metadata for the records\n",
    "for point in response:  # response[0] returns a list of points\n",
    "    payload = point.payload\n",
    "\n",
    "    # Check if metadata exists in the payload\n",
    "    if \"metadata\" in payload:\n",
    "        payload[\"metadata\"][\"source\"] = new_source\n",
    "    else:\n",
    "        payload[\"metadata\"] = {\n",
    "            \"source\": new_source\n",
    "        }  # Add new metadata if it doesn't exist\n",
    "\n",
    "    # Update the point with new metadata\n",
    "    db.client.upsert(\n",
    "        collection_name=db.collection_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=point.id,\n",
    "                payload=payload,\n",
    "                vector=point.vector,\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search Options\n",
    "\n",
    "When using `QdrantVectorStore`, you have three options for performing similarity searches. You can select the desired search mode using the retrieval_mode parameter when you set up the class. The available modes are:\n",
    "\n",
    "- Dense Vector Search (Default)\n",
    "- Sparse Vector Search\n",
    "- Hybrid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Vector Search\n",
    "\n",
    "To perform a search using only dense vectors:\n",
    "\n",
    "The `retrieval_mode` parameter must be set to `RetrievalMode.DENSE`. This is also the default setting.\n",
    "You need to provide a [dense embeddings](https://python.langchain.com/docs/integrations/text_embedding/) value through the embedding parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': '96ed2a69-8749-480a-9fcf-9f4e1ee96e47', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n",
      "* Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets-- good plants and bad plants. In consequence, there were good seeds from good plants, and bad seeds fro\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'c211f71a-6d65-4f4f-a71b-2002ad82a0d5', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n",
      "* \"It is a question of discipline,\" the little prince said to me later on. \"When you‘ve finished your own toilet in the morning, then it is time to attend to the toilet of your planet, just so, with the\n",
      " [{'source': './data/the_little_prince.txt', '_id': '87948c4e-0cd4-4264-a030-f3e61e16bd28', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs[:50],\n",
    "    embedding=embedding,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"dense_collection\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Vector Search\n",
    "\n",
    "To search with only sparse vectors,\n",
    "\n",
    "The `retrieval_mode` parameter should be set to `RetrievalMode.SPARSE` .\n",
    "An implementation of the [SparseEmbeddings](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any sparse embeddings provider has to be provided as value to the `sparse_embedding` parameter.\n",
    "The `langchain-qdrant` package provides a FastEmbed based implementation out of the box.\n",
    "\n",
    "To use it, install the [FastEmbed](https://github.com/qdrant/fastembed) package.\n",
    "\n",
    "pip install fastembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [ Chapter 20 ]\n",
      "- the little prince discovers a garden of roses\n",
      "But it happened that after walking for a long time through sand, and rocks, and snow, the little prince at last came upon a road. And all\n",
      " [{'source': './data/the_little_prince.txt', '_id': '54eaf36e-851d-4548-907c-81451f60a003', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n",
      "* And he went back to meet the fox. \n",
      "\"Goodbye,\" he said. \n",
      "\"Goodbye,\" said the fox. \"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'b6a7acba-319c-40a8-bcfa-05091c2186c5', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n",
      "* \"The men where you live,\" said the little prince, \"raise five thousand roses in the same garden-- and they do not find in it what they are looking for.\" \n",
      "\"They do not find it,\" I replied. \n",
      "\"And yet wh\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'f8a4b68b-9ed1-4c9b-b84b-986862189366', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# Initialize sparse embeddings using FastEmbedSparse\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"sparse_collection\",\n",
    "    retrieval_mode=RetrievalMode.SPARSE,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Vector Search\n",
    "To perform a hybrid search using dense and sparse vectors with score fusion,\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.HYBRID` .\n",
    "- A [ `dense embeddings` ](https://python.langchain.com/docs/integrations/text_embedding/) value should be provided to the `embedding` parameter.\n",
    "- An implementation of the [ `SparseEmbeddings` ](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any sparse embeddings provider has to be provided as value to the `sparse_embedding` parameter.\n",
    "\n",
    "Note that if you've added documents with the `HYBRID` mode, you can switch to any retrieval mode when searching. Since both the dense and sparse vectors are available in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [ Chapter 20 ]\n",
      "- the little prince discovers a garden of roses\n",
      "But it happened that after walking for a long time through sand, and rocks, and snow, the little prince at last came upon a road. And all\n",
      " [{'source': './data/the_little_prince.txt', '_id': '3c84f2d7-234b-416d-a417-32d6311a0844', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n",
      "* \"Go and look again at the roses. You will understand now that yours is unique in all the world. Then come back to say goodbye to me, and I will make you a present of a secret.\" \n",
      "The little prince went\n",
      " [{'source': './data/the_little_prince.txt', '_id': '7d4529d7-3d4d-43f4-85c9-35a11f1c77c2', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n",
      "* [ Chapter 8 ]\n",
      "- the rose arrives at the little prince‘s planet\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'a4cc316b-34aa-4205-b820-253548d164a6', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# Initialize sparse embeddings using FastEmbedSparse\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"hybrid_collection\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-6aJyhYW2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
