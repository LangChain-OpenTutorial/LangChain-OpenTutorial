{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch\n",
    "\n",
    "- Author: [liniar](https://github.com/namyoungkim)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "- This tutorial is designed for beginners to get started with Elasticsearch and its integration with LangChain.\n",
    "- You‚Äôll learn how to set up the environment, prepare data, and explore advanced search features like hybrid and semantic search.\n",
    "- By the end, you‚Äôll be equipped to use Elasticsearch for powerful and intuitive search applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Elasticsearch Setup](#elasticsearch-setup)\n",
    "- [Introduction to Elasticsearch](#introduction-to-elasticsearch)\n",
    "- [ElasticsearchManager](#elasticsearchmanager)\n",
    "- [Data Preparation for Tutorial](#data-preparation-for-tutorial)\n",
    "- [Initialization](#initialization)\n",
    "- [DB Handling](#db-handling)\n",
    "- [Advanced Search](#advanced-search)\n",
    "- [Managing Elasticsearch Connections and Documents](#managing-elasticsearch-connections-and-documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [LangChain VectorStore Documentation](https://python.langchain.com/docs/how_to/vectorstores/)\n",
    "- [LangChain Elasticsearch Integration](https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/)\n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup  \n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.  \n",
    "\n",
    "**[Note]**  \n",
    "- `langchain-opentutorial` is a package that provides a set of **easy-to-use environment setup,** **useful functions,** and **utilities for tutorials.**  \n",
    "- You can check out the [`langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.  \n",
    "\n",
    "\n",
    "### üõ†Ô∏è **The following configurations will be set up**  \n",
    "\n",
    "- **Jupyter Notebook Output Settings**\n",
    "    - Display standard error ( `stderr` ) messages directly instead of capturing them.  \n",
    "- **Install Required Packages** \n",
    "    - Ensure all necessary dependencies are installed.  \n",
    "- **API Key Setup** \n",
    "    - Configure the API key for authentication.  \n",
    "- **PyTorch Device Selection Setup** \n",
    "    - Automatically select the optimal computing device (CPU, CUDA, or MPS).\n",
    "        - `{\"device\": \"mps\"}` : Perform embedding calculations using **MPS** instead of GPU. (For Mac users)\n",
    "        - `{\"device\": \"cuda\"}` : Perform embedding calculations using **GPU.** (For Linux and Windows users, requires CUDA installation)\n",
    "        - `{\"device\": \"cpu\"}` : Perform embedding calculations using **CPU.** (Available for all users)\n",
    "- **Embedding Model Local Storage Path** \n",
    "    - Define a local path for storing embedding models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Setup\n",
    "- In order to use the Elasticsearch vector search you must install the langchain-elasticsearch package.\n",
    "\n",
    "### üöÄ Setting Up Elasticsearch with Elastic Cloud (Colab Compatible)\n",
    "- Elastic Cloud allows you to manage Elasticsearch seamlessly in the cloud, eliminating the need for local installations.\n",
    "- It integrates well with Google Colab, enabling efficient experimentation and prototyping.\n",
    "\n",
    "\n",
    "### üìö What is Elastic Cloud?  \n",
    "- **Elastic Cloud** is a managed Elasticsearch service provided by Elastic.  \n",
    "- Supports **custom cluster configurations** and **auto-scaling.** \n",
    "- Deployable on **AWS**, **GCP**, and **Azure.**  \n",
    "- Compatible with **Google Colab,** allowing simplified cloud-based workflows.  \n",
    "\n",
    "### üìå Getting Started with Elastic Cloud  \n",
    "1. **Sign up for Elastic Cloud‚Äôs Free Trial.**  \n",
    "    - [Free Trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation)\n",
    "2. **Create an Elasticsearch Cluster.**  \n",
    "3. **Retrieve your Elasticsearch URL** and **Elasticsearch API Key** from the Elastic Cloud Console.  \n",
    "4. Add the following to your `.env` file\n",
    "    > ```\n",
    "    > ES_URL=https://my-elasticsearch-project-abd...:123\n",
    "    > ES_API_KEY=bk9X...\n",
    "    > ```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n9NVKk-Zf9Nq"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IMx2hZNXf9QL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-core\",\n",
    "        \"langchain_huggingface\",\n",
    "        \"langchain_elasticsearch\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"elasticsearch\",\n",
    "        \"python-dotenv\",\n",
    "        \"uuid\",\n",
    "        \"torch\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from dotenv import load_dotenv\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
    "if not load_dotenv():\n",
    "    set_env(\n",
    "        {\n",
    "            \"OPENAI_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "            \"LANGCHAIN_PROJECT\": \"Elasticsearch\",\n",
    "            \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
    "            \"ES_URL\": \"\",\n",
    "            \"ES_API_KEY\": \"\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using MPS (Metal Performance Shaders) on macOS\n",
      "üñ•Ô∏è Current device in use: mps\n"
     ]
    }
   ],
   "source": [
    "# Automatically select the appropriate device\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if platform.system() == \"Darwin\":  # macOS specific\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
    "            return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"‚úÖ Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = get_device()\n",
    "print(\"üñ•Ô∏è Current device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N8C6pLTZf9Sb"
   },
   "outputs": [],
   "source": [
    "# Embedding Model Local Storage Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the download path to ./cache/\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Elasticsearch\n",
    "- Elasticsearch is an open-source, distributed search and analytics engine designed to store, search, and analyze both structured and unstructured data in real-time.\n",
    "\n",
    "### üìå Key Features  \n",
    "- **Real-Time Search:** Instantly searchable data upon ingestion  \n",
    "- **Large-Scale Data Processing:** Efficient handling of vast datasets  \n",
    "- **Scalability:** Flexible scaling through clustering and distributed architecture  \n",
    "- **Versatile Search Support:** Keyword search, semantic search, and multimodal search  \n",
    "\n",
    "### üìå Use Cases  \n",
    "- **Log Analytics:** Real-time monitoring of system and application logs  \n",
    "- **Monitoring:** Server and network health tracking  \n",
    "- **Product Recommendations:** Behavior-based recommendation systems  \n",
    "- **Natural Language Processing (NLP):** Semantic text searches  \n",
    "- **Multimodal Search:** Text-to-image and image-to-image searches  \n",
    "\n",
    "### üß† Vector Database Functionality in Elasticsearch  \n",
    "- Elasticsearch supports vector data storage and similarity search via **Dense Vector Fields.** As a vector database, it excels in applications like NLP, image search, and recommendation systems.\n",
    "\n",
    "### üìå Core Vector Database Features  \n",
    "- **Dense Vector Field:** Store and query high-dimensional vectors  \n",
    "- **KNN (k-Nearest Neighbors) Search:** Find vectors most similar to the input  \n",
    "- **Semantic Search:** Perform meaning-based searches beyond keyword matching  \n",
    "- **Multimodal Search:** Combine text and image data for advanced search capabilities  \n",
    "\n",
    "### üìå Vector Search Use Cases  \n",
    "- **Semantic Search:** Understand user intent and deliver precise results  \n",
    "- **Text-to-Image Search:** Retrieve relevant images from textual descriptions  \n",
    "- **Image-to-Image Search:** Find visually similar images in a dataset  \n",
    "\n",
    "### üîó Official Documentation Links  \n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)  \n",
    "\n",
    "Elasticsearch goes beyond traditional text search engines, offering robust vector database capabilities essential for NLP and multimodal search applications. üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticsearchManager\n",
    "- `Purpose:` Simplifies interactions with Elasticsearch, allowing easy management of indices and documents through user-friendly methods.\n",
    "- `Core Features` \n",
    "\t- `Index management:` create, delete, and manage indices.\n",
    "\t- `Document operations:` upsert, retrieve, search, and delete documents.\n",
    "\t- `Bulk and parallel operations:` perform upserts in bulk or in parallel for high performance.\n",
    "\n",
    "### Methods and Parameters\n",
    "\n",
    "1. `__init__` \n",
    "\t- Role: Initializes the ElasticsearchManager instance and connects to the Elasticsearch cluster.\n",
    "\t- Parameters\n",
    "\t\t- `es_url` (str): The URL of the Elasticsearch host (default: \"http://localhost:9200\").\n",
    "\t\t- `api_key` (Optional[str]): The API key for authentication (default: None).\n",
    "\t- Behavior\n",
    "\t\t- Establishes a connection to Elasticsearch.\n",
    "\t\t- Tests the connection using ping() and raises a ConnectionError if it fails.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager = ElasticsearchManager(es_url=\"http://localhost:9200\")\n",
    "\t\t>```\n",
    "\n",
    "2. `create_index` \n",
    "\t- Role: Creates an Elasticsearch index with optional mappings and settings.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to create.\n",
    "\t\t- `mapping` (Optional[Dict]): A dictionary defining the index structure (field types, properties, etc.).\n",
    "\t\t- `settings` (Optional[Dict]): A dictionary defining index settings (e.g., number of shards, replicas).\n",
    "\t- Behavior\n",
    "\t\t- Checks if the index exists.\n",
    "\t\t- If the index does not exist, creates it using the provided mappings and settings.\n",
    "\t- Returns: A string message indicating success or failure.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>mapping = {\"properties\": {\"name\": {\"type\": \"text\"}}}\n",
    "\t\t>settings = {\"number_of_shards\": 1}\n",
    "\t\t>es_manager.create_index(\"my_index\", mapping=mapping, settings=settings)\n",
    "\t\t>```\n",
    "\n",
    "3. `delete_index` \n",
    "\t- Role: Deletes an Elasticsearch index if it exists.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to delete.\n",
    "\t- Behavior\n",
    "\t\t- Checks if the index exists.\n",
    "\t\t- Deletes the index if it exists.\n",
    "\t- Returns: A string message indicating success or failure.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.delete_index(\"my_index\")\n",
    "\t\t```\n",
    "\n",
    "4. `get_document` \n",
    "\t- Role: Retrieves a single document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to retrieve the document from.\n",
    "\t\t- `document_id` (str): The ID of the document to retrieve.\n",
    "\t- Behavior\n",
    "\t\t- Fetches the document using its ID.\n",
    "\t\t- Returns the _source field of the document (its contents).\n",
    "\t- Returns: The document contents (Dict) if found, otherwise None.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>document = es_manager.get_document(\"my_index\", \"1\")\n",
    "\t\t>```\n",
    "\n",
    "5. `search_documents` \n",
    "\t- Role: Searches for documents in an index based on a query.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The name of the index to search.\n",
    "\t\t- `query` (Dict): A query in Elasticsearch DSL format.\n",
    "\t- Behavior\n",
    "\t\t- Executes the query against the specified index.\n",
    "\t\t- Returns the _source field of all matching documents.\n",
    "\t- Returns: A list of matching documents (List[Dict]).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>query = {\"match\": {\"name\": \"John\"}}\n",
    "\t\t>results = es_manager.search_documents(\"my_index\", query=query)\n",
    "\t\t>```\n",
    "\t\t\n",
    "6. `upsert_document` \n",
    "\t- Role: Inserts or updates a document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index to perform the upsert on.\n",
    "\t\t- `document_id` (str): The ID of the document to upsert.\n",
    "\t\t- `document` (Dict): The content of the document.\n",
    "\t- Behavior\n",
    "\t\t- Updates the document if it exists or creates it if it does not.\n",
    "\t\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>document = {\"name\": \"Alice\", \"age\": 30}\n",
    "\t\t>es_manager.upsert_document(\"my_index\", \"1\", document)\n",
    "\t\t>```\n",
    "\n",
    "7. `bulk_upsert` \n",
    "\t- Role: Performs a bulk upsert operation for multiple documents.\n",
    "\t- Parameters\n",
    "\t\t- `documents` (List[Dict]): A list of documents for the bulk operation.\n",
    "\t\t\t- Each document should specify _index, _id, _op_type, and doc_as_upsert.\n",
    "\t- Behavior\n",
    "\t\t- Uses Elasticsearch‚Äôs bulk API to upsert multiple documents in a single request.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>docs = [\n",
    "\t\t>\t{\"_index\": \"my_index\", \"_id\": \"1\", \"_op_type\": \"update\", \"doc\": {\"name\": \"Alice\"}, \"doc_as_upsert\": True},\n",
    "\t\t>\t{\"_index\": \"my_index\", \"_id\": \"2\", \"_op_type\": \"update\", \"doc\": {\"name\": \"Bob\"}, \"doc_as_upsert\": True}\n",
    "\t\t>]\n",
    "\t\t>es_manager.bulk_upsert(docs)\n",
    "\t\t>```\n",
    "\n",
    "8. `parallel_bulk_upsert` \n",
    "\t- Role: Performs a parallelized bulk upsert operation for large datasets.\n",
    "\t- Parameters\n",
    "\t\t- `documents` (List[Dict]): A list of documents for bulk upserts.\n",
    "\t\t- `batch_size` (int): Number of documents per batch (default: 100).\n",
    "\t\t- `max_workers` (int): Number of threads to use for parallel processing (default: 4).\n",
    "\t- Behavior\n",
    "\t\t- Splits the documents into batches and processes them in parallel using threads.\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.parallel_bulk_upsert(docs, batch_size=50, max_workers=4)\n",
    "\t\t>```\n",
    "\n",
    "9. `delete_document` \n",
    "\t- Role: Deletes a single document by its ID.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index containing the document.\n",
    "\t\t- `document_id` (str): The ID of the document to delete.\n",
    "\t- Behavior\n",
    "\t\t- Deletes the specified document using its ID.\n",
    "\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>es_manager.delete_document(\"my_index\", \"1\")\n",
    "\t\t>```\n",
    "\n",
    "10. `delete_by_query` \n",
    "\t- Role: Deletes all documents that match a query.\n",
    "\t- Parameters\n",
    "\t\t- `index_name` (str): The index to delete documents from.\n",
    "\t\t- `query` (Dict): The query defining the documents to delete.\n",
    "\t- Behavior\n",
    "\t\t- Uses Elasticsearch‚Äôs delete_by_query API to remove documents matching the query.\n",
    "\t- Returns: The Elasticsearch response (Dict).\n",
    "\t- Usage Example\n",
    "\t\t>```python\n",
    "\t\t>delete_query = {\"match\": {\"status\": \"inactive\"}}\n",
    "\t\t>es_manager.delete_by_query(\"my_index\", query=delete_query)\n",
    "\t\t>```\n",
    "\n",
    "### Conclusion\n",
    "- This class provides a robust and user-friendly interface to manage Elasticsearch operations.\n",
    "- It encapsulates common tasks like creating indices, searching for documents, and performing upserts, making it ideal for use in data management pipelines or applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, List, Generator\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "class ElasticsearchManager:\n",
    "    def __init__(\n",
    "        self, es_url: str = \"http://localhost:9200\", api_key: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ElasticsearchManager with a connection to the Elasticsearch instance.\n",
    "\n",
    "        Parameters:\n",
    "            es_url (str): URL of the Elasticsearch host.\n",
    "            api_key (Optional[str]): API key for authentication (optional).\n",
    "        \"\"\"\n",
    "        # Initialize the Elasticsearch client\n",
    "        if api_key:\n",
    "            self.es = Elasticsearch(\n",
    "                es_url, api_key=api_key, timeout=120, retry_on_timeout=True\n",
    "            )\n",
    "        else:\n",
    "            self.es = Elasticsearch(es_url, timeout=120, retry_on_timeout=True)\n",
    "\n",
    "        # Test connection\n",
    "        if self.es.ping():\n",
    "            print(\"‚úÖ Successfully connected to Elasticsearch!\")\n",
    "        else:\n",
    "            raise ConnectionError(\"‚ùå Failed to connect to Elasticsearch.\")\n",
    "\n",
    "    def create_index(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        mapping: Optional[Dict] = None,\n",
    "        settings: Optional[Dict] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create an Elasticsearch index with optional mapping and settings.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Name of the index to create.\n",
    "            mapping (Optional[Dict]): Mapping definition for the index.\n",
    "            settings (Optional[Dict]): Settings definition for the index.\n",
    "\n",
    "        Returns:\n",
    "            str: Success or warning message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not self.es.indices.exists(index=index_name):\n",
    "                body = {}\n",
    "                if mapping:\n",
    "                    body[\"mappings\"] = mapping\n",
    "                if settings:\n",
    "                    body[\"settings\"] = settings\n",
    "                self.es.indices.create(index=index_name, body=body)\n",
    "                return f\"‚úÖ Index '{index_name}' created successfully.\"\n",
    "            else:\n",
    "                return f\"‚ö†Ô∏è Index '{index_name}' already exists. Skipping creation.\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error creating index '{index_name}': {e}\"\n",
    "\n",
    "    def delete_index(self, index_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Delete an Elasticsearch index if it exists.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Name of the index to delete.\n",
    "\n",
    "        Returns:\n",
    "            str: Success or warning message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.es.indices.exists(index=index_name):\n",
    "                self.es.indices.delete(index=index_name)\n",
    "                return f\"‚úÖ Index '{index_name}' deleted successfully.\"\n",
    "            else:\n",
    "                return f\"‚ö†Ô∏è Index '{index_name}' does not exist.\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error deleting index '{index_name}': {e}\"\n",
    "\n",
    "    def get_document(self, index_name: str, document_id: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve a single document by its ID.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to retrieve the document from.\n",
    "            document_id (str): The ID of the document to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict]: The document's content if found, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.get(index=index_name, id=document_id)\n",
    "            return response[\"_source\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error retrieving document: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_documents(self, index_name: str, query: Dict) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for documents based on a query.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to search.\n",
    "            query (Dict): The query body for the search.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: List of documents that match the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.search(index=index_name, body={\"query\": query})\n",
    "            return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def upsert_document(\n",
    "        self, index_name: str, document_id: str, document: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform an upsert operation on a single document.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to perform the upsert on.\n",
    "            document_id (str): The ID of the document.\n",
    "            document (Dict): The document content to upsert.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.update(\n",
    "                index=index_name,\n",
    "                id=document_id,\n",
    "                body={\"doc\": document, \"doc_as_upsert\": True},\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error upserting document: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def bulk_upsert(\n",
    "        self, index_name: str, documents: List[Dict], timeout: Optional[str] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert operation.\n",
    "\n",
    "        Parameters:\n",
    "            index (str): Default index name for the documents.\n",
    "            documents (List[Dict]): List of documents for bulk upsert.\n",
    "            timeout (Optional[str]): Timeout duration (e.g., '60s', '2m'). If None, the default timeout is used.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure each document includes an `_index` field\n",
    "            for doc in documents:\n",
    "                if \"_index\" not in doc:\n",
    "                    doc[\"_index\"] = index_name\n",
    "\n",
    "            # Perform the bulk operation\n",
    "            helpers.bulk(self.es, documents, timeout=timeout)\n",
    "            print(\"‚úÖ Bulk upsert completed successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in bulk upsert: {e}\")\n",
    "\n",
    "    def parallel_bulk_upsert(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        documents: List[Dict],\n",
    "        batch_size: int = 100,\n",
    "        max_workers: int = 4,\n",
    "        timeout: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform a parallel bulk upsert operation.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): Default index name for documents.\n",
    "            documents (List[Dict]): List of documents for bulk upsert.\n",
    "            batch_size (int): Number of documents per batch.\n",
    "            max_workers (int): Number of parallel threads.\n",
    "            timeout (Optional[str]): Timeout duration (e.g., '60s', '2m'). If None, the default timeout is used.\n",
    "        \"\"\"\n",
    "\n",
    "        def chunk_data(\n",
    "            data: List[Dict], chunk_size: int\n",
    "        ) -> Generator[List[Dict], None, None]:\n",
    "            \"\"\"Split data into chunks.\"\"\"\n",
    "            for i in range(0, len(data), chunk_size):\n",
    "                yield data[i : i + chunk_size]\n",
    "\n",
    "        # Ensure each document has an `_index` field\n",
    "        for doc in documents:\n",
    "            if \"_index\" not in doc:\n",
    "                doc[\"_index\"] = index_name\n",
    "\n",
    "        batches = list(chunk_data(documents, batch_size))\n",
    "\n",
    "        def bulk_upsert_batch(batch: List[Dict]):\n",
    "            helpers.bulk(self.es, batch, timeout=timeout)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for batch in batches:\n",
    "                executor.submit(bulk_upsert_batch, batch)\n",
    "\n",
    "    def delete_document(self, index_name: str, document_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Delete a single document by its ID.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to delete the document from.\n",
    "            document_id (str): The ID of the document to delete.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.delete(index=index_name, id=document_id)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting document: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def delete_by_query(self, index_name: str, query: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Delete documents based on a query.\n",
    "\n",
    "        Parameters:\n",
    "            index_name (str): The index to delete documents from.\n",
    "            query (Dict): The query body for the delete operation.\n",
    "\n",
    "        Returns:\n",
    "            Dict: The response from Elasticsearch.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.es.delete_by_query(\n",
    "                index=index_name, body={\"query\": query}, conflicts=\"proceed\"\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error deleting documents by query: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Tutorial\n",
    "- Let‚Äôs process **The Little Prince** using the `RecursiveCharacterTextSplitter` to create document chunks.\n",
    "- Then, we‚Äôll generate embeddings for each text chunk and store the resulting data in a vector database to proceed with a vector database tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Little Prince\\nWritten By Antoine de Saiot-Exupery (1900„Äú1944)', '[ Antoine de Saiot-Exupery ]']\n",
      "Total number of chunks: 1359\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Function to read text from a file (Cross-Platform)\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            # Normalize line endings (compatible with Windows, macOS, Linux)\n",
    "            raw_text = f.read().replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        return raw_text\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise ValueError(f\"Failed to decode the file with UTF-8 encoding: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The specified file was not found: {file_path}\")\n",
    "\n",
    "\n",
    "# Function to split the text into chunks\n",
    "def split_text(raw_text, chunk_size=100, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,  # Default string length function\n",
    "        is_separator_regex=False,  # Default separator setting\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents([raw_text])\n",
    "    return [doc.page_content for doc in split_docs]\n",
    "\n",
    "\n",
    "# Set file path and execute\n",
    "file_path = \"./data/the_little_prince.txt\"\n",
    "try:\n",
    "    # Read the file\n",
    "    raw_text = read_text_file(file_path)\n",
    "    # Split the text\n",
    "    docs = split_text(raw_text)\n",
    "\n",
    "    # Verify output\n",
    "    print(docs[:2])  # Print the first 5 chunks\n",
    "    print(f\"Total number of chunks: {len(docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n",
      "1024\n",
      "CPU times: user 7.81 s, sys: 2.37 s, total: 10.2 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## text embedding\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "embedded_documents = hf_embeddings_e5_instruct.embed_documents(docs)\n",
    "\n",
    "print(len(embedded_documents))\n",
    "print(len(embedded_documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "def prepare_documents_with_ids(\n",
    "    docs: List[str], embedded_documents: List[List[float]]\n",
    ") -> Tuple[List[Dict], List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare a list of documents with unique IDs and their corresponding embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        docs (List[str]): List of document texts.\n",
    "        embedded_documents (List[List[float]]): List of embedding vectors corresponding to the documents.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[Dict], List[str]]: A tuple containing:\n",
    "            - List of document dictionaries with `doc_id`, `text`, and `vector`.\n",
    "            - List of unique document IDs (`doc_ids`).\n",
    "    \"\"\"\n",
    "    # Generate unique IDs for each document\n",
    "    doc_ids = [str(uuid4()) for _ in range(len(docs))]\n",
    "\n",
    "    # Prepare the document list with IDs, texts, and embeddings\n",
    "    documents = [\n",
    "        {\"doc_id\": doc_id, \"text\": doc, \"vector\": embedding}\n",
    "        for doc, doc_id, embedding in zip(docs, doc_ids, embedded_documents)\n",
    "    ]\n",
    "\n",
    "    return documents, doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, doc_ids = prepare_documents_with_ids(docs, embedded_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Setting Up the Elasticsearch Client\n",
    "- Begin by creating an Elasticsearch client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "ES_URL = os.environ[\"ES_URL\"]  # Elasticsearch host URL\n",
    "ES_API_KEY = os.environ[\"ES_API_KEY\"]  # Elasticsearch API key\n",
    "\n",
    "# Ensure required environment variables are set\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(\"Both ES_URL and ES_API_KEY must be set in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Elasticsearch!\n"
     ]
    }
   ],
   "source": [
    "es_manager = ElasticsearchManager(es_url=ES_URL, api_key=ES_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Handling\n",
    "### Create index\n",
    "- Use the index method to create a new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index\n",
    "index_name = \"langchain_tutorial_es\"\n",
    "\n",
    "# vector dimension\n",
    "dims = len(embedded_documents[0])\n",
    "\n",
    "\n",
    "# üõ†Ô∏è Define the mapping for the new index\n",
    "# This structure specifies the schema for documents stored in Elasticsearch\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"metadata\": {\"properties\": {\"doc_id\": {\"type\": \"keyword\"}}},\n",
    "        \"text\": {\"type\": \"text\"},  # Field for storing textual content\n",
    "        \"vector\": {  # Field for storing vector embeddings\n",
    "            \"type\": \"dense_vector\",  # Specifies dense vector type\n",
    "            \"dims\": dims,  # Number of dimensions in the vector\n",
    "            \"index\": True,  # Enable indexing for vector search\n",
    "            \"similarity\": \"cosine\",  # Use cosine similarity for vector comparisons\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' created successfully.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_manager.create_index(index_name, mapping=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete index\n",
    "- You can delete an index as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_manager.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "- Let‚Äôs perform an upsert operation for **a single document.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'langchain_tutorial_es', '_id': 'ac440534-f1b1-4e3a-a39e-971bc657272e', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let‚Äôs upsert a single document.\n",
    "\n",
    "es_manager.upsert_document(index_name, doc_ids[0], documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read\n",
    "- Retrieve the upserted data using its `doc_id`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac440534-f1b1-4e3a-a39e-971bc657272e\n",
      "The Little Prince\n",
      "Written By Antoine de Saiot-Exupery (1900„Äú1944)\n"
     ]
    }
   ],
   "source": [
    "# get_document\n",
    "result = es_manager.get_document(index_name, doc_ids[0])\n",
    "print(result[\"doc_id\"])\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- Delete using the `doc_id` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_index': 'langchain_tutorial_es', '_id': 'ac440534-f1b1-4e3a-a39e-971bc657272e', '_version': 2, 'result': 'deleted', '_shards': {'total': 2, 'successful': 2, 'failed': 0}, '_seq_no': 1, '_primary_term': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_document\n",
    "es_manager.delete_document(index_name, doc_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Upsert\n",
    "- Perform a bulk upsert of documents.\n",
    "- In general, **‚Äúbulk‚Äù** refers to something large in quantity or volume, often handled or processed all at once.\n",
    "- For example, ‚Äúbulk operations‚Äù involve managing multiple items simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bulk upsert completed successfully.\n",
      "CPU times: user 663 ms, sys: 83.4 ms, total: 746 ms\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_manager.bulk_upsert(index_name, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Bulk Upsert\n",
    "- Perform a bulk upsert of documents in parallel.\n",
    "- **‚Äúparallel‚Äù** refers to tasks or processes happening at the same time or simultaneously, often independently of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 748 ms, sys: 58.1 ms, total: 806 ms\n",
      "Wall time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parallel_bulk_upsert\n",
    "es_manager.parallel_bulk_upsert(index_name, documents, batch_size=100, max_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that parallel_bulk_upsert is **faster.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read (Document Retrieval)\n",
    "- Retrieve documents based on specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "ac440534-f1b1-4e3a-a39e-971bc657272e\n",
      "The Little Prince\n",
      "Written By Antoine de Saiot-Exupery (1900„Äú1944)\n"
     ]
    }
   ],
   "source": [
    "# search_documents\n",
    "query = {\"match\": {\"doc_id\": doc_ids[0]}}\n",
    "results = es_manager.search_documents(index_name, query=query)\n",
    "\n",
    "print(len(results))\n",
    "print(results[0][\"doc_id\"])\n",
    "print(results[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- Delete documents based on specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 8, 'timed_out': False, 'total': 4, 'deleted': 4, 'batches': 1, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_by_query\n",
    "delete_query = {\"match\": {\"doc_id\": doc_ids[0]}}\n",
    "es_manager.delete_by_query(index_name, query=delete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 328, 'timed_out': False, 'total': 2718, 'deleted': 2714, 'batches': 3, 'version_conflicts': 4, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete_by_query\n",
    "delete_query = {\"match_all\": {}}\n",
    "es_manager.delete_by_query(index_name, query=delete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search\n",
    "- **Keyword Search**  \n",
    "    - This method matches documents that contain the exact keyword in their text field.\n",
    "    - It performs a straightforward text-based search using Elasticsearch's `match` query.\n",
    "\n",
    "- **Semantic Search**  \n",
    "    - Semantic search leverages embeddings to find documents based on their contextual meaning rather than exact text matches.\n",
    "    - It uses a pre-trained model (`hf_embeddings_e5_instruct`) to encode both the query and the documents into vector representations and retrieves the most similar results.\n",
    "\n",
    "- **Hybrid Search**  \n",
    "    - Hybrid search combines both keyword search and semantic search to provide more comprehensive results.\n",
    "    - It uses a filtering mechanism to ensure documents meet specific keyword criteria while scoring and ranking results based on their semantic similarity to the query.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 725 ms, sys: 72.4 ms, total: 798 ms\n",
      "Wall time: 8.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# parallel_bulk_upsert\n",
    "es_manager.parallel_bulk_upsert(index_name, documents, batch_size=100, max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  : \"I am a fox,\" said the fox.\n",
      "1  : \"Good morning,\" said the fox.\n",
      "2  : [ Chapter 21 ]\n",
      "- the little prince befriends the fox\n",
      "It was then that the fox appeared.\n"
     ]
    }
   ],
   "source": [
    "# keyword search\n",
    "\n",
    "keyword = \"fox\"\n",
    "\n",
    "query = {\"match\": {\"text\": keyword}}\n",
    "results = es_manager.search_documents(index_name, query=query)\n",
    "\n",
    "for idx_, result in enumerate(results):\n",
    "    if idx_ < 3:\n",
    "        print(idx_, \" :\", result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "\n",
    "# Initialize ElasticsearchStore\n",
    "vector_store = ElasticsearchStore(\n",
    "    index_name=index_name,  # Elasticsearch index name\n",
    "    embedding=hf_embeddings_e5_instruct,  # Object responsible for text embeddings\n",
    "    es_url=ES_URL,  # Elasticsearch host URL\n",
    "    es_api_key=ES_API_KEY,  # Elasticsearch API key for authentication\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "ü§ñ Semantic Search Results:\n",
      "- \"Who are you?\" said the little prince.\n",
      "- \"Then what?\" asked the little prince.\n",
      "- And the little prince asked himself:\n"
     ]
    }
   ],
   "source": [
    "# Execute Semantic Search\n",
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "results = vector_store.similarity_search(search_query, k=3)\n",
    "\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"ü§ñ Semantic Search Results:\")\n",
    "for result in results:\n",
    "    print(f\"- {result.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç search_query:  Who are the Little Prince‚Äôs friends?\n",
      "üîç keyword:  friend\n",
      "* [SIM=0.927550] \"My friend the fox--\" the little prince said to me.\n"
     ]
    }
   ],
   "source": [
    "# hybrid search with score\n",
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "keyword = \"friend\"\n",
    "\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=search_query,\n",
    "    k=1,\n",
    "    filter=[{\"term\": {\"text\": keyword}}],\n",
    ")\n",
    "\n",
    "print(\"üîç search_query: \", search_query)\n",
    "print(\"üîç keyword: \", keyword)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **It is evident that conducting a Hybrid Search significantly enhances search performance.**  \n",
    "\n",
    "- This approach ensures that the search results are both contextually meaningful and aligned with the specified keyword constraint, making it especially useful in scenarios where both precision and context matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a **Huggingface Cache** , `vector_store` , `embeddings` and `client` .\n",
    "\n",
    "If you created a **vectordb** directory, please **remove** it at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCacheStrategy(expected_freed_size=0, blobs=frozenset(), refs=frozenset(), repos=frozenset(), snapshots=frozenset())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_manager.delete_index(index_name)\n",
    "\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "del embedded_documents\n",
    "del vector_store\n",
    "del es_manager\n",
    "scan = scan_cache_dir()\n",
    "scan.delete_revisions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Elasticsearch Connections and Documents\n",
    "### ElasticsearchConnectionManager\n",
    "- The `ElasticsearchConnectionManager` is a class designed to manage connections to an Elasticsearch instance.\n",
    "- It facilitates connecting to the Elasticsearch server and provides functionalities for creating and deleting indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elasticsearch_interface import ElasticsearchConnectionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain_tutorial_es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n",
      "1024\n",
      "CPU times: user 5.05 s, sys: 3.7 s, total: 8.75 s\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## text embedding\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "embedded_documents = hf_embeddings_e5_instruct.embed_documents(docs)\n",
    "\n",
    "print(len(embedded_documents))\n",
    "print(len(embedded_documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector dimension\n",
    "dims = len(embedded_documents[0])\n",
    "\n",
    "\n",
    "# üõ†Ô∏è Define the mapping for the new index\n",
    "# This structure specifies the schema for documents stored in Elasticsearch\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"metadata\": {\"properties\": {\"doc_id\": {\"type\": \"keyword\"}}},\n",
    "        \"text\": {\"type\": \"text\"},  # Field for storing textual content\n",
    "        \"vector\": {  # Field for storing vector embeddings\n",
    "            \"type\": \"dense_vector\",  # Specifies dense vector type\n",
    "            \"dims\": dims,  # Number of dimensions in the vector\n",
    "            \"index\": True,  # Enable indexing for vector search\n",
    "            \"similarity\": \"cosine\",  # Use cosine similarity for vector comparisons\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll learn how to generate text embeddings for documents using a Hugging Face model.\n",
    "- First, we'll set up a multilingual model with the `HuggingFaceEmbeddings` class and choose the optimal device (mps, cuda, or cpu) for computation.\n",
    "- Then, we'll generate embeddings for a list of documents and print the results to ensure everything is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ElasticsearchConnectionManager` class manages the connection to an Elasticsearch server.\n",
    "- This instance uses the server URL, API key, embedding model, and index name to connect to Elasticsearch and initialize the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/ [status:200 duration:0.570s]\n",
      "INFO:utils.elasticsearch_interface:‚úÖ Successfully connected to Elasticsearch!\n",
      "INFO:elastic_transport.transport:GET https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/ [status:200 duration:0.559s]\n",
      "INFO:utils.elasticsearch_interface:‚úÖ Vector store initialized for index 'langchain_tutorial_es'.\n"
     ]
    }
   ],
   "source": [
    "es_connection_manager = ElasticsearchConnectionManager(\n",
    "    es_url=ES_URL,\n",
    "    api_key=ES_API_KEY,\n",
    "    embedding_model=hf_embeddings_e5_instruct,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:404 duration:0.186s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.293s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' created successfully.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create index\n",
    "es_connection_manager.create_index(index_name, mapping=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.188s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.227s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_connection_manager.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticsearchDocumentManager\n",
    "- The `ElasticsearchDocumentManager` leverages the `ElasticsearchConnectionManager` to handle document management tasks.\n",
    "- This class performs operations such as inserting, deleting, and searching documents, with the capability to enhance performance through parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elasticsearch_interface import ElasticsearchDocumentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_document_manager = ElasticsearchDocumentManager(\n",
    "    connection_manager=es_connection_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "- The `upsert` method of the `es_document_manager` is used to insert or update documents in the specified Elasticsearch index.\n",
    "- It takes the original texts, their corresponding embedded documents, and the index name to efficiently manage the document storage and retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:5.236s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:5.579s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.754s]\n",
      "INFO:utils.elasticsearch_interface:‚úÖ Bulk upsert completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 697 ms, sys: 109 ms, total: 806 ms\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_document_manager.upsert(\n",
    "    texts=docs,\n",
    "    embedded_documents=embedded_documents,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.360s]\n"
     ]
    }
   ],
   "source": [
    "es_document_manager.delete(index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert_parallel\n",
    "- The `upsert_parallel` method of the `es_document_manager` facilitates the parallel insertion or updating of documents in the specified Elasticsearch index.\n",
    "- It processes the documents in batches of 100, utilizing up to 8 workers to enhance performance and efficiency in managing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.375s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.422s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.365s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.883s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.399s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.332s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.259s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.527s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.263s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.722s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:0.930s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.202s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:7.775s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:7.693s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 892 ms, sys: 89.8 ms, total: 981 ms\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_document_manager.upsert_parallel(\n",
    "    index_name=index_name,\n",
    "    texts=docs,\n",
    "    embedded_documents=embedded_documents,\n",
    "    batch_size=100,\n",
    "    max_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that parallel_upsert is **faster.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "- The code performs a search query, \"Who are the Little Prince‚Äôs friends?\", using the `es_document_manager` to retrieve relevant documents from the specified Elasticsearch index.\n",
    "- It fetches the top 10 results, then prints the query and each result in a formatted manner for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.726s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : people. For some, who are travelers, the stars are guides. For others they are no more than little\n",
      "1  : no more than little lights in the sky. For others, who are scholars, they are problems . For my\n",
      "2  : \"Forget what?\" inquired the little prince, who already was sorry for him.\n",
      "3  : \"Ashamed of what?\" insisted the little prince, who wanted to help him.\n",
      "4  : \"Where are the men?\" the little prince asked, politely.\n",
      "5  : But certainly, for us who understand life, figures are a matter of indifference. I should have\n",
      "6  : \"I know some one,\" said the little prince, \"who would make a bad explorer.\"\n",
      "7  : \"They are in a great hurry,\" said the little prince. \"What are they looking for?\"\n",
      "8  : \"Are they pursuing the first travelers?\" demanded the little prince.\n",
      "9  : a friend. And if I forget him, I may become like the grown-ups who are no longer interested in\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "\n",
    "results = es_document_manager.search(index_name=index_name, query=search_query, k=10)\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, result in enumerate(results):\n",
    "    print(idx_, \" :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves the top 10 relevant documents using similarity-based matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search?_source_includes=metadata,text [status:200 duration:0.370s]\n",
      "INFO:utils.elasticsearch_interface:‚úÖ Found 10 similar documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : \"Who are you?\" said the little prince.\n",
      "1  : \"Then what?\" asked the little prince.\n",
      "2  : And the little prince asked himself:\n",
      "3  : \"Why is that?\" asked the little prince.\n",
      "4  : But the little prince was wondering... The planet was tiny. Over what could this king really rule?\n",
      "5  : \"What do you do here?\" the little prince asked.\n",
      "6  : \"Where are the men?\" the little prince asked, politely.\n",
      "7  : \"No,\" said the little prince. \"I am looking for friends. What does that mean-- ‚Äòtame‚Äò?\"\n",
      "8  : [ Chapter 13 ]\n",
      "- the little prince visits the businessman\n",
      "9  : But the little prince added:\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "results = es_document_manager.search(query=search_query, k=10, use_similarity=True)\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, result in enumerate(results):\n",
    "    print(idx_, \" :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a search for the query \"Who are the Little Prince‚Äôs friends?\" while also filtering results based on the keyword \"friend,\" retrieving the top 10 relevant documents and printing their content alongside additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search?_source_includes=metadata,text [status:200 duration:0.187s]\n",
      "INFO:utils.elasticsearch_interface:‚úÖ Hybrid search completed. Found 10 results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : \"My friend the fox--\" the little prince said to me. 0.92783\n",
      "1  : any more. If you want a friend, tame me...\" 0.91324496\n",
      "2  : My friend broke into another peal of laughter: \"But where do you think he would go?\" 0.9049506\n",
      "3  : a grown-up. I have a serious reason: he is the best friend I have in the world. I have another 0.9047897\n",
      "4  : He was only a fox like a hundred thousand other foxes. But I have made him my friend, and now he is 0.9018576\n",
      "5  : a friend. And if I forget him, I may become like the grown-ups who are no longer interested in 0.89573324\n",
      "6  : that you have known me. You will always be my friend. You will want to laugh with me. And you will 0.8953247\n",
      "7  : \"That man is the only one of them all whom I could have made my friend. But his planet is indeed 0.89472246\n",
      "8  : to seek, in other days, merely by pulling up his chair; and he wanted to help his friend. 0.89299285\n",
      "9  : sure that I shall not forget him. To forget a friend is sad. Not every one has had a friend. And if 0.89034927\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "keyword = \"friend\"\n",
    "results = es_document_manager.search(\n",
    "    query=search_query, k=10, use_similarity=True, keyword=keyword\n",
    ")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, contents in enumerate(results):\n",
    "    print(idx_, \" :\", contents[0].page_content, contents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read\n",
    "- This code retrieves the IDs of all documents stored in the specified Elasticsearch index using the `get_documents_ids` method of the `es_document_manager`, and then prints the list of these document IDs for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.182s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qe9o2ZQBg-BrVn24K_9B', 'Qu9o2ZQBg-BrVn24K_9B', 'Q-9o2ZQBg-BrVn24K_9B', 'RO9o2ZQBg-BrVn24K_9B', 'Re9o2ZQBg-BrVn24K_9B', 'Ru9o2ZQBg-BrVn24K_9B', 'R-9o2ZQBg-BrVn24K_9B', 'SO9o2ZQBg-BrVn24K_9B', 'Se9o2ZQBg-BrVn24K_9B', 'Su9o2ZQBg-BrVn24K_9B']\n"
     ]
    }
   ],
   "source": [
    "ids = es_document_manager.get_documents_ids(index_name)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches documents from the specified Elasticsearch index using a list of document IDs, specifically retrieving the first 10 IDs. It then prints each document's ID along with its corresponding text for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.721s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36b5acf8-11d2-4015-bdf9-7e9346ccdc1b :  That night I did not see him set out on his way. He got away from me without making a sound. When I\n",
      "2b2ebe1f-c662-4d30-8237-da1adddba50e :  a sound. When I succeeded in catching up with him he was walking along with a quick and resolute\n",
      "40a590f1-1236-4e3b-a062-9e5e14b0b828 :  quick and resolute step. He said to me merely:\n",
      "ac1cfd05-f2c8-4d4f-a17f-b11fb8947db5 :  \"Ah! You are there...\" \n",
      "And he took me by the hand. But he was still worrying.\n",
      "79bd09e7-e25e-42ae-81d3-cab7ae6f9e83 :  \"It was wrong of you to come. You will suffer. I shall look as if I were dead; and that will not be\n",
      "d06709c4-f949-4ba6-a4da-8bb63973a610 :  that will not be true...\"\n",
      "6f1b7d37-3605-4bb1-992f-06cc685b0893 :  I said nothing.\n",
      "9f273fad-0b55-4bf2-8f89-834558f3cd7d :  \"You understand... it is too far. I cannot carry this body with me. It is too heavy.\"\n",
      "8df8dc63-ff88-4852-a5f7-2f14936af78f :  I said nothing.\n",
      "40a8bd69-19da-448c-9ac8-dce98fc087d5 :  \"But it will be like an old abandoned shell. There is nothing sad about old shells...\"\n"
     ]
    }
   ],
   "source": [
    "responses = es_document_manager.get_documents_by_ids(index_name, ids[:10])\n",
    "\n",
    "for response in responses:\n",
    "    print(response[\"doc_id\"], \": \", response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- This code deletes documents from the specified Elasticsearch index using a list of document IDs, specifically retrieving the first 10 IDs. It then prints each document's ID along with its corresponding text for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Qe9o2ZQBg-BrVn24K_9B [status:200 duration:0.187s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Qu9o2ZQBg-BrVn24K_9B [status:200 duration:0.185s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Q-9o2ZQBg-BrVn24K_9B [status:200 duration:0.186s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/RO9o2ZQBg-BrVn24K_9B [status:200 duration:0.188s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Re9o2ZQBg-BrVn24K_9B [status:200 duration:0.184s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Ru9o2ZQBg-BrVn24K_9B [status:200 duration:0.185s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/R-9o2ZQBg-BrVn24K_9B [status:200 duration:0.188s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/SO9o2ZQBg-BrVn24K_9B [status:200 duration:0.185s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Se9o2ZQBg-BrVn24K_9B [status:200 duration:0.185s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/Su9o2ZQBg-BrVn24K_9B [status:200 duration:0.185s]\n"
     ]
    }
   ],
   "source": [
    "es_document_manager.delete(index_name=index_name, ids=ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.343s]\n"
     ]
    }
   ],
   "source": [
    "# Delete all documents\n",
    "es_document_manager.delete(index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.181s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.263s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_connection_manager.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "langchain-opentutorial-gmgjIYR5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
