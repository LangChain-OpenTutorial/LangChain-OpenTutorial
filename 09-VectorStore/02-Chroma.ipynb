{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Chroma With Langchain\n",
        "\n",
        "- Author: [Gwangwon Jung](https://github.com/pupba)\n",
        "- Design: []()\n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to use `Chroma Vector Store` with `LangChain` .\n",
        "\n",
        "`Chroma` is an `open-source AI application database` .\n",
        "\n",
        "In this tutorial, after learning how to use `langchain-chroma` , we will implement examples of a simple **Text Search** engine using `Chroma` .\n",
        "\n",
        "![search-example](./assets/02-chroma-with-langchain-flow-search-example.png)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [What is Chroma?](#what-is-chroma?)\n",
        "- [LangChain Chroma Basic](#langchain-chroma-basic)\n",
        "- [Manage Store](#manage-store)\n",
        "- [Query vector store](#query-vector-store)\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [Chroma Docs](https://docs.trychroma.com/docs/overview/introduction)\n",
        "- [Langchain-Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\n",
        "- [List of VectorStore supported by Langchain](https://python.langchain.com/docs/integrations/vectorstores/)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain-core\",\n",
        "        \"langchain-chroma\",\n",
        "        \"chromadb\",\n",
        "        \"langchain-text-splitters\",\n",
        "        \"langchain-huggingface\",\n",
        "        \"python-dotenv\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Chroma\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77dbe5de",
      "metadata": {},
      "source": [
        "## What is Chroma?\n",
        "\n",
        "![logo](./assets/02-chroma-with-langchain-chroma-logo.png)\n",
        "\n",
        "`Chroma` is the `open-source vector database` designed for AI application. \n",
        "\n",
        "It specializes in storing high-dimensional vectors and performing fast similariy search, making it ideal for tasks like `semantic search` , `recommendation systems` and `multimodal search` .\n",
        "\n",
        "With its **developer-friendly APIs** and seamless integration with frameworks like `LangChain` , `Chroma` is powerful tool for building scalable, AI-driven solutions.\n",
        "\n",
        "The biggest feature of `Chroma` is that it internally **Indexing ([HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world))** and **Embedding ([all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))** are used when storing data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6d8430",
      "metadata": {},
      "source": [
        "## LangChain Chroma Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13874284",
      "metadata": {},
      "source": [
        "### Create VectorDB\n",
        "\n",
        "The **library** supported by `LangChain` has no `upsert` function and lacks interface uniformity with other **Vector DBs**, so we have implemented a new **Python** class.\n",
        "\n",
        "First, Defines a **Python** class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "id": "7b799511",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.vectordbinterface import VectorDBInterface\n",
        "from langchain_chroma import Chroma\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any, Callable, Optional\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from uuid import uuid4\n",
        "\n",
        "\n",
        "class ChromaDB(VectorDBInterface):\n",
        "    def __init__(self, embeddings: Optional[Any] = None):\n",
        "        self.chroma = None\n",
        "        self.unique_ids = set()\n",
        "        self._embeddings = embeddings if embeddings is not None else None\n",
        "        self._embeddings_function = (\n",
        "            embeddings.embed_documents\n",
        "            if embeddings is not None\n",
        "            else embedding_functions.DefaultEmbeddingFunction()  # all-MiniLM-L6v2\n",
        "        )\n",
        "        self.chroma_search = None\n",
        "\n",
        "    def connect(self, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        ChromaDB Connect\n",
        "        \"\"\"\n",
        "        langchain_config = {}\n",
        "\n",
        "        if kwargs[\"mode\"] == \"in-memory\":  # In-Memory\n",
        "            chroma_client = chromadb.Client()\n",
        "\n",
        "        elif kwargs[\"mode\"] == \"persistent\":  # Local\n",
        "            chroma_client = chromadb.PersistentClient(path=kwargs[\"persistent_path\"])\n",
        "            langchain_config[\"persist_directory\"] = kwargs[\"persistent_path\"]\n",
        "\n",
        "        elif kwargs[\"mode\"] == \"server\":  # Server-Client\n",
        "            chroma_client = chromadb.HttpClient(\n",
        "                host=kwargs[\"host\"], port=kwargs[\"port\"]\n",
        "            )\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"Invalid Input, Enter one of ['in-meory','persistent','server'] modes.\"\n",
        "            )\n",
        "\n",
        "        # The Chroma client allows you to get and delete existing collections by their name.\n",
        "        # It also offers a get or create method to get a collection if it exists, or create it otherwise.\n",
        "        self.chroma = chroma_client.get_or_create_collection(name=kwargs[\"collection\"])\n",
        "        langchain_config[\"collection_name\"] = kwargs[\"collection\"]\n",
        "\n",
        "        existing_ids = self.chroma.get(include=[])[\"ids\"]  # Get existing unique ids\n",
        "        self.unique_ids.update(existing_ids)  # current unique ids update\n",
        "\n",
        "        # Langchain-chroma for Search\n",
        "        self.chroma_search = Chroma(\n",
        "            **langchain_config,\n",
        "            embedding_function=self._embeddings,\n",
        "        )\n",
        "\n",
        "    def create_index(\n",
        "        self, index_name: str, dimension: int, metric: str = \"dotproduct\", **kwargs\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_index(self, index_name: str) -> Any:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def delete_index(self, index_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def list_indexs(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def add(self, pre_documents: List[Document], **kwargs) -> None:\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        for doc in pre_documents:\n",
        "            documents.append(doc.page_content)\n",
        "            ids.append(doc.metadata[\"id\"])\n",
        "            metadatas.append(\n",
        "                {key: value for key, value in doc.metadata.items() if key != \"id\"}\n",
        "            )\n",
        "\n",
        "        embeddings = self._embeddings_function(documents)  # embedding documents\n",
        "\n",
        "        self.chroma.add(\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids,\n",
        "        )\n",
        "        self.unique_ids.update(ids)\n",
        "\n",
        "    def upsert_documents(\n",
        "        self,\n",
        "        documents: List[Dict],\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Upsert documents to Chroma\n",
        "\n",
        "        :param documents: List of documents\n",
        "        :param embedding_function: Embedding function\n",
        "        \"\"\"\n",
        "        # Embedding documents\n",
        "        embeddings = self._embeddings_function([doc.page_content for doc in documents])\n",
        "        # Generate unique ids\n",
        "        unique_ids = [doc.metadata[\"id\"] for doc in documents]\n",
        "        # Upsert documents\n",
        "        self.chroma.upsert(\n",
        "            ids=unique_ids,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=[doc.metadata for doc in documents],\n",
        "            documents=[doc.page_content for doc in documents],\n",
        "        )\n",
        "\n",
        "        print(\"Success Upsert All Documents\")\n",
        "\n",
        "        # update unique_ids\n",
        "        self.unique_ids.update(unique_ids)\n",
        "\n",
        "    def upsert_documents_parallel(\n",
        "        self,\n",
        "        documents: List[Dict],\n",
        "        batch_size: int = 32,\n",
        "        max_workers: int = 10,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Parallel upsert documents to Chroma\n",
        "        :param documents: List of documents\n",
        "        :param batch_size: Batch size\n",
        "        :param max_workers: Number of workers\n",
        "        \"\"\"\n",
        "        # split documents into batches\n",
        "        batches = [\n",
        "            documents[i : i + batch_size] for i in range(0, len(documents), batch_size)\n",
        "        ]\n",
        "        all_unique_ids = set()  # Store all unique IDs from all batches\n",
        "        failed_uids = []  # Store failed batches\n",
        "\n",
        "        # Parallel processing\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [\n",
        "                executor.submit(self.upsert_documents, batch, **kwargs)\n",
        "                for batch in batches\n",
        "            ]\n",
        "\n",
        "        # Wait for all futures to complete\n",
        "        for future, batch in zip(as_completed(futures), batches):\n",
        "            try:\n",
        "                future.result()  # Wait for the batch to complete\n",
        "                # Extract unique IDs from the batch\n",
        "                unique_ids = [doc.metadata[\"id\"] for doc in batch]\n",
        "                all_unique_ids.update(unique_ids)  # Add to the total set\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during upsert: {e}\")\n",
        "                failed_uids.append(unique_ids)  # Store failed batch for retry\n",
        "\n",
        "        self.unique_ids.update(all_unique_ids)\n",
        "\n",
        "        print(f\"Success Upsert Parallel All Documents\\nFailed Batches: {failed_uids}\")\n",
        "\n",
        "    def query(\n",
        "        self, query_vector: List[float], top_k: int = 10, **kwargs\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"\n",
        "        Search in LangChain is better and uses its functionality\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def delete_by_filter(\n",
        "        self, unique_ids: List[str], filters: Optional[Dict] = None, **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Delete documents by filter\n",
        "        :param unique_ids: List of unique ids\n",
        "        :param filters: Filter conditions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.chroma.delete(\n",
        "                ids=unique_ids,\n",
        "                where=filters,\n",
        "            )\n",
        "            pre_count = len(self.unique_ids)\n",
        "            self.unique_ids = set(self.chroma.get(include=[])[\"ids\"])\n",
        "\n",
        "            print(f\"Success Delete {pre_count-len(self.unique_ids)} Documents\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    def preprocess_documents(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        source: Optional[str] = None,\n",
        "        author: Optional[str] = None,\n",
        "        chapter: bool = False,\n",
        "        **kwargs,\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Change LangChain Document to Chroma\n",
        "        :param documents: List of LangChain documents\n",
        "        :param source: Source of the document\n",
        "        :param author: Author of the document\n",
        "        :param chapter: Chapter of the document\n",
        "        :return: List of Chroma documents\n",
        "        \"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        if source is not None:\n",
        "            metadata[\"source\"] = source\n",
        "        if author is not None:\n",
        "            metadata[\"author\"] = author\n",
        "\n",
        "        processed_docs = []\n",
        "        current_chapter = None\n",
        "        save_flag = False\n",
        "        for doc in documents:\n",
        "            content = doc.page_content\n",
        "\n",
        "            content = content.replace(\"(picture)\\n\", \"\")\n",
        "\n",
        "            # Chapter dectect\n",
        "            if content.startswith(\"[ Chapter \") and \"\\n\" in content:\n",
        "                # Chapter Num (example: \"[ Chapter 26 ]\\n\" -> 26)\n",
        "                chapter_part, content_part = content.split(\"\\n\", 1)\n",
        "                current_chapter = int(chapter_part.split()[2].strip(\"]\"))\n",
        "                content = content_part\n",
        "\n",
        "            elif content.strip() == \"[ END ]\":\n",
        "                break\n",
        "\n",
        "            if current_chapter is not None:\n",
        "                # add metadata\n",
        "                if chapter:\n",
        "                    metadata[\"chapter\"] = current_chapter\n",
        "                updated_metadata = {**doc.metadata, **metadata, \"id\": str(uuid4())}\n",
        "                # Document append to processed_docs\n",
        "                processed_docs.append(\n",
        "                    Document(metadata=updated_metadata, page_content=content)\n",
        "                )\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def get_api_key(self) -> str:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f20414",
      "metadata": {},
      "source": [
        "### Select Embedding Model\n",
        "\n",
        "We load the **Embedding Model** with `langchain_huggingface` .\n",
        "\n",
        "If you want to use a different model, use a different model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 439,
      "id": "fead7517",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs={\"trust_remote_code\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4e5e6d",
      "metadata": {},
      "source": [
        "Create `ChromaDB` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "id": "27ccfa77",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store = ChromaDB(embeddings=embeddings)\n",
        "vector_store.connect(\n",
        "    mode=\"persistent\",\n",
        "    persistent_path=\"data/chroma.sqlite\",\n",
        "    collection=\"test\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492fd4a1",
      "metadata": {},
      "source": [
        "### Load Text Documents Data\n",
        "\n",
        "In this tutorial, we will use the `A Little Prince` fairy tale document.\n",
        "\n",
        "To put this data in `Chroma` ,we will do data preprocessing first.\n",
        "\n",
        "First of all, we will load the `data/the_little_prince.txt` file that extracted only the text of the fairy tale document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "8dd04292",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If your \"OS\" is \"Windows\", add 'encoding=utf-8' to the open function\n",
        "with open(\"./data/the_little_prince.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1534b127",
      "metadata": {},
      "source": [
        "Second, chunking the text imported into the `RecursiveCharacterTextSplitter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "id": "3b4b30b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: The Little Prince\n",
            "Written By Antoine de Saiot-Exupery (1900〜1944)\n",
            "Metadata: {}\n",
            "\n",
            "Content: [ Antoine de Saiot-Exupery ]\n",
            "Metadata: {}\n",
            "\n",
            "Content: Over the past century, the thrill of flying has inspired some to perform remarkable feats of\n",
            "Metadata: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.create_documents([raw_text])\n",
        "\n",
        "for docs in split_docs[:3]:\n",
        "    print(f\"Content: {docs.page_content}\\nMetadata: {docs.metadata}\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558e5fe7",
      "metadata": {},
      "source": [
        "Preprocessing document for `Chroma` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "id": "ed0952fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_dosc = vector_store.preprocess_documents(\n",
        "    documents=split_docs,\n",
        "    source=\"The Little Prince\",\n",
        "    author=\"Antoine de Saint-Exupéry\",\n",
        "    chapter=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "id": "7d92d3bc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': 'dcccc8df-23a7-4250-b258-a1508cb3afa5'}, page_content='- we are introduced to the narrator, a pilot, and his ideas about grown-ups'),\n",
              " Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac'}, page_content='Once when I was six years old I saw a magnificent picture in a book, called True Stories from'),\n",
              " Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': 'edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b'}, page_content='True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the')]"
            ]
          },
          "execution_count": 279,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68dfd9a",
      "metadata": {},
      "source": [
        "## Manage Store\n",
        "\n",
        "This section introduces three basic functions.\n",
        "\n",
        "- `add`\n",
        "\n",
        "- `upsert(parallel)`\n",
        "\n",
        "- `delete`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80a3015",
      "metadata": {},
      "source": [
        "### Add\n",
        "\n",
        "Add the new `Documents` .\n",
        "\n",
        "An error occurs if you have the same `ID` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "id": "725bbe68",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "id": "f800cb49",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['52a8a21b-112a-499b-b8a4-3f2ef902a988',\n",
              " 'edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b',\n",
              " '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac',\n",
              " 'dcccc8df-23a7-4250-b258-a1508cb3afa5']"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uids = list(vector_store.unique_ids)\n",
        "uids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "id": "68e78f78",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.chroma.get(ids=uids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb0babb",
      "metadata": {},
      "source": [
        "Error occurs when trying to `add` duplicate `ids` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "id": "3d57fac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Add of existing embedding ID: dcccc8df-23a7-4250-b258-a1508cb3afa5\n",
            "Add of existing embedding ID: 5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac\n",
            "Add of existing embedding ID: edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b\n",
            "Add of existing embedding ID: 52a8a21b-112a-499b-b8a4-3f2ef902a988\n",
            "Insert of existing embedding ID: dcccc8df-23a7-4250-b258-a1508cb3afa5\n",
            "Insert of existing embedding ID: 5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac\n",
            "Insert of existing embedding ID: edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b\n",
            "Insert of existing embedding ID: 52a8a21b-112a-499b-b8a4-3f2ef902a988\n"
          ]
        }
      ],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "id": "9461f96e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['52a8a21b-112a-499b-b8a4-3f2ef902a988',\n",
              " 'edbfea7b-e62f-4563-87a6-a9bf5a3d8b2b',\n",
              " '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac',\n",
              " 'dcccc8df-23a7-4250-b258-a1508cb3afa5']"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uids = list(vector_store.unique_ids)\n",
        "uids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63a62b3",
      "metadata": {},
      "source": [
        "### Upsert(parallel)\n",
        "\n",
        "`Upsert` will `Update` a document or `Add` a new document if the same `ID` exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "id": "4a9ca529",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['dcccc8df-23a7-4250-b258-a1508cb3afa5',\n",
              "  '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['- we are introduced to the narrator, a pilot, and his ideas about grown-ups',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': 'dcccc8df-23a7-4250-b258-a1508cb3afa5',\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac',\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 306,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "id": "8edc27d3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': 'dcccc8df-23a7-4250-b258-a1508cb3afa5'}, page_content='Changed Content')"
            ]
          },
          "execution_count": 308,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[0].page_content = \"Changed Content\"\n",
        "pre_dosc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "id": "d0086c07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Upsert All Documents\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ids': ['dcccc8df-23a7-4250-b258-a1508cb3afa5',\n",
              "  '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['Changed Content',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': 'dcccc8df-23a7-4250-b258-a1508cb3afa5',\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': '5cb1faed-692e-4aa0-9cd6-dfdf2fc23dac',\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.upsert_documents(\n",
        "    documents=pre_dosc[:2],\n",
        ")\n",
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d743a327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# parallel upsert\n",
        "vector_store.upsert_documents_parallel(\n",
        "    documents=pre_dosc,\n",
        "    batch_size=32,\n",
        "    max_workers=10,\n",
        ")\n",
        "# Clear Output cell, because it is too long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "id": "778a4678",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1317"
            ]
          },
          "execution_count": 354,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388f5512",
      "metadata": {},
      "source": [
        "### Delete\n",
        "\n",
        "`Delete` the Documents.\n",
        "\n",
        "You can use with `filter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "id": "83f1aa6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 355,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len([docs for docs in pre_dosc if docs.metadata[\"chapter\"] == 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "id": "0ad80866",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 43 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(\n",
        "    unique_ids=list(vector_store.unique_ids), filters={\"chapter\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "id": "8868202b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1274"
            ]
          },
          "execution_count": 357,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "id": "897d3961",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 30 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(unique_ids=list(vector_store.unique_ids)[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "id": "d3ffbad3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1244"
            ]
          },
          "execution_count": 362,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d515ad98",
      "metadata": {},
      "source": [
        "## Query Vector Store\n",
        "\n",
        "There are two ways to `Query` the `Vector Store` .\n",
        "\n",
        "- **Directly** : Query the vector store directly using methods like `similarity_search` or `similarity_search_with_score` .\n",
        "\n",
        "- **Turning into retriever** : Convert the vector store into a `retriever` object, which can be used in `LangChain` pipelines or chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37c37fb",
      "metadata": {},
      "source": [
        "### similarity_search()\n",
        "\n",
        "`similarity_search()` is run similarity search with Chroma.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `query:str` - Query text to search for.\n",
        "\n",
        "- `k: int = DEFAULT_K` - Number of results to return. Defaults to 4.    \n",
        "\n",
        "- `filter: Dict[str, str] | None = None` - Filter by metadata. Defaults to None.\n",
        "\n",
        "- `**kwargs:Any` - Additional keyword arguments to pass to Chroma collection query.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "- `List[Documents]` - List of documents most similar to the query text.\n",
        "\n",
        "\n",
        "\n",
        "### similarity_search_with_score()\n",
        "\n",
        "`similarity_search_with_score()` is run similarity search with Chroma with distance.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `query:str` - Query text to search for.\n",
        "\n",
        "- `k:int = DEFAULT_K` - Number of results to return. Defaults to 4.\n",
        "\n",
        "- `filter: Dict[str, str] | None = None` - Filter by metadata. Defaults to None.\n",
        "\n",
        "- `where_document: Dict[str, str] | None = None` - dict used to filter by the documents. E.g. {$contains: {\"text\": \"hello\"}}.\n",
        "\n",
        "- `**kwargs:Any` : Additional keyword arguments to pass to Chroma collection query.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "- `List[Tuple[Document, float]]` - List of documents most similar to the query text and distance in float for each. Lower score represents more similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "id": "04e592b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: - the little prince discovers a garden of roses\n",
            "1: one of her kind in all the universe. And here were five thousand of them, all alike, in one single\n"
          ]
        }
      ],
      "source": [
        "# Directly - similarity_search\n",
        "results = vector_store.chroma_search.similarity_search(\n",
        "    query=\"Look at it\",\n",
        "    k=2,\n",
        "    filter={\"chapter\": 20},\n",
        ")\n",
        "\n",
        "for idx, res in enumerate(results):\n",
        "    print(f\"{idx}: {res.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "id": "63674e63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: [Similarity Score: 46623.700000000004%] \"Oh, but I have looked already!\" said the little prince, turning around to give one more glance to\n"
          ]
        }
      ],
      "source": [
        "# Directly - similarity_search_with_score\n",
        "\n",
        "results = vector_store.chroma_search.similarity_search_with_score(\n",
        "    query=\"Look at it\",\n",
        "    k=1,\n",
        "    filter={\"chapter\": 10},\n",
        ")\n",
        "\n",
        "for idx, (res, score) in enumerate(results):\n",
        "    print(f\"{idx}: [Similarity Score: {round(score,3)*100}%] {res.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3de5b67",
      "metadata": {},
      "source": [
        "### as_retriever()\n",
        "\n",
        "The `as_retriever()` method converts a `VectorStore` object into a `Retriever` object.\n",
        "\n",
        "A `Retriever` is an interface used in `LangChain` to query a vector store and retrieve relevant documents.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `search_type:Optional[str]` - Defines the type of search that the Retriever should perform. Can be `similarity` (default), `mmr` , or `similarity_score_threshold`\n",
        "\n",
        "- `search_kwargs:Optional[Dict]` - Keyword arguments to pass to the search function. \n",
        "\n",
        "    Can include things like:\n",
        "\n",
        "    `k` : Amount of documents to return (Default: 4)\n",
        "\n",
        "    `score_threshold` : Minimum relevance threshold for similarity_score_threshold\n",
        "\n",
        "    `fetch_k` : Amount of documents to pass to `MMR` algorithm(Default: 20)\n",
        "        \n",
        "    `lambda_mult` : Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "\n",
        "    `filter` : Filter by document metadata\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `VectorStoreRetriever` - Retriever class for VectorStore.\n",
        "\n",
        "\n",
        "### invoke()\n",
        "\n",
        "Invoke the retriever to get relevant documents.\n",
        "\n",
        "Main entry point for synchronous retriever invocations.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `input:str` - The query string.\n",
        "- `config:RunnableConfig | None = None` - Configuration for the retriever. Defaults to None.\n",
        "- `**kwargs:Any` - Additional arguments to pass to the retriever.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `List[Document]` : List of relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "id": "84cd502e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='98325aa3-653c-4390-abe6-a4ade76f792f', metadata={'author': 'Antoine de Saint-Exupéry', 'chapter': 10, 'id': '98325aa3-653c-4390-abe6-a4ade76f792f', 'source': 'The Little Prince'}, page_content='- the little prince visits the king'),\n",
              " Document(id='2722874b-928d-4830-a34a-4f0a375794f3', metadata={'author': 'Antoine de Saint-Exupéry', 'chapter': 7, 'id': '2722874b-928d-4830-a34a-4f0a375794f3', 'source': 'The Little Prince'}, page_content='- the narrator learns about the secret of the little prince‘s life')]"
            ]
          },
          "execution_count": 463,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vector_store.chroma_search.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2},\n",
        ")\n",
        "\n",
        "retriever.invoke(\"a little prince\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f53e7c",
      "metadata": {},
      "source": [
        "Remove a `Huggingface Cache`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "id": "1ffecd99",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeleteCacheStrategy(expected_freed_size=0, blobs=frozenset(), refs=frozenset(), repos=frozenset(), snapshots=frozenset())"
            ]
          },
          "execution_count": 466,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import scan_cache_dir\n",
        "\n",
        "del embeddings\n",
        "scan = scan_cache_dir()\n",
        "scan.delete_revisions()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-B290FrwJ-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
