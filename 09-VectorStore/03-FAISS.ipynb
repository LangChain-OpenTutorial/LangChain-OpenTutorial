{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FAISS\n",
        "\n",
        "- Author: [Jeongeun Lim](https://www.linkedin.com/in/jeongeun-lim-808978188/)\n",
        "- Design: []()\n",
        "- Peer Review : \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/03-OutputParser/08-OutputFixingParser.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/03-OutputParser/08-OutputFixingParser.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "`FAISS` is a library designed for the efficient similarity search and clustering of dense vectors. It provides robust algorithms for searching vector sets of any size, including those that may not fit entirely in `RAM`.\n",
        "\n",
        "In addition to the core search functionality, `FAISS` includes support code for evaluation and parameter tuning, making it a versatile tool for various applications in machine learning and artificial intelligence.\n",
        "\n",
        "\n",
        "----\n",
        "Key Benefits:\n",
        "\n",
        "- Efficient Large-Scale Search:\n",
        "`FAISS` ensures fast and accurate vector searches, even with millions of high-dimensional vectors.\n",
        "\n",
        "- Memory Optimization:\n",
        "Offers advanced quantization techniques to reduce memory usage without sacrificing performance.\n",
        "\n",
        "- Customizable Search Accuracy:\n",
        "Users can fine-tune parameters to balance between search accuracy and speed according to specific requirements.\n",
        "\n",
        "- Versatile Applications:\n",
        "From machine learning to AI-powered recommendation systems, Faiss supports a wide range of use cases.\n",
        "\n",
        "\n",
        "---- \n",
        "Implementation Steps:\n",
        "\n",
        "To effectively integrate `FAISS` into your workflow, follow these steps:\n",
        "\n",
        "1. Data Preparation:\n",
        "Prepare and normalize your data, ensuring vectors are in a dense representation format.\n",
        "\n",
        "2. Index Creation:\n",
        "Select and build a Faiss index based on your dataset size and performance requirements. Common options include IndexFlat for brute-force search or IVF for scalable inverted file-based search.\n",
        "\n",
        "3. Index Training (if needed):\n",
        "For certain indices, such as `IVF` or `PQ`, train the index with representative data samples to optimize performance.\n",
        "\n",
        "4. Search Execution:\n",
        "Use the index to search for nearest neighbors, leveraging optional GPU acceleration for faster performance.\n",
        "\n",
        "5. Evaluation and Tuning:\n",
        "Test and evaluate the performance of your index, adjusting parameters like quantization levels or clustering size for improved results.\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Load a Sample Dataset](#load-a-sample-dataset)\n",
        "- [Create a VectorStore](#create-a-vectorstore)\n",
        "- [Create a FAISS VectorStore(from_documents)](#using-outputfixingparser-to-correct-incorrect-formatting)\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangChain Docs Faiss](https://python.langchain.com/docs/integrations/vectorstores/faiss)\n",
        "- [Faiss Docs](https://faiss.ai/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_community\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"FAISS\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
        "\n",
        "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a Sample Dataset\n",
        "Demonstrates how to load text files using LangChain’s `TextLoader` and split them into smaller chunks with `RecursiveCharacterTextSplitter`. \n",
        "The resulting documents are prepared for further embedding and storage in a FAISS vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Will be reflected in a fixed sample dataset in the future\n",
        "\"\"\"\n",
        "\n",
        "# from langchain_community.document_loaders import TextLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# # 텍스트 분할\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=0)\n",
        "\n",
        "# # 텍스트 파일을 load -> List[Document] 형태로 변환\n",
        "# loader1 = TextLoader(\"data/nlp-keywords.txt\")\n",
        "# loader2 = TextLoader(\"data/finance-keywords.txt\")\n",
        "\n",
        "# # 문서 분할\n",
        "# split_doc1 = loader1.load_and_split(text_splitter)\n",
        "# split_doc2 = loader2.load_and_split(text_splitter)\n",
        "\n",
        "# # 문서 개수 확인\n",
        "# len(split_doc1), len(split_doc2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of split documents: 18\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Will be reflected in a fixed sample dataset in the future\n",
        "\"\"\"\n",
        "\n",
        "from uuid import uuid4\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Define the dataset\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "\n",
        "# Define the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n",
        "\n",
        "# Split documents into smaller chunks and create a new list\n",
        "split_documents = []\n",
        "for doc in documents:\n",
        "    split_content = text_splitter.split_text(doc.page_content)\n",
        "    for chunk in split_content:\n",
        "        split_documents.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n",
        "# Generate a unique UUID for each split document\n",
        "uuids = [str(uuid4()) for _ in range(len(split_documents))]\n",
        "\n",
        "# Add the split documents to the VectorStore\n",
        "# db.add_documents(documents=split_documents, ids=uuids)\n",
        "\n",
        "# Verify the result (Print the number of split documents)\n",
        "print(f\"Number of split documents: {len(split_documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a VectorStore\n",
        "\n",
        "Key Initialization Parameters:\n",
        "\n",
        "- Indexing Parameters\n",
        "    - `embedding_function` (Embeddings): The embedding function to be used.\n",
        "- Client Parameters\n",
        "    - `index` (Any): The FAISS index to be used.\n",
        "    - `docstore` (Docstore): The document store to be utilized.\n",
        "    - `index_to_docstore_id` (Dict[int, str]): A mapping from the index to document store IDs.\n",
        "\n",
        "**[Note]** \n",
        "\n",
        "- `FAISS` is a high-performance library for vector search and clustering.\n",
        "- This class integrates `FAISS` with LangChain's VectorStore interface.\n",
        "- By combining the `embedding function`, `FAISS index`, and `document store`, you can build an efficient vector search system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Embedding\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Calculate the size of the embedding dimension\n",
        "dimension_size = len(embeddings.embed_query(\"hello world\"))\n",
        "print(dimension_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a FAISS vector store\n",
        "db = FAISS(\n",
        "    embedding_function=OpenAIEmbeddings(),\n",
        "    index=faiss.IndexFlatL2(dimension_size),\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a FAISS VectorStore(from_documents)\n",
        "\n",
        "The `from_documents` class method creates a FAISS vector store using a list of documents and an embedding function.\n",
        "\n",
        "- Parameters:\n",
        "    - `documents` (List[Document]): A list of documents to be added to the vector store.\n",
        "    - `embedding` (Embeddings): The embedding function to be used.\n",
        "    - `**kwargs`: Additional keyword arguments.\n",
        "\n",
        "- How It Works:\n",
        "1. Extracts the text content (`page_content`) and metadata from the list of documents.\n",
        "2. Calls the `from_texts` method using the extracted text and metadata.\n",
        "\n",
        "- Return Value:\n",
        "    - `VectorStore`: An instance of the vector store initialized with the provided documents and embeddings.\n",
        "\n",
        "**Note** \n",
        "- This method internally calls the `from_texts` method to create the vector store.\n",
        "- The `page_content` of each document is used as text, while `metadata` is used as the document's metadata.\n",
        "- Additional configurations can be passed through `kwargs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a FAISS vector store from the documents\n",
        "db = FAISS.from_documents(documents=split_documents, embedding=OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the document store IDs\n",
        "db.index_to_docstore_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the ID of the stored document: Document\n",
        "db.docstore._dict"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-fOxWcZdD-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
