{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개요 \n",
    "1. FastAPI 특징 및 주요 기능 간단 설명\n",
    "2. llm 서버 주의사항?\n",
    "3. langserve 있다는거 알려줌\n",
    "\n",
    "참고: \n",
    "- [RAG in Production - LangChain & FastAPI](https://www.youtube.com/watch?v=Arf7UwWjGyc&t=2s)\n",
    "- [FastAPI](https://fastapi.tiangolo.com/)\n",
    "- [\\[루닥스 블로그\\] 연습만이 살길이다\\:티스토리](https://rudaks.tistory.com/entry/langchain-Langchain%EA%B3%BC-FastAPI%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC-OpenAI-%EB%AA%A8%EB%8D%B8-%ED%98%B8%EC%B6%9C%ED%95%98%EA%B8%B0-invoke-ainvoke-stream-astream?category=1154278)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. fastAPI에 llm 호출 서빙\n",
    "\n",
    "### 테스트 편의를 위하여 `get`만 사용하며, 필요 인자는 쿼리 매개변수로 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# FastAPI 인스턴스 생성\n",
    "\n",
    "app = FastAPI()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = llm | StrOutputParser()\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/ainvoke\")\n",
    "async def async_chat(message: str):\n",
    "    response = await chain.ainvoke(message)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/stream\")\n",
    "def sync_stream_chat(message: str):\n",
    "    def event_stream():\n",
    "        try:\n",
    "            for chunk in chain.stream(message):\n",
    "                if len(chunk) > 0:\n",
    "                    yield f\"{chunk}\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {str(e)}\\n\\n\"\n",
    "    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n",
    "\n",
    "@app.get(\"/astream\")\n",
    "async def async_stream_chat(message: str):\n",
    "    async def event_stream():\n",
    "        try:\n",
    "            async for chunk in chain.astream(message):\n",
    "                if len(chunk) > 0:\n",
    "                    yield f\"{chunk}\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {str(e)}\\n\\n\"\n",
    "    return StreamingResponse(event_stream(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑의 코드를 통해서 FastAPI를 실행시키고 당신의 브라우저에서 url로 접속하여 테스트 해보시오\n",
    "\n",
    "ex: `http://127.0.0.1:8000/invoke?message=구글의 설립연도를 알려줘`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. FastAPI VectorDB 호출\n",
    "간단한 RAG 내용 추가??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "DB_PATH = \"./chroma_db\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "chroma = Chroma(\n",
    "    collection_name=\"FastApiServing\", \n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = chroma.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/add-content\")\n",
    "async def add_content(content: str, author:str):\n",
    "    time.sleep(3)\n",
    "    chroma.add_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"add-content: {content}\"}\n",
    "\n",
    "@app.get('/async-add-content')\n",
    "async def async_add_content(content: str, author:str):\n",
    "    await asyncio.sleep(3)\n",
    "    await chroma.aadd_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"async-add-content: {content}\"}\n",
    "\n",
    "# 추가 예정\n",
    "# get by ids()\n",
    "# aget by ids()\n",
    "\n",
    "# delete\n",
    "# adelete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex: `http://127.0.0.1:8000/async-add-content?content=Hello World&author=donghak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastAPI 서빙시 비동기 중요성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "DB_PATH = \"./chroma_db\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "chroma = Chroma(\n",
    "    collection_name=\"FastApiServing\", \n",
    "    persist_directory=DB_PATH,\n",
    "    embedding_function=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = chroma.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "@app.get(\"/invoke\")\n",
    "def sync_chat(message: str):\n",
    "    response = chain.invoke(message)\n",
    "    return response\n",
    "\n",
    "@app.get(\"/add-content\")\n",
    "async def add_content(content: str, author:str):\n",
    "    time.sleep(3)\n",
    "    # chroma.add_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"add-content: {content}\"}\n",
    "\n",
    "@app.get('/async-add-content')\n",
    "async def async_add_content(content: str, author:str):\n",
    "    await asyncio.sleep(3)\n",
    "    # await chroma.aadd_texts([content], [{\"source\": author}])\n",
    "    return {\"message\": f\"async-add-content: {content}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비동기 비교 테스트 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "\n",
    "async def make_request(endpoint):\n",
    "    url = f\"http://127.0.0.1:8000/{endpoint}\"\n",
    "\n",
    "    # print(f\"-----url: {url}------\")\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=20) as client:\n",
    "        response = await client.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Response from {endpoint}: {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Error from {endpoint}: {response.status_code}\")\n",
    "            print(\"Response content:\", response.text)\n",
    "\n",
    "\n",
    "async def main(endpoint):\n",
    "    tasks = [\n",
    "        make_request(f\"{endpoint}?content=my name is donghak&author=donghak\"),\n",
    "        make_request(\"invoke?message=내이름을 하나만 말해봐\"),\n",
    "    ]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Test sync\n",
    "    start_time = time.time()\n",
    "    for _ in range(3):\n",
    "        asyncio.run(main(endpoint=\"add-content\"))\n",
    "    end_time = time.time()\n",
    "    syncResult = end_time - start_time\n",
    "\n",
    "    #  test async\n",
    "    start_time = time.time()\n",
    "    for _ in range(3):\n",
    "        asyncio.run(main(endpoint=\"async-add-content\"))\n",
    "    end_time = time.time()\n",
    "    asyncResult = end_time - start_time\n",
    "\n",
    "    print(f\"Sync time: {syncResult}\")\n",
    "    print(f\"Async time: {asyncResult}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
