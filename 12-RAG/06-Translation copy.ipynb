{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BaBetterB/LangChain-OpenTutorial/blob/main/15-Agent/05-Iteration-HumanInTheLoop.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial compares two approaches to translating Chinese text into English using LangChain.\n",
    "\n",
    "The first approach utilizes a single LLM (e.g. GPT-4) to generate a straightforward translation. The second approach employs Retrieval-Augmented Generation (RAG), which enhances translation accuracy by retrieving relevant documents.\n",
    "\n",
    "The tutorial evaluates the translation accuracy and performance of each method, helping users choose the most suitable approach for their needs.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Translation using LLM](#translation-using-llm)\n",
    "- [Translation using RAG](#translation-using-rag)\n",
    "- [Evaluation of translation results](#evaluation-of-translation-resultsr)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [ `langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample text and output the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"load_dotenv\",\n",
    "        \"langchain_openai\",\n",
    "        \"transformers\",\n",
    "        \"faiss-cpu\",\n",
    "        \"sentence_transformers\",\n",
    "        \"sacrebleu\",\n",
    "        \"unbabel-comet\",\n",
    "        \"load_from_checkpoint\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Translation\",  # title\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration File for Managing API Keys as Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key Information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using LLM\n",
    "\n",
    "Translation using LLM refers to using a large language model (LLM), such as GPT-4, to translate text from one language to another. \n",
    "The model processes the input text and generates a direct translation based on its pre-trained knowledge. This approach is simple, fast, and effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=6a4fb624-6770-4ba0-b055-2bee90a8fcfc; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=c26c4172-c7cc-40ab-9a24-c374bbb77ddc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese_text: 人工智能正在改变世界，各国都在加紧研究如何利用这一技术提高生产力。\n",
      "Translation: 인공지능이 세계를 변화시키고 있으며, 각국은 이 기술을 활용하여 생산성을 높이는 방법에 대한 연구를 가속화하고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=c26c4172-c7cc-40ab-9a24-c374bbb77ddc\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=f209f675-10f6-4c8d-aa5c-9114535ae27d; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=ea4aac3a-bbf1-40a4-94bb-238a77e5d351; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=856eca3e-b98e-4394-9528-6c418a9451ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=f209f675-10f6-4c8d-aa5c-9114535ae27d; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=856eca3e-b98e-4394-9528-6c418a9451ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=75137c7f-b658-46b7-b2e6-0f212dcb7587; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=1c4315ae-f9ce-48e4-9092-e85c11d8d4eb; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=3041959b-0eb0-4ff5-a7c3-0919373ccaa8\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=75137c7f-b658-46b7-b2e6-0f212dcb7587; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=3041959b-0eb0-4ff5-a7c3-0919373ccaa8\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create PromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional translator.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Please translate the following Chinese document into natural and accurate Korean.\"\n",
    "            \"Consider the context and vocabulary to ensure smooth and fluent sentences.:.\\n\\n\"\n",
    "            \"**Chinese Original Text:** {chinese_text}\\n\\n**English Translation:**\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "translation_chain = RunnableSequence(prompt, llm)\n",
    "\n",
    "chinese_text = \"人工智能正在改变世界，各国都在加紧研究如何利用这一技术提高生产力。\"\n",
    "\n",
    "response = translation_chain.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "print(\"Chinese_text:\", chinese_text)\n",
    "print(\"Translation:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using RAG \n",
    "\n",
    "Translation using RAG (Retrieval-Augmented Generation) enhances translation accuracy by combining a pre-trained LLM with a retrieval mechanism. It first retrieves relevant documents or data related to the input text, then uses this additional context to generate a more precise and contextually accurate translation. This approach is particularly useful for technical terms, specialized content, or context-sensitive translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search Implementation Using FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It is widely used for approximate nearest neighbor (ANN) search in large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result\n",
      "1. 当地球员并非专业人士，而是农民、建筑工人、教师和学生，对足球的热爱将他们凝聚在一起\n",
      "2. ”卡卡说道\n",
      "3. “足球让我们结识新朋友，连接更广阔的世界\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "file_path = \"data/news_cn.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"file not found!!: {file_path}\")\n",
    "\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Vectorizing Sentences Individually\n",
    "sentences = []\n",
    "for doc in docs:\n",
    "    text = doc.page_content\n",
    "    sentence_list = text.split(\"。\")  # Splitting Chinese sentences based on '。'\n",
    "    sentences.extend(\n",
    "        [sentence.strip() for sentence in sentence_list if sentence.strip()]\n",
    "    )\n",
    "\n",
    "\n",
    "# Store sentences in the FAISS vector database\n",
    "vector_store = FAISS.from_texts(sentences, embedding=embeddings)\n",
    "\n",
    "# Search vectors using keywords \"人工智能\"\n",
    "search_results = vector_store.similarity_search(\"人工智能\", k=3)\n",
    "\n",
    "# check result\n",
    "print(\"Search result\")\n",
    "for idx, result in enumerate(search_results, start=1):\n",
    "    print(f\"{idx}. {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare translation using LLM and translation using RAG.\n",
    "\n",
    "First, write the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# NLTK의 문장 토큰화를 위한 데이터 다운로드 (최초 1회 실행 필요)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Document Search Function (Used in RAG)\n",
    "def retrieve_relevant_docs(query, vector_store, k=3):\n",
    "\n",
    "    # Perform search and return relevant documents\n",
    "    search_results = vector_store.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in search_results]\n",
    "\n",
    "\n",
    "# Translation using only LLM\n",
    "def translate_with_llm(chinese_text):\n",
    "\n",
    "    prompt_template_llm = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Translate the following Chinese sentence into Korean:\",\n",
    "            ),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\"user\", \"Please provide an accurate translation.\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_llm = RunnableSequence(prompt_template_llm, llm)\n",
    "\n",
    "    return translation_chain_llm.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# RAG-based Translation\n",
    "def translate_with_rag(chinese_text, vector_store):\n",
    "\n",
    "    retrieved_docs = retrieve_relevant_docs(chinese_text, vector_store)\n",
    "\n",
    "    # Add retrieved documents as context\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Construct prompt template (Using RAG)\n",
    "\n",
    "    prompt_template_rag = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Below is the Chinese text that needs to be translated into Korean. Additionally, the following context has been provided from relevant documents that might help you in producing a more accurate and context-aware translation.\",\n",
    "            ),\n",
    "            (\"system\", f\"Context (Relevant Documents):\\n{context}\"),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Please provide a translation that is both accurate and reflects the context from the documents provided.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_rag = RunnableSequence(prompt_template_rag, llm)\n",
    "\n",
    "    # Request translation using RAG\n",
    "\n",
    "    return translation_chain_rag.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# Function to store document text as a list\n",
    "def chinese_text_from_file_loader(path):\n",
    "    \"\"\"\n",
    "    파일에서 중국어 텍스트를 로드하고 문장 단위로 분리하여 리스트로 반환\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    return split_chinese_sentences_from_docs(docs)\n",
    "\n",
    "\n",
    "def split_chinese_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    문서 리스트에서 문장을 분리하여 리스트로 반환\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content  # 문서 객체에서 텍스트 추출\n",
    "        sentences.extend(split_chinese_sentences(text))  # 문장 단위로 분리하여 추가\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_chinese_sentences(text):\n",
    "    \"\"\"\n",
    "    - 정규 표현식을 사용하여 문장과 문장부호를 함께 분리.\n",
    "    - 문장과 문장부호를 다시 결합하여 반환.\n",
    "    \"\"\"\n",
    "    # 문장과 문장부호 분리\n",
    "    sentence_list = re.split(r\"([。！？])\", text)\n",
    "\n",
    "    # 문장과 문장부호를 결합하여 복원\n",
    "    merged_sentences = [\n",
    "        \"\".join(x) for x in zip(sentence_list[0::2], sentence_list[1::2])\n",
    "    ]\n",
    "\n",
    "    # 빈 문장 제거 후 반환\n",
    "    return [sentence.strip() for sentence in merged_sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def count_chinese_sentences(docs):\n",
    "    if isinstance(docs, str):\n",
    "        # `input_data`가 단순 문자열인 경우 바로 처리\n",
    "        sentences = split_chinese_sentences(docs)\n",
    "\n",
    "    print(f\"전체 문장 개수: {len(sentences)}\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    문서 리스트에서 문장을 분리하여 리스트로 반환\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content  # 문서 객체에서 텍스트 추출\n",
    "        sentences.extend(split_english_sentences(text))  # 문장 단위로 분리하여 추가\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences(text):\n",
    "    \"\"\"\n",
    "    - NLTK의 `sent_tokenize()`를 사용하여 문장을 정확하게 분리.\n",
    "    - 기본적으로 마침표(`.`), 물음표(`?`), 느낌표(`!`)를 인식하여 문장을 구분.\n",
    "    \"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "\n",
    "def count_paragraphs_and_sentences(docs):\n",
    "    \"\"\"\n",
    "    주어진 파일에서 영어 문장의 개수를 세는 함수\n",
    "    \"\"\"\n",
    "    if isinstance(docs, str):\n",
    "        # `input_data`가 단순 문자열인 경우 바로 처리\n",
    "        paragraphs = paragraphs = re.split(r\"\\n\\s*\\n\", docs.strip())\n",
    "        paragraphs = [\n",
    "            para.strip() for para in paragraphs if para.strip()\n",
    "        ]  # 빈 문단 제거\n",
    "        sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "\n",
    "        print(f\"📌 전체 문단 개수: {len(paragraphs)}\")\n",
    "        print(f\"📌 전체 문장 개수: {len(sentences)}\")\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the written functions to perform the comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input chinese text\n",
      "전체 문장 개수: 15\n",
      "数据领域迎来国家标准。10月8日，国家发改委等部门发布关于印发《国家数据标准体系建设指南》(以下简称《指南》)的通知。为“充分发挥标准在激活数据要素潜能、做强做优做大数字经济等方面的规范和引领作用”，国家发展改革委、国家数据局、中央网信办、工业和信息化部、财政部、国家标准委组织编制了《国家数据标准体系建设指南》。《指南》提出，到2026年底，基本建成国家数据标准体系，围绕数据流通利用基础设施、数据管理、数据服务、训练数据集、公共数据授权运营、数据确权、数据资源定价、企业数据范式交易等方面制修订30项以上数据领域基础通用国家标准，形成一批标准应用示范案例，建成标准验证和应用服务平台，培育一批具备数据管理能力评估、数据评价、数据服务能力评估、公共数据授权运营绩效评估等能力的第三方标准化服务机构。《指南》明确，数据标准体系框架包含基础通用、数据基础设施、数据资源、数据技术、数据流通、融合应用、安全保障等7个部分。数据基础设施方面，标准涉及存算设施中的数据算力设施、数据存储设施，网络设施中的5G网络数据传输、光纤数据传输、卫星互联网数据传输，此外还有流通利用设施。数据流通方面，标准包括数据产品、数据确权、数据资源定价、数据流通交易。融合应用方面，标准涉及工业制造、农业农村、商贸流通、交通运输、金融服务、科技创新、文化旅游(文物)、卫生健康、应急管理、气象服务、城市治理、绿色低碳。安全保障方面，标准涉及数据基础设施安全，数据要素市场安全，数据流通安全。数据资源中的数据治理标准包括数据业务规划、数据质量管理、数据调查盘点、数据资源登记；训练数据集方面的标准包括训练数据集采集处理、训练数据集标注、训练数据集合成。在组织保障方面，将指导建立全国数据标准化技术组织，加快推进急用、急需数据标准制修订工作，强化与有关标准化技术组织、行业、地方及相关社团组织之间的沟通协作、协调联动，以标准化促进数据产业生态建设。同时还将完善标准试点政策配套，搭建数据标准化公共服务平台，开展标准宣贯，选择重点地方、行业先行先试，打造典型示范。探索推动数据产品第三方检验检测，深化数据标准实施评价管理。在人才培养方面，将打造标准配套的数据人才培训课程，形成一批数据标准化专业人才。优化数据国际标准化专家队伍，支持参与国际标准化活动，强化国际交流。\n",
      "\n",
      "Translation using LLM\n",
      "📌 전체 문단 개수: 1\n",
      "📌 전체 문장 개수: 15\n",
      "데이터 분야에 국가 표준이 도입됩니다. 10월 8일, 국가발전개혁위원회 등 관련 부처는 《국가 데이터 표준 체계 구축 가이드라인》(이하 《가이드라인》)의 인쇄에 관한 통지를 발표했습니다. “데이터 요소의 잠재력을 활성화하고, 디지털 경제를 강하게 하고 우수하게 하며 크게 만드는 데 있어 표준의 규범적 역할과 선도적 역할을 충분히 발휘하기 위해”, 국가발전개혁위원회, 국가데이터국, 중앙 인터넷 정보 사무소, 산업정보화부, 재정부, 국가표준위원회가 《국가 데이터 표준 체계 구축 가이드라인》을 작성했습니다. 《가이드라인》은 2026년 말까지 국가 데이터 표준 체계의 기본적인 구축을 완료하고, 데이터 유통과 활용을 위한 인프라, 데이터 관리, 데이터 서비스, 훈련 데이터 세트, 공공 데이터 권한 운영, 데이터 권리 확립, 데이터 자원 가격 책정, 기업 데이터 패러다임 거래 등 30개 이상의 데이터 분야의 기본적인 일반 국가 표준을 제정 및 개정하여, 일련의 표준 적용 시범 사례를 형성하고, 표준 검증 및 적용 서비스 플랫폼을 구축하며, 데이터 관리 능력 평가, 데이터 평가, 데이터 서비스 능력 평가, 공공 데이터 권한 운영 성과 평가 등의 능력을 갖춘 제3자 표준화 서비스 기관을 육성할 계획이라고 명시했습니다. 《가이드라인》은 데이터 표준 체계의 프레임워크가 기본 일반, 데이터 인프라, 데이터 자원, 데이터 기술, 데이터 유통, 융합 응용, 안전 보장 등 7개 부분으로 구성된다고 명확히 했습니다. 데이터 인프라 부분에서는 표준이 데이터 연산 시설과 데이터 저장 시설, 네트워크 시설의 5G 네트워크 데이터 전송, 광섬유 데이터 전송, 위성 인터넷 데이터 전송, 그리고 유통 활용 시설 등을 포함합니다. 데이터 유통 부분에서는 데이터 제품, 데이터 권리 확립, 데이터 자원 가격 책정, 데이터 유통 거래를 포함합니다. 융합 응용 부분에서는 산업 제조, 농업 및 농촌, 상업 유통, 교통 운송, 금융 서비스, 과학 기술 혁신, 문화 관광(문화재), 보건 건강, 긴급 관리, 기상 서비스, 도시 관리, 녹색 저탄소 등을 포함한 표준이 다뤄집니다. 안전 보장 부분에서는 데이터 인프라 안전, 데이터 요소 시장 안전, 데이터 유통 안전과 관련된 표준이 포함됩니다. 데이터 자원 중 데이터 거버넌스 표준에는 데이터 업무 계획, 데이터 품질 관리, 데이터 조사 및 점검, 데이터 자원 등록이 포함됩니다; 훈련 데이터 세트 관련 표준에는 훈련 데이터 세트 수집 처리, 훈련 데이터 세트 주석, 훈련 데이터 세트 합성이 포함됩니다. 조직 보장 측면에서는 전국 데이터 표준화 기술 조직을 설립하도록 지도하고, 긴급 필요 데이터 표준의 제정 및 개정 작업을 가속화하며, 관련 표준화 기술 조직, 산업, 지역 및 관련 단체 간의 소통 협력, 조정을 강화하여 표준화를 통해 데이터 산업 생태계 구축을 촉진할 것입니다. 또한 표준 시범 정책을 보완하고, 데이터 표준화 공공 서비스 플랫폼을 구축하며, 표준 선전 활동을 진행하고, 주요 지역과 산업에서 선도적으로 시범 사업을 진행하여 전형적인 사례를 만들어 낼 것입니다. 데이터 제품의 제3자 검사 및 검사를 촉진하고 데이터 표준 시행 평가 관리의 심화를 탐색할 것입니다. 인재 양성 측면에서는 표준화된 데이터 인재 교육 과정을 개발하고, 데이터 표준화 전문 인재를 양성할 것입니다. 데이터 국제 표준화 전문가 팀을 최적화하고, 국제 표준화 활동에 참여를 지원하며, 국제 교류를 강화할 것입니다.\n",
      "\n",
      "Translation using RAG\n",
      "📌 전체 문단 개수: 1\n",
      "📌 전체 문장 개수: 15\n",
      "데이터 분야 국가 표준이 도입되었습니다. 10월 8일, 국가 발전 개혁 위원회 등 부처는 《국가 데이터 표준 체계 구축 가이드라인》(이하 《가이드라인》)의 배포에 관한 통지를 발표했습니다. 이는 \"데이터 요소의 잠재력을 활성화하고, 디지털 경제를 강하게 하고, 우수하게 하고, 크게 만드는 데 있어 표준의 규범적 및 선도적 역할을 충분히 발휘하기 위해\" 국가 발전 개혁 위원회, 국가 데이터국, 중앙 인터넷 정보 사무소, 산업 및 정보화 부, 재무부, 국가 표준 위원회가 《국가 데이터 표준 체계 구축 가이드라인》을 편찬했습니다. 《가이드라인》은 2026년 말까지 국가 데이터 표준 체계를 기본적으로 구축하고, 데이터 유통 활용 인프라, 데이터 관리, 데이터 서비스, 훈련 데이터 세트, 공공 데이터 허가 운영, 데이터 권리 확인, 데이터 자원 가격 책정, 기업 데이터 패러다임 거래 등 30개 이상의 데이터 분야 기본 공통 국가 표준을 제정 및 수정하며, 일련의 표준 적용 시범 사례를 형성하고, 표준 검증 및 응용 서비스 플랫폼을 구축하며, 데이터 관리 능력 평가, 데이터 평가, 데이터 서비스 능력 평가, 공공 데이터 허가 운영 성과 평가 등의 능력을 갖춘 제3자 표준화 서비스 기관을 육성할 계획을 제시했습니다. 《가이드라인》은 데이터 표준 체계의 프레임워크가 기본 공통, 데이터 기반 시설, 데이터 자원, 데이터 기술, 데이터 유통, 융합 응용, 안전 보장 등 7개 부분으로 구성된다고 명시하고 있습니다. 데이터 기반 시설 관련 표준은 데이터 계산 시설, 데이터 저장 시설을 포함한 저장 및 계산 시설, 5G 네트워크 데이터 전송, 광섬유 데이터 전송, 위성 인터넷 데이터 전송을 포함한 네트워크 시설, 그리고 유통 활용 시설을 포함합니다. 데이터 유통 관련 표준은 데이터 제품, 데이터 권리 확인, 데이터 자원 가격 책정, 데이터 유통 거래를 포함합니다. 융합 응용 관련 표준은 산업 제조, 농업 및 농촌, 상업 유통, 교통 운송, 금융 서비스, 과학 기술 혁신, 문화 관광(문화재), 보건 건강, 비상 관리, 기상 서비스, 도시治理, 친환경 저탄소 분야를 포함합니다. 안전 보장 관련 표준은 데이터 기반 시설의 안전, 데이터 요소 시장의 안전, 데이터 유통의 안전을 포함합니다. 데이터 자원 내 데이터 거버넌스 표준은 데이터 사업 계획, 데이터 품질 관리, 데이터 조사 및 점검, 데이터 자원 등록을 포함하며; 훈련 데이터 세트 관련 표준은 훈련 데이터 세트 수집 및 처리, 훈련 데이터 세트 주석, 훈련 데이터 세트 합성을 포함합니다. 조직 보장 측면에서는 전국 데이터 표준화 기술 조직을 설립하도록 안내하고, 긴급히 필요한 데이터 표준의 제정 및 수정을 가속화하며, 관련 표준화 기술 조직, 산업, 지역 및 관련 단체 간의 소통 협력 및 조정 연계를 강화하여 표준화를 통해 데이터 산업 생태계 구축을 촉진할 예정입니다. 또한 표준 시범 정책을 보완하고, 데이터 표준화 공공 서비스 플랫폼을 구축하며, 표준 보급을 실시하고, 주요 지역 및 산업에서 선행 시험을 선택하여 전형적인 시범을 구축할 것입니다. 데이터 제품의 제3자 검증 및 테스트를 촉진하고, 데이터 표준 시행 평가 관리를 심화할 계획입니다. 인재 양성 측면에서는 표준에 부합하는 데이터 인재 교육 과정을 개발하고, 데이터 표준화 전문 인력을 양성할 것입니다. 데이터 국제 표준화 전문가 팀을 최적화하고, 국제 표준화 활동에 참여를 지원하며, 국제 교류를 강화할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# sentences = chinese_text_from_file_loader(\"data/comparison_cn.txt\")\n",
    "sentences = chinese_text_from_file_loader(\"data/comparison_cn copy.txt\")\n",
    "\n",
    "chinese_text = \"\"\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    chinese_text += sentence\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm_translation = translate_with_llm(chinese_text)\n",
    "\n",
    "\n",
    "# RAG\n",
    "rag_translation = translate_with_rag(chinese_text, vector_store)\n",
    "\n",
    "\n",
    "print(\"\\ninput chinese text\")\n",
    "count_chinese_sentences(chinese_text)\n",
    "print(chinese_text)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using LLM\")\n",
    "count_paragraphs_and_sentences(llm_translation.content)\n",
    "print(llm_translation.content)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using RAG\")\n",
    "count_paragraphs_and_sentences(rag_translation.content)\n",
    "print(rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트 데이터 셋 생성 문장의 수가 일치해야함함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make comparison_json\n",
    "def make_comparison_json(chinese_text, translation_1, translation_2):\n",
    "    # 문장별로 나누기 (원문과 번역을 각 문장별로 나누기)\n",
    "    # chinese_sentences = chinese_text.split(\"。\")\n",
    "    chinese_sentences = split_chinese_sentences(chinese_text)\n",
    "    translation_1_sentences = translation_1.split(\".\")\n",
    "    translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "    print(\"\\nchinese_sentences:\", len(chinese_sentences))\n",
    "    print(\"\\ntranslation_1_sentences:\", len(translation_1_sentences))\n",
    "    print(\"\\ntranslation_2_sentences:\", len(translation_2_sentences))\n",
    "\n",
    "    # 각 문장의 원문과 번역을 매핑하여 저장\n",
    "    data = []\n",
    "    for i in range(len(chinese_sentences)):\n",
    "        # 문장별로 원문과 번역을 매핑\n",
    "        sentence_data = {\n",
    "            \"chinese_text\": chinese_sentences[i].strip(),\n",
    "            \"translation_1\": (\n",
    "                translation_1_sentences[i].strip()\n",
    "                if i < len(translation_1_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"translation_2\": (\n",
    "                translation_2_sentences[i].strip()\n",
    "                if i < len(translation_2_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "        }\n",
    "        data.append(sentence_data)\n",
    "\n",
    "    # JSON 파일로 저장\n",
    "    output_file = \"data/translation_comparison2.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Translation comparison data saved to {output_file}\")\n",
    "\n",
    "    make_comparison_json(chinese_text, llm_translation.content, rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of translation results\n",
    "\n",
    "Evaluation of translation results using BLEU and TER scores.\n",
    "Considering the addition of COMET and GPT for further assessment.\n",
    "Aiming to improve accuracy and quality in translation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장별로 나누기 (원문과 번역을 각 문장별로 나누기)\n",
    "chinese_sentences = chinese_text.split(\"。\")\n",
    "translation_1_sentences = translatllm_translation.content.split(\".\")\n",
    "translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "# 각 문장의 원문과 번역을 매핑하여 저장\n",
    "data = []\n",
    "for i in range(len(chinese_sentences)):\n",
    "    # 문장별로 원문과 번역을 매핑\n",
    "    sentence_data = {\n",
    "        \"chinese_text\": chinese_sentences[i].strip(),\n",
    "        \"translation_1\": (\n",
    "            translation_1_sentences[i].strip()\n",
    "            if i < len(translation_1_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "        \"translation_2\": (\n",
    "            translation_2_sentences[i].strip()\n",
    "            if i < len(translation_2_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "    }\n",
    "    data.append(sentence_data)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "output_file = \"translation_comparison.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translation comparison data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 **Translation Quality Evaluation (BLEU & TER Scores)**\n",
      "\n",
      "╒════╤═══════════════╤════════════════════════════════════════════════╤════════╤═══════╕\n",
      "│    │ Category      │ Text                                           │ BLEU   │ TER   │\n",
      "╞════╪═══════════════╪════════════════════════════════════════════════╪════════╪═══════╡\n",
      "│  0 │ Source Text   │ 这个产品在市场上很受欢迎。                     │ -      │ -     │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  1 │ Translation 1 │ This product is very popular in the market.    │ 0.0    │ 800.0 │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  2 │ Translation 2 │ This product is well received in the market.   │ 0.0    │ 800.0 │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  3 │ Source Text   │ 人工智能正在改变世界。                         │ -      │ -     │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  4 │ Translation 1 │ Artificial intelligence is changing the world. │ 0.0    │ 600.0 │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  5 │ Translation 2 │ AI is transforming the world.                  │ 0.0    │ 500.0 │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  6 │ Source Text   │ 天气很好，我们去公园吧。                       │ -      │ -     │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  7 │ Translation 1 │ The weather is great, let's go to the park.    │ 0.0    │ 900.0 │\n",
      "├────┼───────────────┼────────────────────────────────────────────────┼────────┼───────┤\n",
      "│  8 │ Translation 2 │ It's nice outside, let's visit the park.       │ 0.0    │ 700.0 │\n",
      "╘════╧═══════════════╧════════════════════════════════════════════════╧════════╧═══════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "#  BLEU\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return round(sacrebleu.sentence_bleu(candidate, [reference]).score, 3)\n",
    "\n",
    "\n",
    "# TER\n",
    "def calculate_ter(reference, candidate):\n",
    "    ter_metric = sacrebleu.metrics.TER()\n",
    "    return round(ter_metric.corpus_score([candidate], [[reference]]).score, 3)\n",
    "\n",
    "\n",
    "json_file_path = \"data/translations_comparison.json\"\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "translations_data = load_json_data(json_file_path)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(translations_data)\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    source_text = row[\"source_text\"]\n",
    "    translation_1 = row[\"translation_1\"]\n",
    "    translation_2 = row[\"translation_2\"]\n",
    "\n",
    "    # translation_1 evaluation\n",
    "    bleu_1 = calculate_bleu(source_text, translation_1)\n",
    "    ter_1 = calculate_ter(source_text, translation_1)\n",
    "\n",
    "    # translation_2 evaluation\n",
    "    bleu_2 = calculate_bleu(source_text, translation_2)\n",
    "    ter_2 = calculate_ter(source_text, translation_2)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Source Text\",\n",
    "            \"Text\": source_text,\n",
    "            \"BLEU\": \"-\",\n",
    "            \"TER\": \"-\",\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 1\",\n",
    "            \"Text\": translation_1,\n",
    "            \"BLEU\": bleu_1,\n",
    "            \"TER\": ter_1,\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 2\",\n",
    "            \"Text\": translation_2,\n",
    "            \"BLEU\": bleu_2,\n",
    "            \"TER\": ter_2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def display_results(dataframe):\n",
    "    print(\"\\n📌 **Translation Quality Evaluation (BLEU & TER Scores)**\\n\")\n",
    "    print(tabulate(dataframe, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "display_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-9y5W8e20-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
