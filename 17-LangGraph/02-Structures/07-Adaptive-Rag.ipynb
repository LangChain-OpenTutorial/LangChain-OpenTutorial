{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Adaptive RAG\n",
        "\n",
        "- Author: [Yoonji Oh](https://github.com/samdaseuss)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial introduces Adaptive RAG, which finds information and generates answers in a smarter way. Adaptive RAG is a system that analyzes the nature of questions using AI, selects the most appropriate method such as web search or internal document search to find information, and creates the best possible answer by trying different methods when necessary. In this tutorial, we implement routing between web search and internal document search using LangGraph.\n",
        "\n",
        "The purpose of this tutorial is to help users understand the concept of Adaptive RAG and learn how to implement it using LangGraph. Through this, users can perform web searches for questions related to recent events and utilize internal document search with self-correcting capabilities for questions related to indexed content.\n",
        "\n",
        "**What We Will Learn** \n",
        "\n",
        "* **Preparing Data (Create Index)**  \n",
        "  Convert documents into a format our system can understand and load them\n",
        "  \n",
        "* **Using AI (LLMs)**  \n",
        "  Use AI to analyze questions and evaluate how good our retrieved documents are\n",
        "  \n",
        "* **Building Web Search Tool (Web Search Tool)**  \n",
        "  Set up tools to search for up-to-date information on the web\n",
        "  \n",
        "* **Designing System Structure (Construct the Graph)**  \n",
        "  Design how our system will work and in what order\n",
        "  \n",
        "* **Completing the System (Compile Graph)**  \n",
        "  Turn our design into a working system\n",
        "  \n",
        "* **Testing in Action (Use Graph)**  \n",
        "  Run our completed system and verify it works properly\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [Why Adaptive RAG?](#why-adaptive-rag)\n",
        "- [Creating a Basic PDF-based Retrieval Chain](#creating-a-basic-pdf-based-retrieval-chain)\n",
        "- [Query Routing and Document Evaluation](#query-routing-and-document-evaluation)\n",
        "- [Tools](#tools)\n",
        "- [Graph Construction](#graph-construction) \n",
        "- [Define Graph Flows](#define-graph-flows)\n",
        "- [Graph Utilization](#graph-utilization)\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangChain: Query Construction](https://blog.langchain.dev/query-construction/)\n",
        "- [LangGraph: Self-Reflective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/)\n",
        "- [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](https://arxiv.org/abs/2403.14403)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_openai\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Adaptive-RAG\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616661ad",
      "metadata": {},
      "source": [
        "## Why Adaptive RAG?\n",
        "Let's say an astrophysicist asks these questions to an AI chatbot:\n",
        "\n",
        "1. \"Sagittarius A* just released an unprecedented burst of X-rays - why is this significant?\"\n",
        "2. \"Can you analyze how Sagittarius A*'s X-ray emission patterns have changed over the past 5 years?\"\n",
        "\n",
        "While a conventional RAG system would approach information retrieval the same way every time, Adaptive RAG recognizes the different nature of these questions and responds accordingly:\n",
        "\n",
        "* Current X-ray burst => Search the web for real-time observations and urgent analysis from scientists\n",
        "* X-ray emission patterns => Analyze 5 years of observational records from our astrophysics database\n",
        "* Ability to self-correct if the answer is inaccurate\n",
        "\n",
        "**Adaptive RAG** is a **RAG** strategy that combines (1) query analysis and (2) Self-Reflective RAG.\n",
        "\n",
        "The paper \"Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity\" performs routing through query analysis in the following ways:\n",
        "* `No Retrieval` \n",
        "* `Single-shot RAG` \n",
        "* `Iterative RAG` \n",
        "\n",
        "We implement this using LangGraph.\n",
        "In this implementation, we perform the following routing:\n",
        "* **Web Search** : Used for questions about recent events\n",
        "* **Self-correcting RAG** : Used for questions about indexed content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Creating a Basic PDF-based Retrieval Chain\n",
        "Here we create a Retrieval Chain based on PDF documents. This is the most basic structure of a Retrieval Chain.\n",
        "Note that in LangGraph, we create the Retriever and Chain separately. This allows us to process each node in detail.\n",
        "\n",
        "**Note**\n",
        "* Since this was covered in the previous tutorial, we'll skip the detailed explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4dd488",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install rag\n",
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c15d0936",
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = '../data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "12e1b64d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/lz/nlzcstbj3ns0khpcn06c2xgh0000gn/T/ipykernel_99631/1408999571.py:15: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PDFPlumberLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create document loader\n",
        "loader = PDFPlumberLoader(file_path)\n",
        "\n",
        "# Load documents\n",
        "documents = loader.load()\n",
        "\n",
        "# Create embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create vector store\n",
        "vectorstore = Chroma.from_documents(documents, embeddings)\n",
        "\n",
        "# Create retriever\n",
        "pdf_retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Create chain\n",
        "pdf_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0), \n",
        "    chain_type=\"stuff\", \n",
        "    retriever=pdf_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2fc536",
      "metadata": {},
      "source": [
        "## Query Routing and Document Evaluation\n",
        "The **LLMs** stage performs **query routing** and **document evaluation** . This process is a crucial part of **Adaptive RAG** , contributing to efficient information retrieval and generation.\n",
        "\n",
        "* **Query Routing** : Analyzes user queries to route them to appropriate information sources. This enables setting the optimal search path based on the query's purpose.\n",
        "* **Document Evaluation** : Assesses the quality and relevance of retrieved documents to enhance the accuracy of final results. This process is essential for maximizing the performance of **LLMs** .\n",
        "\n",
        "This stage supports the core functionality of **Adaptive RAG** and aims to provide accurate and reliable information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78392fb3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain_teddynote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1b78d33f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_teddynote.models import get_model_name, LLMs\n",
        "\n",
        "# Get the latest LLM model name\n",
        "MODEL_NAME = get_model_name(LLMs.GPT4)\n",
        "\n",
        "# Data model for routing user queries to the most relevant data source\n",
        "class RouteQuery(BaseModel):\n",
        "   \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "   # Literal type field for selecting data source\n",
        "   datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
        "       ...,\n",
        "       description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
        "   )\n",
        "\n",
        "\n",
        "# Initialize LLM and generate structured output through function calling\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
        "\n",
        "# Create prompt template containing system message and user question\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
        "The vectorstore contains documents related to \"A European Approach to Artificial Intelligence - A Policy Perspective\".\n",
        "Use the vectorstore for questions on European AI policy and regulations. Otherwise, use web-search.\"\"\"\n",
        "\n",
        "# Create prompt template for routing\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "   [\n",
        "       (\"system\", system),\n",
        "       (\"human\", \"{question}\"),\n",
        "   ]\n",
        ")\n",
        "\n",
        "# Combine prompt template and structured LLM router to create question router\n",
        "question_router = route_prompt | structured_llm_router"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e4d831",
      "metadata": {},
      "source": [
        "Now let's pass questions to our `rag_chain` to generate responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0874c14b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datasource='vectorstore'\n"
          ]
        }
      ],
      "source": [
        "# Question that needs document search\n",
        "print(\n",
        "    question_router.invoke(\n",
        "        {\"question\": \"According to the European approach, what are the current limitations of AI applications in healthcare settings?According to the European approach, what are the current limitations of AI applications in healthcare settings?\"}\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a2d22b26",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datasource='web_search'\n"
          ]
        }
      ],
      "source": [
        "# Question that needs web search\n",
        "print(question_router.invoke({\"question\": \"Who are the confirmed headliners for Austin City Limits Music Festival 2025?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc43b99",
      "metadata": {},
      "source": [
        "### Retrieval Grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d1221d80",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# Define data model for document evaluation\n",
        "class GradeDocuments(BaseModel):\n",
        "   \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "   binary_score: str = Field(\n",
        "       description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "   )\n",
        "\n",
        "\n",
        "# Initialize LLM and generate structured output through function calling\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Create prompt template containing system message and user question\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
        "   If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "   It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "   Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "   [\n",
        "       (\"system\", system),\n",
        "       (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "   ]\n",
        ")\n",
        "\n",
        "# Create document retrieval grader\n",
        "retrieval_grader = grade_prompt | structured_llm_grader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "927cac10",
      "metadata": {},
      "source": [
        "Using the created `retrieval_grader` to evaluate the document retrieval results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4af0027",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ],
      "source": [
        "# Set user question\n",
        "question = \"According to the European approach, what are the current limitations of AI applications in healthcare settings?According to the European approach, what are the current limitations of AI applications in healthcare settings?\"\n",
        "\n",
        "# Retrieve relevant documents for the question\n",
        "docs = pdf_retriever.invoke(question)\n",
        "\n",
        "# Get content from retrieved document\n",
        "retrieved_doc = docs[0].page_content\n",
        "\n",
        "# Print evaluation results\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": retrieved_doc}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54dce7a1",
      "metadata": {},
      "source": [
        "### Creating RAG Chain for Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "992ef15a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Get prompt from LangChain Hub (RAG prompt can be freely modified)\n",
        "prompt = hub.pull(\"teddynote/rag-prompt\")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# Document formatting function\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(\n",
        "        [\n",
        "            f'<document><content>{doc.page_content}</content><source>{doc.metadata[\"source\"]}</source><page>{doc.metadata[\"page\"]+1}</page></document>'\n",
        "            for doc in docs\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Create RAG chain\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbc96e3",
      "metadata": {},
      "source": [
        "Now we'll use our `rag_chain` to generate responses by passing questions to it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f8d16e04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "According to the European approach, the current limitations of AI applications in healthcare settings include:\n",
            "\n",
            "1. **Confined Applications**: AI is primarily used for administrative tasks and diagnostics, with limited deployment in broader healthcare functions.\n",
            "2. **Data Governance**: AI tools must demonstrate compliance with data privacy regulations beyond GDPR, and show a clear return on investment.\n",
            "3. **Patient Trust**: There are significant concerns regarding personal data security and patient trust in AI systems.\n",
            "4. **Organizational and Skill Challenges**: There is a need for upskilling healthcare practitioners and better communication between data experts and healthcare professionals.\n",
            "5. **Limited Uptake**: Many initiatives remain small-scale, and the adoption of AI by hospitals is still limited.\n",
            "\n",
            "**Source**\n",
            "- ../data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf (page 15)\n"
          ]
        }
      ],
      "source": [
        "# Generate answer by passing question to RAG chain\n",
        "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e9f601",
      "metadata": {},
      "source": [
        "### Adding Hallucination Checker for Responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "40ec0e97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define data model for hallucination check\n",
        "class GradeHallucinations(BaseModel):\n",
        "   \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "   binary_score: str = Field(\n",
        "       description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "   )\n",
        "\n",
        "# Initialize LLM with function calling\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Set up prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
        "   Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "\n",
        "# Create prompt template\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "   [\n",
        "       (\"system\", system),\n",
        "       (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "   ]\n",
        ")\n",
        "\n",
        "# Create hallucination grader\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8550b7cf",
      "metadata": {},
      "source": [
        "Evaluate hallucinations in the generated response using our created `hallucination_grader` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cb593684",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate hallucinations in generated response using grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "110eb9b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GradeAnswer(BaseModel):\n",
        "   \"\"\"Binary scoring to evaluate the appropriateness of answers to questions\"\"\"\n",
        "   binary_score: str = Field(\n",
        "       description=\"Indicate 'yes' or 'no' whether the answer solves the question\"\n",
        "   )\n",
        "\n",
        "# Initialize LLM with function calling\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Set up prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
        "    Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "\n",
        "# Create prompt template\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "   [\n",
        "       (\"system\", system),\n",
        "       (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "   ]\n",
        ")\n",
        "\n",
        "# Create answer grader by combining prompt template and structured LLM grader\n",
        "answer_grader = answer_prompt | structured_llm_grader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "66a26ad6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate if the generated answer resolves the question using grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fc11dd",
      "metadata": {},
      "source": [
        "### Query Rewriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c6eb92e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
        "\n",
        "# Define Query Rewriter prompt (can be freely modified)\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
        "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "\n",
        "# Create Query Rewriter prompt template\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create Query Rewriter\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e819c4",
      "metadata": {},
      "source": [
        "Create an enhanced question by submitting a query to the generated `question_rewriter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "210090cf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What are the current limitations of AI applications in healthcare settings as outlined by the European approach?'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate an improved question by passing a question to the query rewriter\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d5ee42",
      "metadata": {},
      "source": [
        "## Tools\n",
        "\n",
        "### Web Search Tool\n",
        "The **Web Search Tool** is a critical component of **Adaptive RAG** , used to retrieve the latest information. This tool supports users in obtaining quick and accurate answers to questions related to recent events.\n",
        "\n",
        "* **Setup** : Prepare the web search tool to search for the most current information.\n",
        "* **Perform Search** : Search the web for relevant information based on the user's query.\n",
        "* **Result Analysis** : Analyze the retrieved search results to provide the most appropriate information for the user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e004263c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_teddynote.tools.tavily import TavilySearch\n",
        "\n",
        "# Create web search tool\n",
        "web_search_tool = TavilySearch(max_results=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d60abe",
      "metadata": {},
      "source": [
        "Run the web search tool and check the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c13be8f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'title': 'Austin City Limits Music Festival 2025 - Holler', 'url': 'https://holler.country/festivals/austin-city-limits-music-festival-2025', 'content': '* The dates for Austin City Limits Music Festival 2025 are yet to be confirmed - October 3-5 and 10-12 are speculative dates. ... were able to snag acts like Chris Stapleton, Sturgill Simpson, Dua Lipa, Tyler the Creator, Blink-182 and more as headliners for the thrilling installment. ... When tickets for Austin City Limits Music Festival 2025', 'score': 0.93735445, 'raw_content': \"Austin City Limits Music Festival 2025\\nBy Lydia Farthing\\n\\nLink copied\\n* The dates for Austin City Limits Music Festival 2025 are yet to be confirmed - October 3-5 and 10-12 are speculative dates.\\nAustin City Limits Music Festival is one of the year's most exciting events, regardless of your genre preferences. Welcoming acts from all across the musical spectrum, ACL is a once in a lifetime experience that just so happens to take place twice a year.\\nBoasting two consecutive weekends jam-packed with sizzling up-and-comers, established hitmakers and award-winning entertainers, it's an unforgettable spread that will likely make its return to Austin, Texas in October of 2025.\\nHere, we'll have all the up-to-date news on line-up announcements, the latest ticket and festival pass details, and any further exciting info and details for 2025. \\nSince opening its gates in 2002, ACL has hosted well over 130 artists and welcomed more than 225,000 attendees to Zilker Park every year.\\nWith a truly sprawling lineup of talent teed up across the 2024 weekends, organizers were able to snag acts like Chris Stapleton, Sturgill Simpson, Dua Lipa, Tyler the Creator, Blink-182 and more as headliners for the thrilling installment.\\nRepresenting the country, Americana and roots genres across the event were Caamp, The Red Clay Strays, Orville Peck, Medium Build, Mickey Guyton, Asleep at the Wheel, Richy Mitch & the Coal Miners, Katie Pruitt, Tanner Adell, Emily Nenni, Tyler Halverson and viral sensation Dasha, among others.\\nWhen tickets for Austin City Limits Music Festival 2025 are released, they'll be available to purchase below:\\nTo support Holler, book your stay for Austin City Limits Music Festival 2025 below:\\nCheck out other country festivals set for 2024 and 2025.\\n“Let's Ride”: Jelly Roll Features in John Cena's New TV Series, ‘What Drives You’\\n“I'm Just Asking For a Little Bit of Grace”: Bailey Zimmerman Apologises for Drunken Crash My Playa Performance\\n“There's an Awful Lot of Freedom to The Songs”: Kenny Chesney Finalises Setlist Ahead of Las Vegas Sphere Residency\\nWATCH: Parker McCollum Covers Toby Keith's ‘Courtesy of the Red, White and Blue’ at Liberty Ball for Trump's Inauguration\\nGet the best of Country in your inbox\\n\"}, {'title': 'Austin City Limits Music Festival 2025 | ACL Festival - Visit Austin', 'url': 'https://www.austintexas.org/events/acl-fest/', 'content': 'The Austin City Limits Music Festival began in 2002 and was named after the music TV series \"Austin City Limits,\" the longest-running music television show in history. Over the years, ACL has hosted international superstars, including Metallica, the Red Hot Chili Peppers, Kendrick Lamar, Arcade Fire, Jay Z, and Billie Eilish.', 'score': 0.8793233, 'raw_content': 'Austin City Limits 2025\\nAustin City Limits Music Festival (ACL) is the city’s highly anticipated music festival that takes place every October. The festival features a diverse lineup of acts yearly with nine stages, 100+ performances – and, best of all, two weekends. Learn more about this year’s lineup and what to expect for the 2025 festival (October 3-5 and 10-12, 2025).\\nAbout Austin City Limits Festival\\nThe Austin City Limits Music Festival began in 2002 and was named after the music TV series “Austin City Limits,” the longest-running music television show in history. Over the years, ACL has hosted international superstars, including Metallica, the Red Hot Chili Peppers, Kendrick Lamar, Arcade Fire, Jay Z, and Billie Eilish. Festival goers may also get the chance to catch the next big act in their early career on one of the many stages.\\nThe ACL festival is held at\\xa0Zilker Park\\xa0in South Austin and can be accessed through three different entrances open from midday to 10 PM. While there are plenty of accommodations near Zilker Park, it’s essential to book early—hotels\\xa0in the area fill up quickly!\\nACL Festival. Credit Taylor Regulski. Courtesy ACL Festival.\\xa0\\nACL Lineup\\nACL features nine stages showcasing solo artists and bands across genres such as hip-hop, rock, country, folk, and EDM. While the ACL lineup changes annually, it consistently offers a diverse selection of stars, emerging musicians, and local talent. The 2025 lineup has not been announced yet, however past headliners included Chris Stapleton, Blink-182, Dua Lipa, Tyler the Creator and more.\\nVisit aclfestival.com for the lineup announcements.\\nFestival Experience\\nFood & Drink\\nAt the Austin City Limits music festival, you can indulge in a wide variety of food and drink booths at ACL Eats, featuring delicious options from some of Austin’s most well-known eateries, breweries, and food trucks.\\xa0\\nWhether you’re craving gourmet tacos, artisanal ice cream\\xa0or refreshing fresh juice, the festival’s culinary offerings are sure to delight every palate. Foodies will appreciate the diverse selection of local favorites, making it a true taste of Austin’s vibrant food scene. Plus, visit the Barton Springs Beer Hall for a variety of craft beers from local and regional brewers, or the Wine Grove for wine options.\\nCredit Taylor Regulski. Courtesy ACL Festival.\\nShopping, Perks & Amenities\\nFor those looking to shop or find unique souvenirs, ACL’s Art Market is a must-visit. Here, you can browse through booths featuring handcrafted jewelry, clothing, posters, and other artisanal goods created by local and national artists. It’s a great place to pick up a memento of your festival experience or find a special gift. Or, swing by the Festival and Artist Merch tent for ACL-branded swag and takeaways from your favorite performers.\\nThe festival also provides several amenities to enhance your experience, including hydration stations where you can refill your water bottle, ensuring you stay refreshed throughout the day. Additionally, there are ample restroom facilities, including accessible options, to accommodate all guests comfortably.\\nACL also offers various relaxation zones where you can take a break from the crowds, enjoy some shade, and recharge before heading back to the music.\\xa0\\nKid-Friendly ACL\\nAnother standout feature of ACL is Austin Kiddie Limits, a dedicated kid-friendly area that ensures fun for the whole family. This lively section boasts its own stage with kid-focused performances and interactive activities, including arts and crafts, face painting, and more.\\xa0\\nParents can relax knowing that their little ones are entertained and engaged in a safe and welcoming environment. Austin Kiddie Limits also offers family-friendly amenities such as shaded areas and comfortable seating, making it easy for everyone to enjoy the festival at their own pace.\\nVisitor Information & Accessibility\\nCredit Charles Reagan. Courtesy ACL Fest.\\nGetting There\\nSince there’s no parking directly in Zilker Park or the surrounding area, you’ll want to take advantage of the ACL shuttle, which picks folks up at\\xa0Republic Square Park\\xa0in Downtown Austin. You can also use the Cap Metro, which has several stops near Zilker Park. However, the easiest way to get to and from the festival is by foot or bike—there are plenty of bike racks nearby. Plus, Uber and Lyft pickup and dropoff are on Barton Springs Road, along with plenty of pedi-cabs for a fun ride.\\nFind\\xa0more transportation options.\\nAccessibility\\nACL Festival is committed to making sure all festival fans have an enjoyable experience.\\xa0There are specially designated lanes at the Festival entrances for patrons with disabilities, along with parking accommodations and more. Check the\\xa0ACL website\\xa0for more details or reach out to\\xa0access@aclfestival.com\\xa0to request necessary accommodations.\\xa0\\nGet Tickets\\nOpt for 1-day or 3-day passes, General Admission+ for extra amenities, or VIP tickets for exclusive access to private lounges and viewing decks. Tickets for ACL 2025 will go on sale in the summer so be sure to check out the ACL Festival website for the most up-to-date information.\\xa0\\nStart planning for Austin City Limits 2025 today! Secure your tickets and prepare for an unforgettable experience filled with incredible music, delicious food, and memorable moments.\\n\\nGet A Free Visitors Guide\\n\\n\\nSign Up for  Our E-News\\n\\n \\n#TrueAustin\\n512-478-0098\\xa0or\\xa0512-474-5171\\nAustin Visitor Center: 602 E. Fourth St, Austin, 78701\\nVisit Austin Admin: 111 Congress Ave, Suite 700 Austin, 78701\\n  \\nCookies are used for measurement, ads and optimization. By continuing to use our site, you agree to our privacy policy.\\xa0\\n'}, {'title': 'Austin City Limits Music Festival | Austin, TX - Visit Austin', 'url': 'https://www.austintexas.org/event/austin-city-limits-music-festival/385066/', 'content': \"It's that time of year again! Back for its 24th year, Austin City Limits Music Festival (October 3-5 and October 10-12, 2025) brings the magic of the famed public TV series Austin City Limits outside the studio and into Austin's most beloved park. ACL Festival features a diverse lineup of acts every year with 9 stages, 100+ performances – and, best of all, two weekends. Ticket and lineup\", 'score': 0.568057, 'raw_content': \"Austin City Limits Music Festival\\nAbout\\nIt's that time of year again! Back for its 24th year, Austin City Limits Music Festival (October 3-5 and October 10-12, 2025) brings the magic of the famed public TV series Austin City Limits outside the studio and into Austin's most beloved park. ACL Festival features a diverse lineup of acts every year with 9 stages, 100+ performances – and, best of all, two weekends. Ticket and lineup details will be announced closer to the event date. Visit the ACL Music Festival website for the most up-to-date information.\\nLearn more about the Austin City Limits Music Festival here.\\xa0\\nIt's that time of year again! Back for its 24th year, Austin City Limits Music Festival (October 3-5 and October 10-12, 2025) brings the magic of the famed public TV series Austin City Limits outside the studio and into Austin's most beloved park. ACL Festival features a diverse lineup of acts every year with 9 stages, 100+ performances – and, best of all, two weekends. Ticket and lineup details will be announced closer to the event date. Visit the ACL Music Festival website for the most up-to-date information.\\nLearn more about the Austin City Limits Music Festival here.\\xa0\\n\\nGet A Free Visitors Guide\\n\\n\\nSign Up for  Our E-News\\n\\n \\n#TrueAustin\\n512-478-0098\\xa0or\\xa0512-474-5171\\nAustin Visitor Center: 602 E. Fourth St, Austin, 78701\\nVisit Austin Admin: 111 Congress Ave, Suite 700 Austin, 78701\\n  \\nCookies are used for measurement, ads and optimization. By continuing to use our site, you agree to our privacy policy.\\xa0\\n\"}]\n"
          ]
        }
      ],
      "source": [
        "# Call the web search tool\n",
        "result = web_search_tool.search(\"Who are the confirmed headliners for Austin City Limits Music Festival 2025?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1904c95c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Austin City Limits Music Festival 2025 - Holler',\n",
              " 'url': 'https://holler.country/festivals/austin-city-limits-music-festival-2025',\n",
              " 'content': '* The dates for Austin City Limits Music Festival 2025 are yet to be confirmed - October 3-5 and 10-12 are speculative dates. ... were able to snag acts like Chris Stapleton, Sturgill Simpson, Dua Lipa, Tyler the Creator, Blink-182 and more as headliners for the thrilling installment. ... When tickets for Austin City Limits Music Festival 2025',\n",
              " 'score': 0.93735445,\n",
              " 'raw_content': \"Austin City Limits Music Festival 2025\\nBy Lydia Farthing\\n\\nLink copied\\n* The dates for Austin City Limits Music Festival 2025 are yet to be confirmed - October 3-5 and 10-12 are speculative dates.\\nAustin City Limits Music Festival is one of the year's most exciting events, regardless of your genre preferences. Welcoming acts from all across the musical spectrum, ACL is a once in a lifetime experience that just so happens to take place twice a year.\\nBoasting two consecutive weekends jam-packed with sizzling up-and-comers, established hitmakers and award-winning entertainers, it's an unforgettable spread that will likely make its return to Austin, Texas in October of 2025.\\nHere, we'll have all the up-to-date news on line-up announcements, the latest ticket and festival pass details, and any further exciting info and details for 2025. \\nSince opening its gates in 2002, ACL has hosted well over 130 artists and welcomed more than 225,000 attendees to Zilker Park every year.\\nWith a truly sprawling lineup of talent teed up across the 2024 weekends, organizers were able to snag acts like Chris Stapleton, Sturgill Simpson, Dua Lipa, Tyler the Creator, Blink-182 and more as headliners for the thrilling installment.\\nRepresenting the country, Americana and roots genres across the event were Caamp, The Red Clay Strays, Orville Peck, Medium Build, Mickey Guyton, Asleep at the Wheel, Richy Mitch & the Coal Miners, Katie Pruitt, Tanner Adell, Emily Nenni, Tyler Halverson and viral sensation Dasha, among others.\\nWhen tickets for Austin City Limits Music Festival 2025 are released, they'll be available to purchase below:\\nTo support Holler, book your stay for Austin City Limits Music Festival 2025 below:\\nCheck out other country festivals set for 2024 and 2025.\\n“Let's Ride”: Jelly Roll Features in John Cena's New TV Series, ‘What Drives You’\\n“I'm Just Asking For a Little Bit of Grace”: Bailey Zimmerman Apologises for Drunken Crash My Playa Performance\\n“There's an Awful Lot of Freedom to The Songs”: Kenny Chesney Finalises Setlist Ahead of Las Vegas Sphere Residency\\nWATCH: Parker McCollum Covers Toby Keith's ‘Courtesy of the Red, White and Blue’ at Liberty Ball for Trump's Inauguration\\nGet the best of Country in your inbox\\n\"}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# heck the first result of the web search result\n",
        "result[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ac37855",
      "metadata": {},
      "source": [
        "## Graph Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70ab91c2",
      "metadata": {},
      "source": [
        "### Defining graph states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6d23ab6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "\n",
        "# Define the state of the graph\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Data model representing the state of the graph\n",
        "    Attributes:\n",
        "        question: Question\n",
        "        generation: LLM generated answer\n",
        "        documents: List of documents\n",
        "    \"\"\"\n",
        "    question: Annotated[str, \"User question\"]\n",
        "    generation: Annotated[str, \"LLM generated answer\"]\n",
        "    documents: Annotated[List[str], \"List of documents\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f266cc42",
      "metadata": {},
      "source": [
        "## Define Graph Flows\n",
        "\n",
        "**Graph Flow** is defined to clarify the operation of **Adaptive RAG** . In this stage, the graph's state and transitions are established to enhance query processing efficiency.\n",
        "\n",
        "- **State Definition** : Clearly define each state of the graph to track the progression of the query.\n",
        "- **Transition Setup** : Configure transitions between states to ensure the query follows the appropriate path.\n",
        "- **Flow Optimization** : Optimize the graph's flow to improve the accuracy of information retrieval and generation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "633bf00c",
      "metadata": {},
      "source": [
        "### Define Nodes\n",
        "\n",
        "Define the nodes to be utilized:\n",
        "\n",
        "- `retrieve` : Document retrieval node\n",
        "- `generate` : Answer generation node\n",
        "- `grade_documents` : Document relevance evaluation node\n",
        "- `transform_query` : Question rewriting node\n",
        "- `web_search` : Web search node\n",
        "- `route_question` : Question routing node\n",
        "- `decide_to_generate` : Answer generation decision node\n",
        "- `hallucination_check` : Hallucination assessment node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ee6f34d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "# Document retrieval node\n",
        "def retrieve(state):\n",
        "    print(\"==== [RETRIEVE] ====\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Perform document retrieval\n",
        "    documents = pdf_retriever.invoke(question)\n",
        "    return {\"documents\": documents}\n",
        "\n",
        "\n",
        "# Answer generation node\n",
        "def generate(state):\n",
        "    print(\"==== [GENERATE] ====\")\n",
        "    # Get question and retrieved documents\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Generate RAG answer\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"generation\": generation}\n",
        "\n",
        "\n",
        "# Document relevance evaluation node\n",
        "def grade_documents(state):\n",
        "    print(\"==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\")\n",
        "    # Get question and retrieved documents\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Calculate relevance score for each document\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            # Add relevant documents\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            # Skip irrelevant documents\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs}\n",
        "\n",
        "\n",
        "# Question rewriting node\n",
        "def transform_query(state):\n",
        "    print(\"==== [TRANSFORM QUERY] ====\")\n",
        "    # Get question and retrieved documents\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Rewrite question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"question\": better_question}\n",
        "\n",
        "\n",
        "# Web search node\n",
        "def web_search(state):\n",
        "    print(\"==== [WEB SEARCH] ====\")\n",
        "    # Get question and retrieved documents\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Perform web search\n",
        "    web_results = web_search_tool.invoke({\"query\": question})\n",
        "    web_results_docs = [\n",
        "        Document(\n",
        "            page_content=web_result[\"content\"],\n",
        "            metadata={\"source\": web_result[\"url\"]},\n",
        "        )\n",
        "        for web_result in web_results\n",
        "    ]\n",
        "\n",
        "    return {\"documents\": web_results_docs}\n",
        "\n",
        "\n",
        "# Question routing node\n",
        "def route_question(state):\n",
        "    print(\"==== [ROUTE QUESTION] ====\")\n",
        "    # Get question\n",
        "    question = state[\"question\"]\n",
        "    # Route question\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    # Route nodes based on routing result\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"==== [ROUTE QUESTION TO WEB SEARCH] ====\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"==== [ROUTE QUESTION TO VECTORSTORE] ====\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "\n",
        "# Document relevance evaluation node\n",
        "def decide_to_generate(state):\n",
        "    print(\"==== [DECISION TO GENERATE] ====\")\n",
        "    # Get document retrieval results\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # If all documents are irrelevant\n",
        "        print(\n",
        "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # If there are relevant documents\n",
        "        print(\"==== [DECISION: GENERATE] ====\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def hallucination_check(state):\n",
        "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
        "    # Get question and retrieved documents\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    # Hallucination evaluation\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check for hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
        "\n",
        "        # Evaluate answer relevance\n",
        "        print(\"==== [GRADE GENERATED ANSWER vs QUESTION] ====\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "\n",
        "        # Process based on relevance evaluation results\n",
        "        if grade == \"yes\":\n",
        "            print(\"==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\")\n",
        "            return \"relevant\"\n",
        "        else:\n",
        "            print(\"==== [DECISION: GENERATED ANSWER DOES NOT ADDRESS QUESTION] ====\")\n",
        "            return \"not relevant\"\n",
        "    else:\n",
        "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
        "        return \"hallucination\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694ef29c",
      "metadata": {},
      "source": [
        "### Define Edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c21c1de6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question routing node\n",
        "def route_question(state):\n",
        "    print(\"==== [ROUTE QUESTION] ====\")\n",
        "    # Get question\n",
        "    question = state[\"question\"]\n",
        "    # Route question\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    # Route nodes based on routing result\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"==== [ROUTE QUESTION TO WEB SEARCH] ====\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"==== [ROUTE QUESTION TO VECTORSTORE] ====\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "# Document relevance evaluation node\n",
        "def decide_to_generate(state):\n",
        "    print(\"==== [DECISION TO GENERATE] ====\")\n",
        "    # Get question and document search results\n",
        "    question = state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "    if not filtered_documents:\n",
        "        # If all documents are irrelevant, rewrite the question\n",
        "        print(\n",
        "            \"==== [DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY] ====\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # If relevant documents exist, generate answer\n",
        "        print(\"==== [DECISION: GENERATE] ====\")\n",
        "        return \"generate\"\n",
        "\n",
        "def hallucination_check(state):\n",
        "    print(\"==== [CHECK HALLUCINATIONS] ====\")\n",
        "    # Get question and document search results\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "    # Hallucination evaluation\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "    # Check for hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\")\n",
        "        # Evaluate answer relevance\n",
        "        print(\"==== [GRADE GENERATED ANSWER vs QUESTION] ====\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        # Process based on relevance evaluation results\n",
        "        if grade == \"yes\":\n",
        "            print(\"==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\")\n",
        "            return \"relevant\"\n",
        "        else:\n",
        "            print(\"==== [DECISION: GENERATED ANSWER DOES NOT ADDRESS QUESTION] ====\")\n",
        "            return \"not relevant\"\n",
        "    else:\n",
        "        print(\"==== [DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY] ====\")\n",
        "        return \"hallucination\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99033673",
      "metadata": {},
      "source": [
        "### **Graph Compilation**\n",
        "\n",
        "In the **Graph Compilation** stage, the **Adaptive RAG** workflow is constructed and brought to an executable state. This process defines the entire query processing flow by connecting each node and edge of the graph.\n",
        "\n",
        "* **Node Definition** : Define each node to clarify the graph's states and transitions.\n",
        "* **Edge Configuration** : Set up edges between nodes to ensure the query follows the appropriate path.\n",
        "* **Workflow Construction** : Build the overall graph flow to maximize the efficiency of information retrieval and generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e895f668",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# Initialize graph state\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define nodes\n",
        "workflow.add_node(\"web_search\", web_search)  # Web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # Document retrieval\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # Document evaluation\n",
        "workflow.add_node(\"generate\", generate)  # Answer generation\n",
        "workflow.add_node(\"transform_query\", transform_query)  # Query transformation\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "   START,\n",
        "   route_question,\n",
        "   {\n",
        "       \"web_search\": \"web_search\",  # Route to web search\n",
        "       \"vectorstore\": \"retrieve\",  # Route to vectorstore\n",
        "   },\n",
        ")\n",
        "workflow.add_edge(\"web_search\", \"generate\")  # Generate answer after web search\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")  # Evaluate documents after retrieval\n",
        "workflow.add_conditional_edges(\n",
        "   \"grade_documents\",\n",
        "   decide_to_generate,\n",
        "   {\n",
        "       \"transform_query\": \"transform_query\",  # Query transformation needed\n",
        "       \"generate\": \"generate\",  # Answer generation possible\n",
        "   },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")  # Retrieve documents after query transformation\n",
        "workflow.add_conditional_edges(\n",
        "   \"generate\",\n",
        "   hallucination_check,\n",
        "   {\n",
        "       \"hallucination\": \"generate\",  # Regenerate if hallucination occurs\n",
        "       \"relevant\": END,  # Pass if answer is relevant\n",
        "       \"not relevant\": \"transform_query\",  # Transform query if answer is not relevant\n",
        "   },\n",
        ")\n",
        "\n",
        "# Compile graph\n",
        "app = workflow.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "195b7e06",
      "metadata": {},
      "source": [
        "## Graph Utilization\n",
        "\n",
        "In the **Graph Utilization** stage, the execution of **Adaptive RAG** is used to verify query processing results. This process follows the nodes and edges of the graph to generate the final outcome.\n",
        "\n",
        "* **Graph Execution** : Execute the defined graph by following the query's flow.\n",
        "* **Result Verification** : Review the generated results after graph execution to confirm that the query was processed appropriately.\n",
        "* **Result Analysis** : Analyze the generated results to evaluate their alignment with the query's objectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2a554da2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== [ROUTE QUESTION] ====\n",
            "==== [ROUTE QUESTION TO VECTORSTORE] ====\n",
            "==== [RETRIEVE] ====\n",
            "==== [CHECK DOCUMENT RELEVANCE TO QUESTION] ====\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "==== [DECISION TO GENERATE] ====\n",
            "==== [DECISION: GENERATE] ====\n",
            "==== [GENERATE] ====\n",
            "\n",
            "==================================================\n",
            "🔄 Node: \u001b[1;36mgenerate\u001b[0m 🔄\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "The current limitations of AI applications in healthcare settings, as outlined by the European approach, include:\n",
            "\n",
            "1. **Confined Applications**: AI is primarily used for administrative tasks and diagnostics, with limited deployment in broader healthcare functions.\n",
            "2. **Data Privacy Concerns**: There are significant concerns regarding data privacy and the need for AI tools to demonstrate compliance with GDPR and ensure patient trust.\n",
            "3. **Fragmented Systems**: The fragmentation of European healthcare systems complicates data access and interoperability, hindering the effective use of AI.\n",
            "4. **Organizational and Skill Challenges**: There is a lack of common language and understanding between data experts and healthcare professionals, necessitating upskilling and tailored educational programs.\n",
            "5. **Limited Hospital Uptake**: Many AI initiatives remain small-scale, and hospitals have been slow to adopt these technologies.\n",
            "\n",
            "**Source**\n",
            "- A European Approach to Artificial Intelligence - A Policy Perspective.pdf (page 14-15)==== [CHECK HALLUCINATIONS] ====\n",
            "==== [DECISION: GENERATION IS GROUNDED IN DOCUMENTS] ====\n",
            "==== [GRADE GENERATED ANSWER vs QUESTION] ====\n",
            "==== [DECISION: GENERATED ANSWER ADDRESSES QUESTION] ====\n"
          ]
        }
      ],
      "source": [
        "from langchain_teddynote.messages import stream_graph\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "import uuid\n",
        "\n",
        "# Configure settings (maximum recursion, thread_id)\n",
        "config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": uuid.uuid4()})\n",
        "\n",
        "# Input question\n",
        "inputs = {\n",
        "   \"question\": \"What are the current limitations of AI applications in healthcare settings as outlined by the European approach?\",\n",
        "}\n",
        "\n",
        "# Execute graph\n",
        "stream_graph(app, inputs, config, [\"agent\", \"rewrite\", \"generate\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
