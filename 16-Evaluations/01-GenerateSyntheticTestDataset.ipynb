{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Generate synthetic test dataset (with RAGAS)\n",
        "\n",
        "- Author: [Yoonji](https://github.com/samdaseuss)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "### Welcome Back!\n",
        "Hi everyone! Welcome to our first lecture in the evaluation section. We're going to try something special today! While we've been building RAG systems, we haven't really talked about how to test if they're working well. To properly evaluate a RAG system, we need good test data - and that's exactly what we'll be creating in this tutorial! We'll learn how to build datasets that will help us measure our RAG pipeline's performance.\n",
        "\n",
        "### Today, what we are going to learn...\n",
        "We'll be using RAGAS to generate evaluation datasets. Specifically, we'll dive into:\n",
        "* How to preprocess documents for evaluation\n",
        "* How to define various evaluation objects\n",
        "* How to generate different types of test questions by configuring data distributions\n",
        "\n",
        "Through hands-on practice, you'll learn all these techniques and be able to create your own evaluation datasets!\n",
        "\n",
        "### We're going to learn ...\n",
        "The main goal of this section is to create test datasets that can objectively evaluate our RAG system. Think of it as building a really good test that can tell us exactly how well our RAG system is performing on different types of questions and scenarios.\n",
        "\n",
        "By the end of this tutorial, you'll have all the tools you need to create comprehensive test datasets that will help you understand your RAG system's strengths and areas for improvement. Ready to get started? Let's dive in!\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [Looking Back at What We've Learned](#looking-back-at-what-weve-learned)\n",
        "    * [We Have Learned About RAG](#we-have-learned-about-rag)\n",
        "    * [Is Our RAG Design Effective?](#is-our-rag-design-effective)\n",
        "    * [Why Use Synthetic Test Dataset?](#why-use-synthetic-test-dataset)\n",
        "- [Installation](#installation)\n",
        "- [What is RAGAS](#what-is-ragas)\n",
        "- [RAGAS in Python](#ragas-in-python)\n",
        "- [Document](#document)\n",
        "- [Document Preprocessing](#document-preprocessing)\n",
        "- [Dataset Generation](#dataset-generation)\n",
        "- [Distribution of Question Types](#distribution-of-question-types)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Testset Generation for RAG](https://docs.ragas.io/en/stable/getstarted/rag_testset_generation/)\n",
        "- [Testset Generation for RAG : 📚 Core Concepts > Test Data Generation > RAG](https://docs.ragas.io/en/stable/concepts/test_data_generation/rag/)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Generate synthetic test dataset (with RAGAS)\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7bbbc5",
      "metadata": {},
      "source": [
        "## Looking Back at What We've Learned\n",
        "\n",
        "### We Have Learned About RAG\n",
        "\n",
        "LLM is a powerful technology, but it has limitations in reflecting real-time information due to the constraints of its training data.\n",
        "\n",
        "For example, let's say NASA discovered a new planet yesterday, making the total number of planets in the solar system nine. What would happen if we asked an LLM about the number of planets in the solar system? Because LLM responds based on its trained data, it would say there are eight planets. We call this phenomenon 'hallucination,' and to resolve this, we need to wait for a model 'version up.'\n",
        "\n",
        "RAG emerged to overcome these limitations. Instead of immediately responding to user questions, the RAG pipeline first searches for the latest information from external knowledge repositories and then generates responses based on this information. This enables the system to provide answers that reflect the most up-to-date information.\n",
        "\n",
        "### Is Our RAG Design Effective?\n",
        "\n",
        "You have learned various techniques for implementing RAG. Some of you may have already built your own RAG systems and applied them to your work.\n",
        "\n",
        "However, we need to ask an important question: Is our RAG system truly a 'good' RAG? How can we judge the quality of RAG?\n",
        "\n",
        "Simply saying \"this RAG doesn't perform well\" is not enough. We need to be able to measure and verify RAG's performance through objective evaluation metrics.\n",
        "\n",
        "### Why Use Synthetic Test Dataset?\n",
        "\n",
        "Evaluating the performance of RAG systems is a crucial process. However, manually creating hundreds of question-answer pairs requires enormous time and effort.\n",
        "\n",
        "Moreover, manually written questions often remain at a simple and superficial level, making it difficult to thoroughly evaluate the performance of RAG systems.\n",
        "\n",
        "By utilizing synthetic data to solve these problems, we can reduce developer time spent on building test datasets by up to 90%. Additionally, it enables more thorough performance evaluation by automatically generating test cases of various difficulty levels and types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616661ad",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "To proceed with this tutorial, you need to install the `RAGAS` package. Through the command below, we'll install the `RAGAS` package, and immediately after, we'll explore the concept of `RAGAS` and learn about Python's `RAGAS package` in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "17fcdae9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1557aa10",
      "metadata": {},
      "source": [
        "## What is RAGAS?\n",
        "RAGAS (Retrieval Augmented Generation Assessment Suite) is a comprehensive evaluation framework designed to assess the performance of RAG systems. It helps developers and researchers measure how well their RAG implementations are working through various metrics and evaluation methods.\n",
        "\n",
        "Let's revisit the example we saw earlier.\n",
        "\n",
        "Let's say NASA discovered a new planet yesterday, making the total number of planets in our solar system nine. To evaluate the performance of a RAG system, let's ask the test question \"How many planets are in our solar system?\" RAGAS evaluates the system's response using these key metrics:\n",
        "\n",
        "1. `Answer Relevancy`: Checks if the answer directly addresses the question about the number of planets\n",
        "2. `Context Relevancy`: Checks if the system retrieved the recent NASA announcement instead of old astronomy textbooks\n",
        "3. `Faithfulness`: Checks if the answer about nine planets is based on the NASA announcement and not on outdated data\n",
        "4. `Context Precision`: Checks if the system used the NASA announcement efficiently without including unnecessary space information\n",
        "\n",
        "For example, if the RAG system responds with **outdated information** saying there are eight planets, RAGAS will give it a low context relevancy score. Or if it makes claims about the new planet that aren't in the NASA announcement, it will receive a low faithfulness score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e075b7",
      "metadata": {},
      "source": [
        "## RAGAS in Python\n",
        "You can easily use `RAGAS` with Python libraries.\n",
        "\n",
        "Ragas is a library that provides tools to supercharge the evaluation of Large Language Model (LLM) applications. It is designed to help you evaluate your LLM applications with ease and confidence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc7629d",
      "metadata": {},
      "source": [
        "## Document\n",
        "While the official RAGAS package website demonstrates tutorials using `markdown`, in this tutorial, we'll be working with `pdf` files. Please use the files located in the `data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6362e072",
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd08a6c",
      "metadata": {},
      "source": [
        "## Document Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bf1727b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Create a document loader\n",
        "loader = DirectoryLoader(file_path, glob=\"**/*.pdf\")\n",
        "\n",
        "# Load documents\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7cf0a637",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/Newwhitepaper_Agents2.pdf'}, page_content='Agents\\n\\nAuthors: Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic\\n\\nAcknowledgements\\n\\nReviewers and Contributors\\n\\nEvan Huang\\n\\nEmily Xue\\n\\nOlcan Sercinoglu\\n\\nSebastian Riedel\\n\\nSatinder Baveja\\n\\nAntonio Gulli\\n\\nAnant Nawalgaria\\n\\nCurators and Editors\\n\\nAntonio Gulli\\n\\nAnant Nawalgaria\\n\\nGrace Mollison\\n\\nTechnical Writer\\n\\nJoey Haymaker\\n\\nDesigner\\n\\nMichael Lanning\\n\\n2\\n\\nTable of contents\\n\\nIntroduction\\n\\nWhat is an agent?\\n\\nThe model\\n\\nThe tools\\n\\nThe orchestration layer\\n\\nAgents vs. models\\n\\nCognitive architectures: How agents operate\\n\\nTools: Our keys to the outside world\\n\\nExtensions\\n\\nSample Extensions\\n\\nFunctions\\n\\nUse cases\\n\\nFunction sample code\\n\\nData stores\\n\\nImplementation and application\\n\\nTools recap\\n\\nEnhancing model performance with targeted learning\\n\\nAgent quick start with LangChain\\n\\nProduction applications with Vertex AI agents\\n\\nSummary\\n\\nEndnotes\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n7\\n\\n8\\n\\n8\\n\\n12\\n\\n13\\n\\n15\\n\\n18\\n\\n21\\n\\n24\\n\\n27\\n\\n28\\n\\n32\\n\\n33\\n\\n35\\n\\n38\\n\\n40\\n\\n42\\n\\nThis combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent.\\n\\nIntroduction\\n\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools\\n\\nlike books, Google Search, or a calculator - to supplement their prior knowledge before\\n\\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools\\n\\nto access real-time information or suggest a real-world action. For example, a model can\\n\\nleverage a database retrieval tool to access specific information, like a customer\\'s purchase\\n\\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a\\n\\nuser\\'s query, a model can make various API calls to send an email response to a colleague\\n\\nor complete a financial transaction on your behalf. To do so, the model must not only have\\n\\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\n\\ndirected fashion. This combination of reasoning, logic, and access to external information\\n\\nthat are all connected to a Generative AI model invokes the concept of an agent, or a\\n\\nprogram that extends beyond the standalone capabilities of a Generative AI model. This\\n\\nwhitepaper dives into all these and associated aspects in more detail.\\n\\n4\\n\\nWhat is an agent?\\n\\nIn its most fundamental form, a Generative AI agent can be defined as an application that\\n\\nattempts to achieve a goal by observing the world and acting upon it using the tools that it\\n\\nhas at its disposal. Agents are autonomous and can act independently of human intervention,\\n\\nespecially when provided with proper goals or objectives they are meant to achieve. Agents\\n\\ncan also be proactive in their approach to reaching their goals. Even in the absence of\\n\\nexplicit instruction sets from a human, an agent can reason about what it should do next to\\n\\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this\\n\\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of\\n\\nbuilding at the time of publication.\\n\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational\\n\\ncomponents that drive the agent’s behavior, actions, and decision making. The combination\\n\\nof these components can be described as a cognitive architecture, and there are many\\n\\nsuch architectures that can be achieved by the mixing and matching of these components.\\n\\nFocusing on the core functionalities, there are three essential components in an agent’s\\n\\ncognitive architecture as shown in Figure 1.\\n\\n5\\n\\nFigure 1. General agent architecture and components\\n\\nThe model\\n\\nIn the scope of an agent, a model refers to the language model (LM) that will be utilized as\\n\\nthe centralized decision maker for agent processes. The model used by an agent can be one\\n\\nor multiple LM’s of any size (small / large) that are capable of following instruction based\\n\\nreasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models\\n\\ncan be general purpose, multimodal or fine-tuned based on the needs of your specific agent\\n\\narchitecture. For best production results, you should leverage a model that best fits your\\n\\ndesired end application and, ideally, has been trained on data signatures associated with the\\n\\ntools that you plan to use in the cognitive architecture. It’s important to note that the model is\\n\\ntypically not trained with the specific configuration settings (i.e. tool choices, orchestration/\\n\\nreasoning setup) of the agent. However, it’s possible to further refine the model for the\\n\\nagent’s tasks by providing it with examples that showcase the agent’s capabilities, including\\n\\ninstances of the agent using specific tools or reasoning steps in various contexts.\\n\\n6\\n\\nThe tools\\n\\nFoundational models, despite their impressive text and image generation, remain constrained\\n\\nby their inability to interact with the outside world. Tools bridge this gap, empowering agents\\n\\nto interact with external data and services while unlocking a wider range of actions beyond\\n\\nthat of the underlying model alone. Tools can take a variety of forms and have varying\\n\\ndepths of complexity, but typically align with common web API methods like GET, POST,\\n\\nPATCH, and DELETE. For example, a tool could update customer information in a database\\n\\nor fetch weather data to influence a travel recommendation that the agent is providing to\\n\\nthe user. With tools, agents can access and process real-world information. This empowers\\n\\nthem to support more specialized systems like retrieval augmented generation (RAG),\\n\\nwhich significantly extends an agent’s capabilities beyond what the foundational model can\\n\\nachieve on its own. We’ll discuss tools in more detail below, but the most important thing\\n\\nto understand is that tools bridge the gap between the agent’s internal capabilities and the\\n\\nexternal world, unlocking a broader range of possibilities.\\n\\nThe orchestration layer\\n\\nThe orchestration layer describes a cyclical process that governs how the agent takes in\\n\\ninformation, performs some internal reasoning, and uses that reasoning to inform its next\\n\\naction or decision. In general, this loop will continue until an agent has reached its goal or a\\n\\nstopping point. The complexity of the orchestration layer can vary greatly depending on the\\n\\nagent and task it’s performing. Some loops can be simple calculations with decision rules,\\n\\nwhile others may contain chained logic, involve additional machine learning algorithms, or\\n\\nimplement other probabilistic reasoning techniques. We’ll discuss more about the detailed\\n\\nimplementation of the agent orchestration layers in the cognitive architecture section.\\n\\n7\\n\\nAgents vs. models\\n\\nTo gain a clearer understanding of the distinction between agents and models, consider the\\n\\nfollowing chart:\\n\\nModels\\n\\nAgents\\n\\nKnowledge is limited to what is available in their training data.\\n\\nKnowledge is extended through the connection with external systems via tools\\n\\nSingle inference / prediction based on the user query. Unless explicitly implemented for the model, there is no management of session history or continuous context. (i.e. chat history)\\n\\nManaged session history (i.e. chat history) to allow for multi turn inference / prediction based on user queries and decisions made in the orchestration layer. In this context, a ‘turn’ is defined as an interaction between the interacting system and the agent. (i.e. 1 incoming event/ query and 1 agent response)\\n\\nNo native tool implementation.\\n\\nTools are natively implemented in agent architecture.\\n\\nNo native logic layer implemented. Users can form prompts as simple questions or use reasoning frameworks (CoT, ReAct, etc.) to form complex prompts to guide the model in prediction.\\n\\nNative cognitive architecture that uses reasoning frameworks like CoT, ReAct, or other pre-built agent frameworks like LangChain.\\n\\nCognitive architectures: How agents operate\\n\\nImagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant\\n\\npatrons which involves some cycle of planning, execution, and adjustment.\\n\\n8\\n\\nThey gather information, like the patron’s order and what ingredients are in the pantry\\n\\nand refrigerator.\\n\\nThey perform some internal reasoning about what dishes and flavor profiles they can\\n\\ncreate based on the information they have just gathered.\\n\\nThey take action to create the dish: chopping vegetables, blending spices, searing meat.\\n\\nAt each stage in the process the chef makes adjustments as needed, refining their plan as\\n\\ningredients are depleted or customer feedback is received, and uses the set of previous\\n\\noutcomes to determine the next plan of action. This cycle of information intake, planning,\\n\\nexecuting, and adjusting describes a unique cognitive architecture that the chef employs to\\n\\nreach their goal.\\n\\nJust like the chef, agents can use cognitive architectures to reach their end goals by\\n\\niteratively processing information, making informed decisions, and refining next actions\\n\\nbased on previous outputs. At the core of agent cognitive architectures lies the orchestration\\n\\nlayer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly\\n\\nevolving field of prompt engineering and associated frameworks to guide reasoning and\\n\\nplanning, enabling the agent to interact more effectively with its environment and complete\\n\\ntasks. Research in the area of prompt engineering frameworks and task planning for\\n\\nlanguage models is rapidly evolving, yielding a variety of promising approaches. While not an\\n\\nexhaustive list, these are a few of the most popular frameworks and reasoning techniques\\n\\navailable at the time of this publication:\\n\\nReAct, a prompt engineering framework that provides a thought process strategy for\\n\\nlanguage models to Reason and take action on a user query, with or without in-context\\n\\nexamples. ReAct prompting has shown to outperform several SOTA baselines and improve\\n\\nhuman interoperability and trustworthiness of LLMs.\\n\\n9\\n\\nChain-of-Thought (CoT), a prompt engineering framework that enables reasoning\\n\\ncapabilities through intermediate steps. There are various sub-techniques of CoT including\\n\\nself-consistency, active-prompt, and multimodal CoT that each have strengths and\\n\\nweaknesses depending on the specific application.\\n\\nTree-of-thoughts (ToT),, a prompt engineering framework that is well suited for\\n\\nexploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting\\n\\nand allows the model to explore various thought chains that serve as intermediate steps\\n\\nfor general problem solving with language models.\\n\\nAgents can utilize one of the above reasoning techniques, or many other techniques, to\\n\\nchoose the next best action for the given user request. For example, let’s consider an agent\\n\\nthat is programmed to use the ReAct framework to choose the correct actions and tools for\\n\\nthe user query. The sequence of events might go something like this:\\n\\n1. User sends query to the agent\\n\\n2. Agent begins the ReAct sequence\\n\\n3. The agent provides a prompt to the model, asking it to generate one of the next ReAct\\n\\nsteps and its corresponding output:\\n\\na. Question: The input question from the user query, provided with the prompt\\n\\nb. Thought: The model’s thoughts about what it should do next\\n\\nc. Action: The model’s decision on what action to take next\\n\\ni. This is where tool choice can occur\\n\\nii. For example, an action could be one of [Flights, Search, Code, None], where the first\\n\\n3 represent a known tool that the model can choose, and the last represents “no\\n\\ntool choice”\\n\\n10\\n\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\n\\ne. Observation: The result of the action / action input sequence\\n\\ni. This thought / action / action input / observation could repeat N-times as needed\\n\\nf. Final answer: The model’s final answer to provide to the original user query\\n\\n4. The ReAct loop concludes and a final answer is provided back to the user\\n\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\n\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide\\n\\na grounded, concise response back to the user based on the user’s original query. While\\n\\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge,\\n\\nit instead used a tool (Flights) to search for real-time external information. This additional\\n\\ninformation was provided to the model, allowing it to make a more informed decision based\\n\\non real factual data and to summarize this information back to the user.\\n\\n11\\n\\nIn summary, the quality of agent responses can be tied directly to the model’s ability to\\n\\nreason and act about these various tasks, including the ability to select the right tools, and\\n\\nhow well that tools has been defined. Like a chef crafting a dish with fresh ingredients and\\n\\nattentive to customer feedback, agents rely on sound reasoning and reliable information to\\n\\ndeliver optimal results. In the next section, we’ll dive into the various ways agents connect\\n\\nwith fresh data.\\n\\nTools: Our keys to the outside world\\n\\nWhile language models excel at processing information, they lack the ability to directly\\n\\nperceive and influence the real world. This limits their usefulness in situations requiring\\n\\ninteraction with external systems or data. This means that, in a sense, a language model\\n\\nis only as good as what it has learned from its training data. But regardless of how much\\n\\ndata we throw at a model, they still lack the fundamental ability to interact with the outside\\n\\nworld. So how can we empower our models to have real-time, context-aware interaction with\\n\\nexternal systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this\\n\\ncritical capability to the model.\\n\\nWhile they go by many names, tools are what create a link between our foundational models\\n\\nand the outside world. This link to external systems and data allows our agent to perform a\\n\\nwider variety of tasks and do so with more accuracy and reliability. For instance, tools can\\n\\nenable agents to adjust smart home settings, update calendars, fetch user information from\\n\\na database, or send emails based on a specific set of instructions.\\n\\nAs of the date of this publication, there are three primary tool types that Google models are\\n\\nable to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools,\\n\\nwe unlock a vast potential for them to not only understand the world but also act upon it,\\n\\nopening doors to a myriad of new applications and possibilities.\\n\\n12\\n\\nExtensions\\n\\nThe easiest way to understand Extensions is to think of them as bridging the gap between\\n\\nan API and an agent in a standardized way, allowing agents to seamlessly execute APIs\\n\\nregardless of their underlying implementation. Let’s say that you’ve built an agent with a goal\\n\\nof helping users book flights. You know that you want to use the Google Flights API to retrieve\\n\\nflight information, but you’re not sure how you’re going to get your agent to make calls to this\\n\\nAPI endpoint.\\n\\nFigure 3. How do Agents interact with External APIs?\\n\\nOne approach could be to implement custom code that would take the incoming user query,\\n\\nparse the query for relevant information, then make the API call. For example, in a flight\\n\\nbooking use case a user might state “I want to book a flight from Austin to Zurich.” In this\\n\\nscenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant\\n\\nentities from the user query before attempting to make the API call. But what happens if the\\n\\nuser says “I want to book a flight to Zurich” and never provides a departure city? The API call\\n\\nwould fail without the required data and more code would need to be implemented in order\\n\\nto catch edge and corner cases like this. This approach is not scalable and could easily break\\n\\nin any scenario that falls outside of the implemented custom code.\\n\\n13\\n\\nA more resilient approach would be to use an Extension. An Extension bridges the gap\\n\\nbetween an agent and an API by:\\n\\n1. Teaching the agent how to use the API endpoint using examples.\\n\\n2. Teaching the agent what arguments or parameters are needed to successfully call the\\n\\nAPI endpoint.\\n\\nFigure 4. Extensions connect Agents to External APIs\\n\\nExtensions can be crafted independently of the agent, but should be provided as part of the\\n\\nagent’s configuration. The agent uses the model and examples at run time to decide which\\n\\nExtension, if any, would be suitable for solving the user’s query. This highlights a key strength\\n\\nof Extensions, their built-in example types, that allow the agent to dynamically select the\\n\\nmost appropriate Extension for the task.\\n\\nFigure 5. 1-to-many relationship between Agents, Extensions and APIs\\n\\n14\\n\\nThink of this the same way that a software developer decides which API endpoints to use\\n\\nwhile solving and solutioning for a user’s problem. If the user wants to book a flight, the\\n\\ndeveloper might use the Google Flights API. If the user wants to know where the nearest\\n\\ncoffee shop is relative to their location, the developer might use the Google Maps API. In\\n\\nthis same way, the agent / model stack uses a set of known Extensions to decide which one\\n\\nwill be the best fit for the user’s query. If you’d like to see Extensions in action, you can try\\n\\nthem out on the Gemini application by going to Settings > Extensions and then enabling any\\n\\nyou would like to test. For example, you could enable the Google Flights extension then ask\\n\\nGemini “Show me flights from Austin to Zurich leaving next Friday.”\\n\\nSample Extensions\\n\\nTo simplify the usage of Extensions, Google provides some out of the box extensions that\\n\\ncan be quickly imported into your project and used with minimal configurations. For example,\\n\\nthe Code Interpreter extension in Snippet 1 allows you to generate and run Python code from\\n\\na natural language description.\\n\\n15\\n\\nPython\\n\\nimport vertexai import pprint\\n\\nPROJECT_ID = \"YOUR_PROJECT_ID\" REGION = \"us-central1\"\\n\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n\\nfrom vertexai.preview.extensions import Extension\\n\\nextension_code_interpreter = Extension.from_hub(\"code_interpreter\") CODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\"\\n\\nresponse = extension_code_interpreter.execute( operation_id = \"generate_and_execute\", operation_params = {\"query\": CODE_QUERY} )\\n\\nprint(\"Generated Code:\") pprint.pprint({response[\\'generated_code\\']})\\n\\n# The above snippet will generate the following code. ``` Generated Code: class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right\\n\\nContinues next page...\\n\\n16\\n\\nPython\\n\\ndef invert_binary_tree(root): \"\"\" Inverts a binary tree. Args: root: The root of the binary tree. Returns: The root of the inverted binary tree. \"\"\"\\n\\nif not root: return None\\n\\n# Swap the left and right children recursively root.left, root.right = invert_binary_tree(root.right), invert_binary_tree(root.left)\\n\\nreturn root\\n\\n# Example usage: # Construct a sample binary tree root = TreeNode(4) root.left = TreeNode(2) root.right = TreeNode(7) root.left.left = TreeNode(1) root.left.right = TreeNode(3) root.right.left = TreeNode(6) root.right.right = TreeNode(9)\\n\\n# Invert the binary tree inverted_root = invert_binary_tree(root) ```\\n\\nSnippet 1. Code Interpreter Extension can generate and run Python code\\n\\n17\\n\\nTo summarize, Extensions provide a way for agents to perceive, interact, and influence the\\n\\noutside world in a myriad of ways. The selection and invocation of these Extensions is guided\\n\\nby the use of Examples, all of which are defined as part of the Extension configuration.\\n\\nFunctions\\n\\nIn the world of software engineering, functions are defined as self-contained modules\\n\\nof code that accomplish a specific task and can be reused as needed. When a software\\n\\ndeveloper is writing a program, they will often create many functions to do various tasks.\\n\\nThey will also define the logic for when to call function_a versus function_b, as well as the\\n\\nexpected inputs and outputs.\\n\\nFunctions work very similarly in the world of agents, but we can replace the software\\n\\ndeveloper with a model. A model can take a set of known functions and decide when to use\\n\\neach Function and what arguments the Function needs based on its specification. Functions\\n\\ndiffer from Extensions in a few ways, most notably:\\n\\n1. A model outputs a Function and its arguments, but doesn’t make a live API call.\\n\\n2. Functions are executed on the client-side, while Extensions are executed on\\n\\nthe agent-side.\\n\\nUsing our Google Flights example again, a simple setup for functions might look like the\\n\\nexample in Figure 7.\\n\\n18\\n\\nFigure 7. How do functions interact with external APIs?\\n\\nNote that the main difference here is that neither the Function nor the agent interact directly\\n\\nwith the Google Flights API. So how does the API call actually happen?\\n\\nWith functions, the logic and execution of calling the actual API endpoint is offloaded away\\n\\nfrom the agent and back to the client-side application as seen in Figure 8 and Figure 9 below.\\n\\nThis offers the developer more granular control over the flow of data in the application. There\\n\\nare many reasons why a Developer might choose to use functions over Extensions, but a few\\n\\ncommon use cases are:\\n\\nAPI calls need to be made at another layer of the application stack, outside of the direct\\n\\nagent architecture flow (e.g. a middleware system, a front end framework, etc.)\\n\\nSecurity or Authentication restrictions that prevent the agent from calling an API directly\\n\\n(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)\\n\\nTiming or order-of-operations constraints that prevent the agent from making API calls in\\n\\nreal-time. (i.e. batch operations, human-in-the-loop review, etc.)\\n\\n19\\n\\nAdditional data transformation logic needs to be applied to the API Response that the\\n\\nagent cannot perform. For example, consider an API endpoint that doesn’t provide a\\n\\nfiltering mechanism for limiting the number of results returned. Using Functions on the\\n\\nclient-side provides the developer additional opportunities to make these transformations.\\n\\nThe developer wants to iterate on agent development without deploying additional\\n\\ninfrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)\\n\\nWhile the difference in internal architecture between the two approaches is subtle as seen in\\n\\nFigure 8, the additional control and decoupled dependency on external infrastructure makes\\n\\nFunction Calling an appealing option for the Developer.\\n\\nFigure 8. Delineating client vs. agent side control for extensions and function calling\\n\\n20\\n\\nUse cases\\n\\nA model can be used to invoke functions in order to handle complex, client-side execution\\n\\nflows for the end user, where the agent Developer might not want the language model to\\n\\nmanage the API execution (as is the case with Extensions). Let’s consider the following\\n\\nexample where an agent is being trained as a travel concierge to interact with users that want\\n\\nto book vacation trips. The goal is to get the agent to produce a list of cities that we can use\\n\\nin our middleware application to download images, data, etc. for the user’s trip planning. A\\n\\nuser might say something like:\\n\\nI’d like to take a ski trip with my family but I’m not sure where to go.\\n\\nIn a typical prompt to the model, the output might look like the following:\\n\\nSure, here’s a list of cities that you can consider for family ski trips:\\n\\nCrested Butte, Colorado, USA\\n\\nWhistler, BC, Canada\\n\\nZermatt, Switzerland\\n\\nWhile the above output contains the data that we need (city names), the format isn’t ideal\\n\\nfor parsing. With Function Calling, we can teach a model to format this output in a structured\\n\\nstyle (like JSON) that’s more convenient for another system to parse. Given the same input\\n\\nprompt from the user, an example JSON output from a Function might look like Snippet\\n\\n5 instead.\\n\\n21\\n\\nUnset\\n\\nfunction_call { name: \"display_cities\" args: { \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"], \"preferences\": \"skiing\" } }\\n\\nSnippet 5. Sample Function Call payload for displaying a list of cities and user preferences\\n\\nThis JSON payload is generated by the model, and then sent to our Client-side server to do\\n\\nwhatever we would like to do with it. In this specific case, we’ll call the Google Places API to\\n\\ntake the cities provided by the model and look up Images, then provide them as formatted\\n\\nrich content back to our User. Consider this sequence diagram in Figure 9 showing the above\\n\\ninteraction in step by step detail.\\n\\n22\\n\\nFigure 9. Sequence diagram showing the lifecycle of a Function Call\\n\\nThe result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with\\n\\nthe parameters required for the Client side UI to make the call to the Google Places API. The\\n\\nClient side UI manages the actual API call using the parameters provided by the model in the\\n\\nreturned Function. This is just one use case for Function Calling, but there are many other\\n\\nscenarios to consider like:\\n\\nYou want a language model to suggest a function that you can use in your code, but you\\n\\ndon\\'t want to include credentials in your code. Because function calling doesn\\'t run the\\n\\nfunction, you don\\'t need to include credentials in your code with the function information.\\n\\n23\\n\\nYou are running asynchronous operations that can take more than a few seconds. These\\n\\nscenarios work well with function calling because it\\'s an asynchronous operation.\\n\\nYou want to run functions on a device that\\'s different from the system producing the\\n\\nfunction calls and their arguments.\\n\\nOne key thing to remember about functions is that they are meant to offer the developer\\n\\nmuch more control over not only the execution of API calls, but also the entire flow of data\\n\\nin the application as a whole. In the example in Figure 9, the developer chose to not return\\n\\nAPI information back to the agent as it was not pertinent for future actions the agent might\\n\\ntake. However, based on the architecture of the application, it may make sense to return the\\n\\nexternal API call data to the agent in order to influence future reasoning, logic, and action\\n\\nchoices. Ultimately, it is up to the application developer to choose what is right for the\\n\\nspecific application.\\n\\nFunction sample code\\n\\nTo achieve the above output from our ski vacation scenario, let’s build out each of the\\n\\ncomponents to make this work with our gemini-1.5-flash-001 model.\\n\\nFirst, we’ll define our display_cities function as a simple Python method.\\n\\n24\\n\\nPython\\n\\ndef display_cities(cities: list[str], preferences: Optional[str] = None):\\n\\n\"\"\"Provides a list of cities based on the user\\'s search query and preferences.\\n\\nArgs:\\n\\npreferences (str): The user\\'s preferences for the search, like skiing, beach, restaurants, bbq, etc. cities (list[str]): The list of cities being recommended to the user.\\n\\nReturns:\\n\\nlist[str]: The list of cities being recommended to the user.\\n\\n\"\"\"\\n\\nreturn cities\\n\\nSnippet 6. Sample python method for a function that will display a list of cities.\\n\\nNext, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to\\n\\nthe model. Executing the code below would result in the output as seen at the bottom of the\\n\\ncode snippet.\\n\\n25\\n\\nPython\\n\\nfrom vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration\\n\\nmodel = GenerativeModel(\"gemini-1.5-flash-001\")\\n\\ndisplay_cities_function = FunctionDeclaration.from_func(display_cities) tool = Tool(function_declarations=[display_cities_function])\\n\\nmessage = \"I’d like to take a ski trip with my family but I’m not sure where to go.\"\\n\\nres = model.generate_content(message, tools=[tool])\\n\\nprint(f\"Function Name: {res.candidates[0].content.parts[0].function_call.name}\") print(f\"Function Args: {res.candidates[0].content.parts[0].function_call.args}\")\\n\\n> Function Name: display_cities > Function Args: {\\'preferences\\': \\'skiing\\', \\'cities\\': [\\'Aspen\\', \\'Vail\\', \\'Park City\\']}\\n\\nSnippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place\\n\\nIn summary, functions offer a straightforward framework that empowers application\\n\\ndevelopers with fine-grained control over data flow and system execution, while effectively\\n\\nleveraging the agent/model for critical input generation. Developers can selectively choose\\n\\nwhether to keep the agent “in the loop” by returning external data, or omit it based on\\n\\nspecific application architecture requirements.\\n\\n26\\n\\nData stores\\n\\nImagine a language model as a vast library of books, containing its training data. But unlike\\n\\na library that continuously acquires new volumes, this one remains static, holding only the\\n\\nknowledge it was initially trained on. This presents a challenge, as real-world knowledge is\\n\\nconstantly evolving. Data Stores address this limitation by providing access to more dynamic\\n\\nand up-to-date information, and ensuring a model’s responses remain grounded in factuality\\n\\nand relevance.\\n\\nConsider a common scenario where a developer might need to provide a small amount of\\n\\nadditional data to a model, perhaps in the form of spreadsheets or PDFs.\\n\\nFigure 10. How can Agents interact with structured and unstructured data?\\n\\n27\\n\\nData Stores allow developers to provide additional data in its original format to an agent,\\n\\neliminating the need for time-consuming data transformations, model retraining, or fine-\\n\\ntuning. The Data Store converts the incoming document into a set of vector database\\n\\nembeddings that the agent can use to extract the information it needs to supplement its next\\n\\naction or response to the user.\\n\\nFigure 11. Data Stores connect Agents to new real-time data sources of various types.\\n\\nImplementation and application\\n\\nIn the context of Generative AI agents, Data Stores are typically implemented as a vector\\n\\ndatabase that the developer wants the agent to have access to at runtime. While we won’t\\n\\ncover vector databases in depth here, the key point to understand is that they store data\\n\\nin the form of vector embeddings, a type of high-dimensional vector or mathematical\\n\\nrepresentation of the data provided. One of the most prolific examples of Data Store usage\\n\\nwith language models in recent times has been the implementation of Retrieval Augmented\\n\\n28\\n\\nGeneration (RAG) based applications. These applications seek to extend the breadth and\\n\\ndepth of a model’s knowledge beyond the foundational training data by giving the model\\n\\naccess to data in various formats like:\\n\\nWebsite content\\n\\nStructured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n\\nUnstructured Data in formats like HTML, PDF, TXT, etc.\\n\\nFigure 12. 1-to-many relationship between agents and data stores, which can represent various types of\\n\\npre-indexed data\\n\\nThe underlying process for each user request and agent response loop is generally modeled\\n\\nas seen in Figure 13.\\n\\n1. A user query is sent to an embedding model to generate embeddings for the query\\n\\n2. The query embeddings are then matched against the contents of the vector database\\n\\nusing a matching algorithm like SCaNN\\n\\n3. The matched content is retrieved from the vector database in text format and sent back to\\n\\nthe agent\\n\\n4. The agent receives both the user query and retrieved content, then formulates a response\\n\\nor action\\n\\n29\\n\\n5. A final response is sent to the user\\n\\nFigure 13. The lifecycle of a user request and agent response in a RAG based application\\n\\nThe end result is an application that allows the agent to match a user’s query to a known data\\n\\nstore through vector search, retrieve the original content, and provide it to the orchestration\\n\\nlayer and model for further processing. The next action might be to provide a final answer to\\n\\nthe user, or perform an additional vector search to further refine the results.\\n\\nA sample interaction with an agent that implements RAG with ReAct reasoning/planning can\\n\\nbe seen in Figure 14.\\n\\n30\\n\\nFigure 14. Sample RAG based application w/ ReAct reasoning/planning\\n\\n31\\n\\nTools recap\\n\\nTo summarize, extensions, functions and data stores make up a few different tool types\\n\\navailable for agents to use at runtime. Each has their own purpose and they can be used\\n\\ntogether or independently at the discretion of the agent developer.\\n\\nExtensions\\n\\nFunction Calling\\n\\nData Stores\\n\\nExecution\\n\\nAgent-Side Execution\\n\\nClient-Side Execution\\n\\nAgent-Side Execution\\n\\nUse Case\\n\\nDeveloper wants agent to control interactions with the API endpoints\\n\\nSecurity or\\n\\nAuthentication restrictions prevent the agent from calling an API directly\\n\\nDeveloper wants to implement Retrieval Augmented Generation (RAG) with any of the following data types:\\n\\nUseful when\\n\\nleveraging native pre- built Extensions (i.e. Vertex Search, Code Interpreter, etc.)\\n\\nMulti-hop planning and API calling (i.e. the next agent action depends on the outputs of the previous action / API call)\\n\\nTiming constraints or order-of-operations constraints that prevent the agent from making API calls in real-time. (i.e. batch operations, human-in- the-loop review, etc.)\\n\\nAPI that is not exposed\\n\\nto the internet, or non-accessible by Google systems\\n\\nWebsite Content from pre-indexed domains and URLs\\n\\nStructured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n\\nRelational / Non-\\n\\nRelational Databases\\n\\nUnstructured Data in\\n\\nformats like HTML, PDF, TXT, etc.\\n\\n32\\n\\nEnhancing model performance with targeted learning\\n\\nA crucial aspect of using models effectively is their ability to choose the right tools when\\n\\ngenerating output, especially when using tools at scale in production. While general training\\n\\nhelps models develop this skill, real-world scenarios often require knowledge beyond the\\n\\ntraining data. Imagine this as the difference between basic cooking skills and mastering\\n\\na specific cuisine. Both require foundational cooking knowledge, but the latter demands\\n\\ntargeted learning for more nuanced results.\\n\\nTo help the model gain access to this type of specific knowledge, several approaches exist:\\n\\n\\n\\nIn-context learning: This method provides a generalized model with a prompt, tools, and\\n\\nfew-shot examples at inference time which allows it to learn ‘on the fly\\' how and when to\\n\\nuse those tools for a specific task. The ReAct framework is an example of this approach in\\n\\nnatural language.\\n\\nRetrieval-based in-context learning: This technique dynamically populates the model\\n\\nprompt with the most relevant information, tools, and associated examples by retrieving\\n\\nthem from external memory. An example of this would be the ‘Example Store’ in Vertex AI\\n\\nextensions or the data stores RAG based architecture mentioned previously.\\n\\nFine-tuning based learning: This method involves training a model using a larger dataset\\n\\nof specific examples prior to inference. This helps the model understand when and how to\\n\\napply certain tools prior to receiving any user queries.\\n\\nTo provide additional insights on each of the targeted learning approaches, let’s revisit our\\n\\ncooking analogy.\\n\\n33\\n\\n\\n\\nImagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant\\n\\ntools) and some example dishes (few-shot examples) from a customer. Based on this\\n\\nlimited information and the chef’s general knowledge of cooking, they will need to figure\\n\\nout how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the\\n\\ncustomer’s preferences. This is in-context learning.\\n\\nNow let’s imagine our chef in a kitchen that has a well-stocked pantry (external data\\n\\nstores) filled with various ingredients and cookbooks (examples and tools). The chef is now\\n\\nable to dynamically choose ingredients and cookbooks from the pantry and better align\\n\\nto the customer’s recipe and preferences. This allows the chef to create a more informed\\n\\nand refined dish leveraging both existing and new knowledge. This is retrieval-based\\n\\nin-context learning.\\n\\nFinally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of\\n\\ncuisines (pre-training on a larger dataset of specific examples). This allows the chef to\\n\\napproach future unseen customer recipes with deeper understanding. This approach is\\n\\nperfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-\\n\\ntuning based learning.\\n\\nEach of these approaches offers unique advantages and disadvantages in terms of speed,\\n\\ncost, and latency. However, by combining these techniques in an agent framework, we can\\n\\nleverage the various strengths and minimize their weaknesses, allowing for a more robust and\\n\\nadaptable solution.\\n\\n34\\n\\nAgent quick start with LangChain\\n\\nIn order to provide a real-world executable example of an agent in action, we’ll build a quick\\n\\nprototype with the LangChain and LangGraph libraries. These popular open source libraries\\n\\nallow users to build customer agents by “chaining” together sequences of logic, reasoning, and tool calls to answer a user’s query. We’ll use our gemini-1.5-flash-001 model and\\n\\nsome simple tools to answer a multi-stage query from the user as seen in Snippet 8.\\n\\nThe tools we are using are the SerpAPI (for Google Search) and the Google Places API. After\\n\\nexecuting our program in Snippet 8, you can see the sample output in Snippet 9.\\n\\n35\\n\\nPython\\n\\nfrom langgraph.prebuilt import create_react_agent from langchain_core.tools import tool from langchain_community.utilities import SerpAPIWrapper from langchain_community.tools import GooglePlacesTool\\n\\nos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\" os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\\n\\n@tool def search(query: str):\\n\\n\"\"\"Use the SerpAPI to run a Google Search.\"\"\" search = SerpAPIWrapper() return search.run(query)\\n\\n@tool def places(query: str):\\n\\n\"\"\"Use the Google Places API to run a Google Places Query.\"\"\" places = GooglePlacesTool() return places.run(query)\\n\\nmodel = ChatVertexAI(model=\"gemini-1.5-flash-001\") tools = [search, places]\\n\\nquery = \"Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium?\"\\n\\nagent = create_react_agent(model, tools) input = {\"messages\": [(\"human\", query)]}\\n\\nfor s in agent.stream(input, stream_mode=\"values\"):\\n\\nmessage = s[\"messages\"][-1] if isinstance(message, tuple):\\n\\nprint(message)\\n\\nelse:\\n\\nmessage.pretty_print()\\n\\nSnippet 8. Sample LangChain and LangGraph based agent with tools\\n\\n36\\n\\nUnset\\n\\n=============================== Human Message ================================ Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium? ================================= Ai Message ================================= Tool Calls: search Args:\\n\\nquery: Texas Longhorns football schedule\\n\\n================================ Tool Message ================================ Name: search {...Results: \"NCAA Division I Football, Georgia, Date...\"} ================================= Ai Message ================================= The Texas Longhorns played the Georgia Bulldogs last week. Tool Calls: places Args:\\n\\nquery: Georgia Bulldogs stadium\\n\\n================================ Tool Message ================================ Name: places\\n\\n{...Sanford Stadium Address: 100 Sanford...} ================================= Ai Message ================================= The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 30602, USA.\\n\\nSnippet 9. Output from our program in Snippet 8\\n\\nWhile this is a fairly simple agent example, it demonstrates the foundational components\\n\\nof Model, Orchestration, and tools all working together to achieve a specific goal. In the\\n\\nfinal section, we’ll explore how these components come together in Google-scale managed\\n\\nproducts like Vertex AI agents and Generative Playbooks.\\n\\n37\\n\\nProduction applications with Vertex AI agents\\n\\nWhile this whitepaper explored the core components of agents, building production-grade\\n\\napplications requires integrating them with additional tools like user interfaces, evaluation\\n\\nframeworks, and continuous improvement mechanisms. Google’s Vertex AI platform\\n\\nsimplifies this process by offering a fully managed environment with all the fundamental\\n\\nelements covered earlier. Using a natural language interface, developers can rapidly\\n\\ndefine crucial elements of their agents - goals, task instructions, tools, sub-agents for task\\n\\ndelegation, and examples - to easily construct the desired system behavior. In addition, the\\n\\nplatform comes with a set of development tools that allow for testing, evaluation, measuring\\n\\nagent performance, debugging, and improving the overall quality of developed agents. This\\n\\nallows developers to focus on building and refining their agents while the complexities of\\n\\ninfrastructure, deployment and maintenance are managed by the platform itself.\\n\\nIn Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex\\n\\nAI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex\\n\\nFunction Calling and Vertex Example Store to name a few. The architecture includes many of\\n\\nthe various components necessary for a production ready application.\\n\\n38\\n\\nFigure 15. Sample end-to-end agent architecture built on Vertex AI platform\\n\\nYou can try a sample of this prebuilt agent architecture from our official documentation.\\n\\n39\\n\\nSummary\\n\\nIn this whitepaper we’ve discussed the foundational building blocks of Generative AI\\n\\nagents, their compositions, and effective ways to implement them in the form of cognitive\\n\\narchitectures. Some key takeaways from this whitepaper include:\\n\\n1. Agents extend the capabilities of language models by leveraging tools to access real-\\n\\ntime information, suggest real-world actions, and plan and execute complex tasks\\n\\nautonomously. agents can leverage one or more language models to decide when and\\n\\nhow to transition through states and use external tools to complete any number of\\n\\ncomplex tasks that would be difficult or impossible for the model to complete on its own.\\n\\n2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that\\n\\nstructures reasoning, planning, decision-making and guides its actions. Various reasoning\\n\\ntechniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework\\n\\nfor the orchestration layer to take in information, perform internal reasoning, and generate\\n\\ninformed decisions or responses.\\n\\n3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside\\n\\nworld for agents, allowing them to interact with external systems and access knowledge\\n\\nbeyond their training data. Extensions provide a bridge between agents and external APIs,\\n\\nenabling the execution of API calls and retrieval of real-time information. functions provide\\n\\na more nuanced control for the developer through the division of labor, allowing agents\\n\\nto generate Function parameters which can be executed client-side. Data Stores provide\\n\\nagents with access to structured or unstructured data, enabling data-driven applications.\\n\\nThe future of agents holds exciting advancements and we’ve only begun to scratch the\\n\\nsurface of what is possible. As tools become more sophisticated and reasoning capabilities\\n\\nare enhanced, agents will be empowered to solve increasingly complex problems.\\n\\nFurthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By\\n\\n40\\n\\ncombining specialized agents - each excelling in a particular domain or task - we can create\\n\\na ‘mixture of agent experts’ approach, capable of delivering exceptional results across\\n\\nvarious industries and problem areas.\\n\\nIt’s important to remember that building complex agent architectures demands an iterative\\n\\napproach. Experimentation and refinement are key to finding solutions for specific business\\n\\ncases and organizational needs. No two agents are created alike due to the generative nature\\n\\nof the foundational models that underpin their architecture. However, by harnessing the\\n\\nstrengths of each of these foundational components, we can create impactful applications\\n\\nthat extend the capabilities of language models and drive real-world value.\\n\\n41\\n\\nEndnotes\\n\\n1. Shafran, I., Cao, Y. et al., 2022, \\'ReAct: Synergizing Reasoning and Acting in Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2210.03629\\n\\n2. Wei, J., Wang, X. et al., 2023, \\'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\\'.\\n\\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf.\\n\\n3. Wang, X. et al., 2022, \\'Self-Consistency Improves Chain of Thought Reasoning in Language Models\\'.\\n\\nAvailable at: https://arxiv.org/abs/2203.11171.\\n\\n4. Diao, S. et al., 2023, \\'Active Prompting with Chain-of-Thought for Large Language Models\\'. Available at:\\n\\nhttps://arxiv.org/pdf/2302.12246.pdf.\\n\\n5. Zhang, H. et al., 2023, \\'Multimodal Chain-of-Thought Reasoning in Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2302.00923.\\n\\n6. Yao, S. et al., 2023, \\'Tree of Thoughts: Deliberate Problem Solving with Large Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2305.10601.\\n\\n7. Long, X., 2023, \\'Large Language Model Guided Tree-of-Thought\\'. Available at:\\n\\nhttps://arxiv.org/abs/2305.08291.\\n\\n8. Google. \\'Google Gemini Application\\'. Available at: http://gemini.google.com.\\n\\n9. Swagger. \\'OpenAPI Specification\\'. Available at: https://swagger.io/specification/.\\n\\n10. Xie, M., 2022, \\'How does in-context learning work? A framework for understanding the differences from\\n\\ntraditional supervised learning\\'. Available at: https://ai.stanford.edu/blog/understanding-incontext/.\\n\\n11. Google Research. \\'ScaNN (Scalable Nearest Neighbors)\\'. Available at:\\n\\nhttps://github.com/google-research/google-research/tree/master/scann.\\n\\n12. LangChain. \\'LangChain\\'. Available at: https://python.langchain.com/v0.2/docs/introduction/.\\n\\n42')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02216b24",
      "metadata": {},
      "source": [
        "Each document object includes a metadata dictionary that can be used to store additional information about the document, which can be accessed through `metadata`.\n",
        "\n",
        "Please check if the metadata dictionary contains a key called `filename`.\n",
        "\n",
        "This key will be used in the `Test datasets generation process`. The `filename` attribute in metadata is used to identify chunks belonging to the same document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6981dea9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set metadata ('filename' must exist)\n",
        "for doc in docs:\n",
        "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0b216a9f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'data/Newwhitepaper_Agents2.pdf', 'filename': 'data/Newwhitepaper_Agents2.pdf'}, page_content='Agents\\n\\nAuthors: Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic\\n\\nAcknowledgements\\n\\nReviewers and Contributors\\n\\nEvan Huang\\n\\nEmily Xue\\n\\nOlcan Sercinoglu\\n\\nSebastian Riedel\\n\\nSatinder Baveja\\n\\nAntonio Gulli\\n\\nAnant Nawalgaria\\n\\nCurators and Editors\\n\\nAntonio Gulli\\n\\nAnant Nawalgaria\\n\\nGrace Mollison\\n\\nTechnical Writer\\n\\nJoey Haymaker\\n\\nDesigner\\n\\nMichael Lanning\\n\\n2\\n\\nTable of contents\\n\\nIntroduction\\n\\nWhat is an agent?\\n\\nThe model\\n\\nThe tools\\n\\nThe orchestration layer\\n\\nAgents vs. models\\n\\nCognitive architectures: How agents operate\\n\\nTools: Our keys to the outside world\\n\\nExtensions\\n\\nSample Extensions\\n\\nFunctions\\n\\nUse cases\\n\\nFunction sample code\\n\\nData stores\\n\\nImplementation and application\\n\\nTools recap\\n\\nEnhancing model performance with targeted learning\\n\\nAgent quick start with LangChain\\n\\nProduction applications with Vertex AI agents\\n\\nSummary\\n\\nEndnotes\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n7\\n\\n8\\n\\n8\\n\\n12\\n\\n13\\n\\n15\\n\\n18\\n\\n21\\n\\n24\\n\\n27\\n\\n28\\n\\n32\\n\\n33\\n\\n35\\n\\n38\\n\\n40\\n\\n42\\n\\nThis combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent.\\n\\nIntroduction\\n\\nHumans are fantastic at messy pattern recognition tasks. However, they often rely on tools\\n\\nlike books, Google Search, or a calculator - to supplement their prior knowledge before\\n\\narriving at a conclusion. Just like humans, Generative AI models can be trained to use tools\\n\\nto access real-time information or suggest a real-world action. For example, a model can\\n\\nleverage a database retrieval tool to access specific information, like a customer\\'s purchase\\n\\nhistory, so it can generate tailored shopping recommendations. Alternatively, based on a\\n\\nuser\\'s query, a model can make various API calls to send an email response to a colleague\\n\\nor complete a financial transaction on your behalf. To do so, the model must not only have\\n\\naccess to a set of external tools, it needs the ability to plan and execute any task in a self-\\n\\ndirected fashion. This combination of reasoning, logic, and access to external information\\n\\nthat are all connected to a Generative AI model invokes the concept of an agent, or a\\n\\nprogram that extends beyond the standalone capabilities of a Generative AI model. This\\n\\nwhitepaper dives into all these and associated aspects in more detail.\\n\\n4\\n\\nWhat is an agent?\\n\\nIn its most fundamental form, a Generative AI agent can be defined as an application that\\n\\nattempts to achieve a goal by observing the world and acting upon it using the tools that it\\n\\nhas at its disposal. Agents are autonomous and can act independently of human intervention,\\n\\nespecially when provided with proper goals or objectives they are meant to achieve. Agents\\n\\ncan also be proactive in their approach to reaching their goals. Even in the absence of\\n\\nexplicit instruction sets from a human, an agent can reason about what it should do next to\\n\\nachieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this\\n\\nwhitepaper focuses on the specific types of agents that Generative AI models are capable of\\n\\nbuilding at the time of publication.\\n\\nIn order to understand the inner workings of an agent, let’s first introduce the foundational\\n\\ncomponents that drive the agent’s behavior, actions, and decision making. The combination\\n\\nof these components can be described as a cognitive architecture, and there are many\\n\\nsuch architectures that can be achieved by the mixing and matching of these components.\\n\\nFocusing on the core functionalities, there are three essential components in an agent’s\\n\\ncognitive architecture as shown in Figure 1.\\n\\n5\\n\\nFigure 1. General agent architecture and components\\n\\nThe model\\n\\nIn the scope of an agent, a model refers to the language model (LM) that will be utilized as\\n\\nthe centralized decision maker for agent processes. The model used by an agent can be one\\n\\nor multiple LM’s of any size (small / large) that are capable of following instruction based\\n\\nreasoning and logic frameworks, like ReAct, Chain-of-Thought, or Tree-of-Thoughts. Models\\n\\ncan be general purpose, multimodal or fine-tuned based on the needs of your specific agent\\n\\narchitecture. For best production results, you should leverage a model that best fits your\\n\\ndesired end application and, ideally, has been trained on data signatures associated with the\\n\\ntools that you plan to use in the cognitive architecture. It’s important to note that the model is\\n\\ntypically not trained with the specific configuration settings (i.e. tool choices, orchestration/\\n\\nreasoning setup) of the agent. However, it’s possible to further refine the model for the\\n\\nagent’s tasks by providing it with examples that showcase the agent’s capabilities, including\\n\\ninstances of the agent using specific tools or reasoning steps in various contexts.\\n\\n6\\n\\nThe tools\\n\\nFoundational models, despite their impressive text and image generation, remain constrained\\n\\nby their inability to interact with the outside world. Tools bridge this gap, empowering agents\\n\\nto interact with external data and services while unlocking a wider range of actions beyond\\n\\nthat of the underlying model alone. Tools can take a variety of forms and have varying\\n\\ndepths of complexity, but typically align with common web API methods like GET, POST,\\n\\nPATCH, and DELETE. For example, a tool could update customer information in a database\\n\\nor fetch weather data to influence a travel recommendation that the agent is providing to\\n\\nthe user. With tools, agents can access and process real-world information. This empowers\\n\\nthem to support more specialized systems like retrieval augmented generation (RAG),\\n\\nwhich significantly extends an agent’s capabilities beyond what the foundational model can\\n\\nachieve on its own. We’ll discuss tools in more detail below, but the most important thing\\n\\nto understand is that tools bridge the gap between the agent’s internal capabilities and the\\n\\nexternal world, unlocking a broader range of possibilities.\\n\\nThe orchestration layer\\n\\nThe orchestration layer describes a cyclical process that governs how the agent takes in\\n\\ninformation, performs some internal reasoning, and uses that reasoning to inform its next\\n\\naction or decision. In general, this loop will continue until an agent has reached its goal or a\\n\\nstopping point. The complexity of the orchestration layer can vary greatly depending on the\\n\\nagent and task it’s performing. Some loops can be simple calculations with decision rules,\\n\\nwhile others may contain chained logic, involve additional machine learning algorithms, or\\n\\nimplement other probabilistic reasoning techniques. We’ll discuss more about the detailed\\n\\nimplementation of the agent orchestration layers in the cognitive architecture section.\\n\\n7\\n\\nAgents vs. models\\n\\nTo gain a clearer understanding of the distinction between agents and models, consider the\\n\\nfollowing chart:\\n\\nModels\\n\\nAgents\\n\\nKnowledge is limited to what is available in their training data.\\n\\nKnowledge is extended through the connection with external systems via tools\\n\\nSingle inference / prediction based on the user query. Unless explicitly implemented for the model, there is no management of session history or continuous context. (i.e. chat history)\\n\\nManaged session history (i.e. chat history) to allow for multi turn inference / prediction based on user queries and decisions made in the orchestration layer. In this context, a ‘turn’ is defined as an interaction between the interacting system and the agent. (i.e. 1 incoming event/ query and 1 agent response)\\n\\nNo native tool implementation.\\n\\nTools are natively implemented in agent architecture.\\n\\nNo native logic layer implemented. Users can form prompts as simple questions or use reasoning frameworks (CoT, ReAct, etc.) to form complex prompts to guide the model in prediction.\\n\\nNative cognitive architecture that uses reasoning frameworks like CoT, ReAct, or other pre-built agent frameworks like LangChain.\\n\\nCognitive architectures: How agents operate\\n\\nImagine a chef in a busy kitchen. Their goal is to create delicious dishes for restaurant\\n\\npatrons which involves some cycle of planning, execution, and adjustment.\\n\\n8\\n\\nThey gather information, like the patron’s order and what ingredients are in the pantry\\n\\nand refrigerator.\\n\\nThey perform some internal reasoning about what dishes and flavor profiles they can\\n\\ncreate based on the information they have just gathered.\\n\\nThey take action to create the dish: chopping vegetables, blending spices, searing meat.\\n\\nAt each stage in the process the chef makes adjustments as needed, refining their plan as\\n\\ningredients are depleted or customer feedback is received, and uses the set of previous\\n\\noutcomes to determine the next plan of action. This cycle of information intake, planning,\\n\\nexecuting, and adjusting describes a unique cognitive architecture that the chef employs to\\n\\nreach their goal.\\n\\nJust like the chef, agents can use cognitive architectures to reach their end goals by\\n\\niteratively processing information, making informed decisions, and refining next actions\\n\\nbased on previous outputs. At the core of agent cognitive architectures lies the orchestration\\n\\nlayer, responsible for maintaining memory, state, reasoning and planning. It uses the rapidly\\n\\nevolving field of prompt engineering and associated frameworks to guide reasoning and\\n\\nplanning, enabling the agent to interact more effectively with its environment and complete\\n\\ntasks. Research in the area of prompt engineering frameworks and task planning for\\n\\nlanguage models is rapidly evolving, yielding a variety of promising approaches. While not an\\n\\nexhaustive list, these are a few of the most popular frameworks and reasoning techniques\\n\\navailable at the time of this publication:\\n\\nReAct, a prompt engineering framework that provides a thought process strategy for\\n\\nlanguage models to Reason and take action on a user query, with or without in-context\\n\\nexamples. ReAct prompting has shown to outperform several SOTA baselines and improve\\n\\nhuman interoperability and trustworthiness of LLMs.\\n\\n9\\n\\nChain-of-Thought (CoT), a prompt engineering framework that enables reasoning\\n\\ncapabilities through intermediate steps. There are various sub-techniques of CoT including\\n\\nself-consistency, active-prompt, and multimodal CoT that each have strengths and\\n\\nweaknesses depending on the specific application.\\n\\nTree-of-thoughts (ToT),, a prompt engineering framework that is well suited for\\n\\nexploration or strategic lookahead tasks. It generalizes over chain-of-thought prompting\\n\\nand allows the model to explore various thought chains that serve as intermediate steps\\n\\nfor general problem solving with language models.\\n\\nAgents can utilize one of the above reasoning techniques, or many other techniques, to\\n\\nchoose the next best action for the given user request. For example, let’s consider an agent\\n\\nthat is programmed to use the ReAct framework to choose the correct actions and tools for\\n\\nthe user query. The sequence of events might go something like this:\\n\\n1. User sends query to the agent\\n\\n2. Agent begins the ReAct sequence\\n\\n3. The agent provides a prompt to the model, asking it to generate one of the next ReAct\\n\\nsteps and its corresponding output:\\n\\na. Question: The input question from the user query, provided with the prompt\\n\\nb. Thought: The model’s thoughts about what it should do next\\n\\nc. Action: The model’s decision on what action to take next\\n\\ni. This is where tool choice can occur\\n\\nii. For example, an action could be one of [Flights, Search, Code, None], where the first\\n\\n3 represent a known tool that the model can choose, and the last represents “no\\n\\ntool choice”\\n\\n10\\n\\nd. Action input: The model’s decision on what inputs to provide to the tool (if any)\\n\\ne. Observation: The result of the action / action input sequence\\n\\ni. This thought / action / action input / observation could repeat N-times as needed\\n\\nf. Final answer: The model’s final answer to provide to the original user query\\n\\n4. The ReAct loop concludes and a final answer is provided back to the user\\n\\nFigure 2. Example agent with ReAct reasoning in the orchestration layer\\n\\nAs shown in Figure 2, the model, tools, and agent configuration work together to provide\\n\\na grounded, concise response back to the user based on the user’s original query. While\\n\\nthe model could have guessed at an answer (hallucinated) based on its prior knowledge,\\n\\nit instead used a tool (Flights) to search for real-time external information. This additional\\n\\ninformation was provided to the model, allowing it to make a more informed decision based\\n\\non real factual data and to summarize this information back to the user.\\n\\n11\\n\\nIn summary, the quality of agent responses can be tied directly to the model’s ability to\\n\\nreason and act about these various tasks, including the ability to select the right tools, and\\n\\nhow well that tools has been defined. Like a chef crafting a dish with fresh ingredients and\\n\\nattentive to customer feedback, agents rely on sound reasoning and reliable information to\\n\\ndeliver optimal results. In the next section, we’ll dive into the various ways agents connect\\n\\nwith fresh data.\\n\\nTools: Our keys to the outside world\\n\\nWhile language models excel at processing information, they lack the ability to directly\\n\\nperceive and influence the real world. This limits their usefulness in situations requiring\\n\\ninteraction with external systems or data. This means that, in a sense, a language model\\n\\nis only as good as what it has learned from its training data. But regardless of how much\\n\\ndata we throw at a model, they still lack the fundamental ability to interact with the outside\\n\\nworld. So how can we empower our models to have real-time, context-aware interaction with\\n\\nexternal systems? Functions, Extensions, Data Stores and Plugins are all ways to provide this\\n\\ncritical capability to the model.\\n\\nWhile they go by many names, tools are what create a link between our foundational models\\n\\nand the outside world. This link to external systems and data allows our agent to perform a\\n\\nwider variety of tasks and do so with more accuracy and reliability. For instance, tools can\\n\\nenable agents to adjust smart home settings, update calendars, fetch user information from\\n\\na database, or send emails based on a specific set of instructions.\\n\\nAs of the date of this publication, there are three primary tool types that Google models are\\n\\nable to interact with: Extensions, Functions, and Data Stores. By equipping agents with tools,\\n\\nwe unlock a vast potential for them to not only understand the world but also act upon it,\\n\\nopening doors to a myriad of new applications and possibilities.\\n\\n12\\n\\nExtensions\\n\\nThe easiest way to understand Extensions is to think of them as bridging the gap between\\n\\nan API and an agent in a standardized way, allowing agents to seamlessly execute APIs\\n\\nregardless of their underlying implementation. Let’s say that you’ve built an agent with a goal\\n\\nof helping users book flights. You know that you want to use the Google Flights API to retrieve\\n\\nflight information, but you’re not sure how you’re going to get your agent to make calls to this\\n\\nAPI endpoint.\\n\\nFigure 3. How do Agents interact with External APIs?\\n\\nOne approach could be to implement custom code that would take the incoming user query,\\n\\nparse the query for relevant information, then make the API call. For example, in a flight\\n\\nbooking use case a user might state “I want to book a flight from Austin to Zurich.” In this\\n\\nscenario, our custom code solution would need to extract “Austin” and “Zurich” as relevant\\n\\nentities from the user query before attempting to make the API call. But what happens if the\\n\\nuser says “I want to book a flight to Zurich” and never provides a departure city? The API call\\n\\nwould fail without the required data and more code would need to be implemented in order\\n\\nto catch edge and corner cases like this. This approach is not scalable and could easily break\\n\\nin any scenario that falls outside of the implemented custom code.\\n\\n13\\n\\nA more resilient approach would be to use an Extension. An Extension bridges the gap\\n\\nbetween an agent and an API by:\\n\\n1. Teaching the agent how to use the API endpoint using examples.\\n\\n2. Teaching the agent what arguments or parameters are needed to successfully call the\\n\\nAPI endpoint.\\n\\nFigure 4. Extensions connect Agents to External APIs\\n\\nExtensions can be crafted independently of the agent, but should be provided as part of the\\n\\nagent’s configuration. The agent uses the model and examples at run time to decide which\\n\\nExtension, if any, would be suitable for solving the user’s query. This highlights a key strength\\n\\nof Extensions, their built-in example types, that allow the agent to dynamically select the\\n\\nmost appropriate Extension for the task.\\n\\nFigure 5. 1-to-many relationship between Agents, Extensions and APIs\\n\\n14\\n\\nThink of this the same way that a software developer decides which API endpoints to use\\n\\nwhile solving and solutioning for a user’s problem. If the user wants to book a flight, the\\n\\ndeveloper might use the Google Flights API. If the user wants to know where the nearest\\n\\ncoffee shop is relative to their location, the developer might use the Google Maps API. In\\n\\nthis same way, the agent / model stack uses a set of known Extensions to decide which one\\n\\nwill be the best fit for the user’s query. If you’d like to see Extensions in action, you can try\\n\\nthem out on the Gemini application by going to Settings > Extensions and then enabling any\\n\\nyou would like to test. For example, you could enable the Google Flights extension then ask\\n\\nGemini “Show me flights from Austin to Zurich leaving next Friday.”\\n\\nSample Extensions\\n\\nTo simplify the usage of Extensions, Google provides some out of the box extensions that\\n\\ncan be quickly imported into your project and used with minimal configurations. For example,\\n\\nthe Code Interpreter extension in Snippet 1 allows you to generate and run Python code from\\n\\na natural language description.\\n\\n15\\n\\nPython\\n\\nimport vertexai import pprint\\n\\nPROJECT_ID = \"YOUR_PROJECT_ID\" REGION = \"us-central1\"\\n\\nvertexai.init(project=PROJECT_ID, location=REGION)\\n\\nfrom vertexai.preview.extensions import Extension\\n\\nextension_code_interpreter = Extension.from_hub(\"code_interpreter\") CODE_QUERY = \"\"\"Write a python method to invert a binary tree in O(n) time.\"\"\"\\n\\nresponse = extension_code_interpreter.execute( operation_id = \"generate_and_execute\", operation_params = {\"query\": CODE_QUERY} )\\n\\nprint(\"Generated Code:\") pprint.pprint({response[\\'generated_code\\']})\\n\\n# The above snippet will generate the following code. ``` Generated Code: class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right\\n\\nContinues next page...\\n\\n16\\n\\nPython\\n\\ndef invert_binary_tree(root): \"\"\" Inverts a binary tree. Args: root: The root of the binary tree. Returns: The root of the inverted binary tree. \"\"\"\\n\\nif not root: return None\\n\\n# Swap the left and right children recursively root.left, root.right = invert_binary_tree(root.right), invert_binary_tree(root.left)\\n\\nreturn root\\n\\n# Example usage: # Construct a sample binary tree root = TreeNode(4) root.left = TreeNode(2) root.right = TreeNode(7) root.left.left = TreeNode(1) root.left.right = TreeNode(3) root.right.left = TreeNode(6) root.right.right = TreeNode(9)\\n\\n# Invert the binary tree inverted_root = invert_binary_tree(root) ```\\n\\nSnippet 1. Code Interpreter Extension can generate and run Python code\\n\\n17\\n\\nTo summarize, Extensions provide a way for agents to perceive, interact, and influence the\\n\\noutside world in a myriad of ways. The selection and invocation of these Extensions is guided\\n\\nby the use of Examples, all of which are defined as part of the Extension configuration.\\n\\nFunctions\\n\\nIn the world of software engineering, functions are defined as self-contained modules\\n\\nof code that accomplish a specific task and can be reused as needed. When a software\\n\\ndeveloper is writing a program, they will often create many functions to do various tasks.\\n\\nThey will also define the logic for when to call function_a versus function_b, as well as the\\n\\nexpected inputs and outputs.\\n\\nFunctions work very similarly in the world of agents, but we can replace the software\\n\\ndeveloper with a model. A model can take a set of known functions and decide when to use\\n\\neach Function and what arguments the Function needs based on its specification. Functions\\n\\ndiffer from Extensions in a few ways, most notably:\\n\\n1. A model outputs a Function and its arguments, but doesn’t make a live API call.\\n\\n2. Functions are executed on the client-side, while Extensions are executed on\\n\\nthe agent-side.\\n\\nUsing our Google Flights example again, a simple setup for functions might look like the\\n\\nexample in Figure 7.\\n\\n18\\n\\nFigure 7. How do functions interact with external APIs?\\n\\nNote that the main difference here is that neither the Function nor the agent interact directly\\n\\nwith the Google Flights API. So how does the API call actually happen?\\n\\nWith functions, the logic and execution of calling the actual API endpoint is offloaded away\\n\\nfrom the agent and back to the client-side application as seen in Figure 8 and Figure 9 below.\\n\\nThis offers the developer more granular control over the flow of data in the application. There\\n\\nare many reasons why a Developer might choose to use functions over Extensions, but a few\\n\\ncommon use cases are:\\n\\nAPI calls need to be made at another layer of the application stack, outside of the direct\\n\\nagent architecture flow (e.g. a middleware system, a front end framework, etc.)\\n\\nSecurity or Authentication restrictions that prevent the agent from calling an API directly\\n\\n(e.g API is not exposed to the internet, or non-accessible by agent infrastructure)\\n\\nTiming or order-of-operations constraints that prevent the agent from making API calls in\\n\\nreal-time. (i.e. batch operations, human-in-the-loop review, etc.)\\n\\n19\\n\\nAdditional data transformation logic needs to be applied to the API Response that the\\n\\nagent cannot perform. For example, consider an API endpoint that doesn’t provide a\\n\\nfiltering mechanism for limiting the number of results returned. Using Functions on the\\n\\nclient-side provides the developer additional opportunities to make these transformations.\\n\\nThe developer wants to iterate on agent development without deploying additional\\n\\ninfrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)\\n\\nWhile the difference in internal architecture between the two approaches is subtle as seen in\\n\\nFigure 8, the additional control and decoupled dependency on external infrastructure makes\\n\\nFunction Calling an appealing option for the Developer.\\n\\nFigure 8. Delineating client vs. agent side control for extensions and function calling\\n\\n20\\n\\nUse cases\\n\\nA model can be used to invoke functions in order to handle complex, client-side execution\\n\\nflows for the end user, where the agent Developer might not want the language model to\\n\\nmanage the API execution (as is the case with Extensions). Let’s consider the following\\n\\nexample where an agent is being trained as a travel concierge to interact with users that want\\n\\nto book vacation trips. The goal is to get the agent to produce a list of cities that we can use\\n\\nin our middleware application to download images, data, etc. for the user’s trip planning. A\\n\\nuser might say something like:\\n\\nI’d like to take a ski trip with my family but I’m not sure where to go.\\n\\nIn a typical prompt to the model, the output might look like the following:\\n\\nSure, here’s a list of cities that you can consider for family ski trips:\\n\\nCrested Butte, Colorado, USA\\n\\nWhistler, BC, Canada\\n\\nZermatt, Switzerland\\n\\nWhile the above output contains the data that we need (city names), the format isn’t ideal\\n\\nfor parsing. With Function Calling, we can teach a model to format this output in a structured\\n\\nstyle (like JSON) that’s more convenient for another system to parse. Given the same input\\n\\nprompt from the user, an example JSON output from a Function might look like Snippet\\n\\n5 instead.\\n\\n21\\n\\nUnset\\n\\nfunction_call { name: \"display_cities\" args: { \"cities\": [\"Crested Butte\", \"Whistler\", \"Zermatt\"], \"preferences\": \"skiing\" } }\\n\\nSnippet 5. Sample Function Call payload for displaying a list of cities and user preferences\\n\\nThis JSON payload is generated by the model, and then sent to our Client-side server to do\\n\\nwhatever we would like to do with it. In this specific case, we’ll call the Google Places API to\\n\\ntake the cities provided by the model and look up Images, then provide them as formatted\\n\\nrich content back to our User. Consider this sequence diagram in Figure 9 showing the above\\n\\ninteraction in step by step detail.\\n\\n22\\n\\nFigure 9. Sequence diagram showing the lifecycle of a Function Call\\n\\nThe result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with\\n\\nthe parameters required for the Client side UI to make the call to the Google Places API. The\\n\\nClient side UI manages the actual API call using the parameters provided by the model in the\\n\\nreturned Function. This is just one use case for Function Calling, but there are many other\\n\\nscenarios to consider like:\\n\\nYou want a language model to suggest a function that you can use in your code, but you\\n\\ndon\\'t want to include credentials in your code. Because function calling doesn\\'t run the\\n\\nfunction, you don\\'t need to include credentials in your code with the function information.\\n\\n23\\n\\nYou are running asynchronous operations that can take more than a few seconds. These\\n\\nscenarios work well with function calling because it\\'s an asynchronous operation.\\n\\nYou want to run functions on a device that\\'s different from the system producing the\\n\\nfunction calls and their arguments.\\n\\nOne key thing to remember about functions is that they are meant to offer the developer\\n\\nmuch more control over not only the execution of API calls, but also the entire flow of data\\n\\nin the application as a whole. In the example in Figure 9, the developer chose to not return\\n\\nAPI information back to the agent as it was not pertinent for future actions the agent might\\n\\ntake. However, based on the architecture of the application, it may make sense to return the\\n\\nexternal API call data to the agent in order to influence future reasoning, logic, and action\\n\\nchoices. Ultimately, it is up to the application developer to choose what is right for the\\n\\nspecific application.\\n\\nFunction sample code\\n\\nTo achieve the above output from our ski vacation scenario, let’s build out each of the\\n\\ncomponents to make this work with our gemini-1.5-flash-001 model.\\n\\nFirst, we’ll define our display_cities function as a simple Python method.\\n\\n24\\n\\nPython\\n\\ndef display_cities(cities: list[str], preferences: Optional[str] = None):\\n\\n\"\"\"Provides a list of cities based on the user\\'s search query and preferences.\\n\\nArgs:\\n\\npreferences (str): The user\\'s preferences for the search, like skiing, beach, restaurants, bbq, etc. cities (list[str]): The list of cities being recommended to the user.\\n\\nReturns:\\n\\nlist[str]: The list of cities being recommended to the user.\\n\\n\"\"\"\\n\\nreturn cities\\n\\nSnippet 6. Sample python method for a function that will display a list of cities.\\n\\nNext, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to\\n\\nthe model. Executing the code below would result in the output as seen at the bottom of the\\n\\ncode snippet.\\n\\n25\\n\\nPython\\n\\nfrom vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration\\n\\nmodel = GenerativeModel(\"gemini-1.5-flash-001\")\\n\\ndisplay_cities_function = FunctionDeclaration.from_func(display_cities) tool = Tool(function_declarations=[display_cities_function])\\n\\nmessage = \"I’d like to take a ski trip with my family but I’m not sure where to go.\"\\n\\nres = model.generate_content(message, tools=[tool])\\n\\nprint(f\"Function Name: {res.candidates[0].content.parts[0].function_call.name}\") print(f\"Function Args: {res.candidates[0].content.parts[0].function_call.args}\")\\n\\n> Function Name: display_cities > Function Args: {\\'preferences\\': \\'skiing\\', \\'cities\\': [\\'Aspen\\', \\'Vail\\', \\'Park City\\']}\\n\\nSnippet 7. Building a Tool, sending to the model with a user query and allowing the function call to take place\\n\\nIn summary, functions offer a straightforward framework that empowers application\\n\\ndevelopers with fine-grained control over data flow and system execution, while effectively\\n\\nleveraging the agent/model for critical input generation. Developers can selectively choose\\n\\nwhether to keep the agent “in the loop” by returning external data, or omit it based on\\n\\nspecific application architecture requirements.\\n\\n26\\n\\nData stores\\n\\nImagine a language model as a vast library of books, containing its training data. But unlike\\n\\na library that continuously acquires new volumes, this one remains static, holding only the\\n\\nknowledge it was initially trained on. This presents a challenge, as real-world knowledge is\\n\\nconstantly evolving. Data Stores address this limitation by providing access to more dynamic\\n\\nand up-to-date information, and ensuring a model’s responses remain grounded in factuality\\n\\nand relevance.\\n\\nConsider a common scenario where a developer might need to provide a small amount of\\n\\nadditional data to a model, perhaps in the form of spreadsheets or PDFs.\\n\\nFigure 10. How can Agents interact with structured and unstructured data?\\n\\n27\\n\\nData Stores allow developers to provide additional data in its original format to an agent,\\n\\neliminating the need for time-consuming data transformations, model retraining, or fine-\\n\\ntuning. The Data Store converts the incoming document into a set of vector database\\n\\nembeddings that the agent can use to extract the information it needs to supplement its next\\n\\naction or response to the user.\\n\\nFigure 11. Data Stores connect Agents to new real-time data sources of various types.\\n\\nImplementation and application\\n\\nIn the context of Generative AI agents, Data Stores are typically implemented as a vector\\n\\ndatabase that the developer wants the agent to have access to at runtime. While we won’t\\n\\ncover vector databases in depth here, the key point to understand is that they store data\\n\\nin the form of vector embeddings, a type of high-dimensional vector or mathematical\\n\\nrepresentation of the data provided. One of the most prolific examples of Data Store usage\\n\\nwith language models in recent times has been the implementation of Retrieval Augmented\\n\\n28\\n\\nGeneration (RAG) based applications. These applications seek to extend the breadth and\\n\\ndepth of a model’s knowledge beyond the foundational training data by giving the model\\n\\naccess to data in various formats like:\\n\\nWebsite content\\n\\nStructured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n\\nUnstructured Data in formats like HTML, PDF, TXT, etc.\\n\\nFigure 12. 1-to-many relationship between agents and data stores, which can represent various types of\\n\\npre-indexed data\\n\\nThe underlying process for each user request and agent response loop is generally modeled\\n\\nas seen in Figure 13.\\n\\n1. A user query is sent to an embedding model to generate embeddings for the query\\n\\n2. The query embeddings are then matched against the contents of the vector database\\n\\nusing a matching algorithm like SCaNN\\n\\n3. The matched content is retrieved from the vector database in text format and sent back to\\n\\nthe agent\\n\\n4. The agent receives both the user query and retrieved content, then formulates a response\\n\\nor action\\n\\n29\\n\\n5. A final response is sent to the user\\n\\nFigure 13. The lifecycle of a user request and agent response in a RAG based application\\n\\nThe end result is an application that allows the agent to match a user’s query to a known data\\n\\nstore through vector search, retrieve the original content, and provide it to the orchestration\\n\\nlayer and model for further processing. The next action might be to provide a final answer to\\n\\nthe user, or perform an additional vector search to further refine the results.\\n\\nA sample interaction with an agent that implements RAG with ReAct reasoning/planning can\\n\\nbe seen in Figure 14.\\n\\n30\\n\\nFigure 14. Sample RAG based application w/ ReAct reasoning/planning\\n\\n31\\n\\nTools recap\\n\\nTo summarize, extensions, functions and data stores make up a few different tool types\\n\\navailable for agents to use at runtime. Each has their own purpose and they can be used\\n\\ntogether or independently at the discretion of the agent developer.\\n\\nExtensions\\n\\nFunction Calling\\n\\nData Stores\\n\\nExecution\\n\\nAgent-Side Execution\\n\\nClient-Side Execution\\n\\nAgent-Side Execution\\n\\nUse Case\\n\\nDeveloper wants agent to control interactions with the API endpoints\\n\\nSecurity or\\n\\nAuthentication restrictions prevent the agent from calling an API directly\\n\\nDeveloper wants to implement Retrieval Augmented Generation (RAG) with any of the following data types:\\n\\nUseful when\\n\\nleveraging native pre- built Extensions (i.e. Vertex Search, Code Interpreter, etc.)\\n\\nMulti-hop planning and API calling (i.e. the next agent action depends on the outputs of the previous action / API call)\\n\\nTiming constraints or order-of-operations constraints that prevent the agent from making API calls in real-time. (i.e. batch operations, human-in- the-loop review, etc.)\\n\\nAPI that is not exposed\\n\\nto the internet, or non-accessible by Google systems\\n\\nWebsite Content from pre-indexed domains and URLs\\n\\nStructured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.\\n\\nRelational / Non-\\n\\nRelational Databases\\n\\nUnstructured Data in\\n\\nformats like HTML, PDF, TXT, etc.\\n\\n32\\n\\nEnhancing model performance with targeted learning\\n\\nA crucial aspect of using models effectively is their ability to choose the right tools when\\n\\ngenerating output, especially when using tools at scale in production. While general training\\n\\nhelps models develop this skill, real-world scenarios often require knowledge beyond the\\n\\ntraining data. Imagine this as the difference between basic cooking skills and mastering\\n\\na specific cuisine. Both require foundational cooking knowledge, but the latter demands\\n\\ntargeted learning for more nuanced results.\\n\\nTo help the model gain access to this type of specific knowledge, several approaches exist:\\n\\n\\n\\nIn-context learning: This method provides a generalized model with a prompt, tools, and\\n\\nfew-shot examples at inference time which allows it to learn ‘on the fly\\' how and when to\\n\\nuse those tools for a specific task. The ReAct framework is an example of this approach in\\n\\nnatural language.\\n\\nRetrieval-based in-context learning: This technique dynamically populates the model\\n\\nprompt with the most relevant information, tools, and associated examples by retrieving\\n\\nthem from external memory. An example of this would be the ‘Example Store’ in Vertex AI\\n\\nextensions or the data stores RAG based architecture mentioned previously.\\n\\nFine-tuning based learning: This method involves training a model using a larger dataset\\n\\nof specific examples prior to inference. This helps the model understand when and how to\\n\\napply certain tools prior to receiving any user queries.\\n\\nTo provide additional insights on each of the targeted learning approaches, let’s revisit our\\n\\ncooking analogy.\\n\\n33\\n\\n\\n\\nImagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant\\n\\ntools) and some example dishes (few-shot examples) from a customer. Based on this\\n\\nlimited information and the chef’s general knowledge of cooking, they will need to figure\\n\\nout how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the\\n\\ncustomer’s preferences. This is in-context learning.\\n\\nNow let’s imagine our chef in a kitchen that has a well-stocked pantry (external data\\n\\nstores) filled with various ingredients and cookbooks (examples and tools). The chef is now\\n\\nable to dynamically choose ingredients and cookbooks from the pantry and better align\\n\\nto the customer’s recipe and preferences. This allows the chef to create a more informed\\n\\nand refined dish leveraging both existing and new knowledge. This is retrieval-based\\n\\nin-context learning.\\n\\nFinally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of\\n\\ncuisines (pre-training on a larger dataset of specific examples). This allows the chef to\\n\\napproach future unseen customer recipes with deeper understanding. This approach is\\n\\nperfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine-\\n\\ntuning based learning.\\n\\nEach of these approaches offers unique advantages and disadvantages in terms of speed,\\n\\ncost, and latency. However, by combining these techniques in an agent framework, we can\\n\\nleverage the various strengths and minimize their weaknesses, allowing for a more robust and\\n\\nadaptable solution.\\n\\n34\\n\\nAgent quick start with LangChain\\n\\nIn order to provide a real-world executable example of an agent in action, we’ll build a quick\\n\\nprototype with the LangChain and LangGraph libraries. These popular open source libraries\\n\\nallow users to build customer agents by “chaining” together sequences of logic, reasoning, and tool calls to answer a user’s query. We’ll use our gemini-1.5-flash-001 model and\\n\\nsome simple tools to answer a multi-stage query from the user as seen in Snippet 8.\\n\\nThe tools we are using are the SerpAPI (for Google Search) and the Google Places API. After\\n\\nexecuting our program in Snippet 8, you can see the sample output in Snippet 9.\\n\\n35\\n\\nPython\\n\\nfrom langgraph.prebuilt import create_react_agent from langchain_core.tools import tool from langchain_community.utilities import SerpAPIWrapper from langchain_community.tools import GooglePlacesTool\\n\\nos.environ[\"SERPAPI_API_KEY\"] = \"XXXXX\" os.environ[\"GPLACES_API_KEY\"] = \"XXXXX\"\\n\\n@tool def search(query: str):\\n\\n\"\"\"Use the SerpAPI to run a Google Search.\"\"\" search = SerpAPIWrapper() return search.run(query)\\n\\n@tool def places(query: str):\\n\\n\"\"\"Use the Google Places API to run a Google Places Query.\"\"\" places = GooglePlacesTool() return places.run(query)\\n\\nmodel = ChatVertexAI(model=\"gemini-1.5-flash-001\") tools = [search, places]\\n\\nquery = \"Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium?\"\\n\\nagent = create_react_agent(model, tools) input = {\"messages\": [(\"human\", query)]}\\n\\nfor s in agent.stream(input, stream_mode=\"values\"):\\n\\nmessage = s[\"messages\"][-1] if isinstance(message, tuple):\\n\\nprint(message)\\n\\nelse:\\n\\nmessage.pretty_print()\\n\\nSnippet 8. Sample LangChain and LangGraph based agent with tools\\n\\n36\\n\\nUnset\\n\\n=============================== Human Message ================================ Who did the Texas Longhorns play in football last week? What is the address of the other team\\'s stadium? ================================= Ai Message ================================= Tool Calls: search Args:\\n\\nquery: Texas Longhorns football schedule\\n\\n================================ Tool Message ================================ Name: search {...Results: \"NCAA Division I Football, Georgia, Date...\"} ================================= Ai Message ================================= The Texas Longhorns played the Georgia Bulldogs last week. Tool Calls: places Args:\\n\\nquery: Georgia Bulldogs stadium\\n\\n================================ Tool Message ================================ Name: places\\n\\n{...Sanford Stadium Address: 100 Sanford...} ================================= Ai Message ================================= The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 30602, USA.\\n\\nSnippet 9. Output from our program in Snippet 8\\n\\nWhile this is a fairly simple agent example, it demonstrates the foundational components\\n\\nof Model, Orchestration, and tools all working together to achieve a specific goal. In the\\n\\nfinal section, we’ll explore how these components come together in Google-scale managed\\n\\nproducts like Vertex AI agents and Generative Playbooks.\\n\\n37\\n\\nProduction applications with Vertex AI agents\\n\\nWhile this whitepaper explored the core components of agents, building production-grade\\n\\napplications requires integrating them with additional tools like user interfaces, evaluation\\n\\nframeworks, and continuous improvement mechanisms. Google’s Vertex AI platform\\n\\nsimplifies this process by offering a fully managed environment with all the fundamental\\n\\nelements covered earlier. Using a natural language interface, developers can rapidly\\n\\ndefine crucial elements of their agents - goals, task instructions, tools, sub-agents for task\\n\\ndelegation, and examples - to easily construct the desired system behavior. In addition, the\\n\\nplatform comes with a set of development tools that allow for testing, evaluation, measuring\\n\\nagent performance, debugging, and improving the overall quality of developed agents. This\\n\\nallows developers to focus on building and refining their agents while the complexities of\\n\\ninfrastructure, deployment and maintenance are managed by the platform itself.\\n\\nIn Figure 15 we’ve provided a sample architecture of an agent that was built on the Vertex\\n\\nAI platform using various features such as Vertex Agent Builder, Vertex Extensions, Vertex\\n\\nFunction Calling and Vertex Example Store to name a few. The architecture includes many of\\n\\nthe various components necessary for a production ready application.\\n\\n38\\n\\nFigure 15. Sample end-to-end agent architecture built on Vertex AI platform\\n\\nYou can try a sample of this prebuilt agent architecture from our official documentation.\\n\\n39\\n\\nSummary\\n\\nIn this whitepaper we’ve discussed the foundational building blocks of Generative AI\\n\\nagents, their compositions, and effective ways to implement them in the form of cognitive\\n\\narchitectures. Some key takeaways from this whitepaper include:\\n\\n1. Agents extend the capabilities of language models by leveraging tools to access real-\\n\\ntime information, suggest real-world actions, and plan and execute complex tasks\\n\\nautonomously. agents can leverage one or more language models to decide when and\\n\\nhow to transition through states and use external tools to complete any number of\\n\\ncomplex tasks that would be difficult or impossible for the model to complete on its own.\\n\\n2. At the heart of an agent’s operation is the orchestration layer, a cognitive architecture that\\n\\nstructures reasoning, planning, decision-making and guides its actions. Various reasoning\\n\\ntechniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework\\n\\nfor the orchestration layer to take in information, perform internal reasoning, and generate\\n\\ninformed decisions or responses.\\n\\n3. Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside\\n\\nworld for agents, allowing them to interact with external systems and access knowledge\\n\\nbeyond their training data. Extensions provide a bridge between agents and external APIs,\\n\\nenabling the execution of API calls and retrieval of real-time information. functions provide\\n\\na more nuanced control for the developer through the division of labor, allowing agents\\n\\nto generate Function parameters which can be executed client-side. Data Stores provide\\n\\nagents with access to structured or unstructured data, enabling data-driven applications.\\n\\nThe future of agents holds exciting advancements and we’ve only begun to scratch the\\n\\nsurface of what is possible. As tools become more sophisticated and reasoning capabilities\\n\\nare enhanced, agents will be empowered to solve increasingly complex problems.\\n\\nFurthermore, the strategic approach of ‘agent chaining’ will continue to gain momentum. By\\n\\n40\\n\\ncombining specialized agents - each excelling in a particular domain or task - we can create\\n\\na ‘mixture of agent experts’ approach, capable of delivering exceptional results across\\n\\nvarious industries and problem areas.\\n\\nIt’s important to remember that building complex agent architectures demands an iterative\\n\\napproach. Experimentation and refinement are key to finding solutions for specific business\\n\\ncases and organizational needs. No two agents are created alike due to the generative nature\\n\\nof the foundational models that underpin their architecture. However, by harnessing the\\n\\nstrengths of each of these foundational components, we can create impactful applications\\n\\nthat extend the capabilities of language models and drive real-world value.\\n\\n41\\n\\nEndnotes\\n\\n1. Shafran, I., Cao, Y. et al., 2022, \\'ReAct: Synergizing Reasoning and Acting in Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2210.03629\\n\\n2. Wei, J., Wang, X. et al., 2023, \\'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\\'.\\n\\nAvailable at: https://arxiv.org/pdf/2201.11903.pdf.\\n\\n3. Wang, X. et al., 2022, \\'Self-Consistency Improves Chain of Thought Reasoning in Language Models\\'.\\n\\nAvailable at: https://arxiv.org/abs/2203.11171.\\n\\n4. Diao, S. et al., 2023, \\'Active Prompting with Chain-of-Thought for Large Language Models\\'. Available at:\\n\\nhttps://arxiv.org/pdf/2302.12246.pdf.\\n\\n5. Zhang, H. et al., 2023, \\'Multimodal Chain-of-Thought Reasoning in Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2302.00923.\\n\\n6. Yao, S. et al., 2023, \\'Tree of Thoughts: Deliberate Problem Solving with Large Language Models\\'. Available at:\\n\\nhttps://arxiv.org/abs/2305.10601.\\n\\n7. Long, X., 2023, \\'Large Language Model Guided Tree-of-Thought\\'. Available at:\\n\\nhttps://arxiv.org/abs/2305.08291.\\n\\n8. Google. \\'Google Gemini Application\\'. Available at: http://gemini.google.com.\\n\\n9. Swagger. \\'OpenAPI Specification\\'. Available at: https://swagger.io/specification/.\\n\\n10. Xie, M., 2022, \\'How does in-context learning work? A framework for understanding the differences from\\n\\ntraditional supervised learning\\'. Available at: https://ai.stanford.edu/blog/understanding-incontext/.\\n\\n11. Google Research. \\'ScaNN (Scalable Nearest Neighbors)\\'. Available at:\\n\\nhttps://github.com/google-research/google-research/tree/master/scann.\\n\\n12. LangChain. \\'LangChain\\'. Available at: https://python.langchain.com/v0.2/docs/introduction/.\\n\\n42')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3c4934",
      "metadata": {},
      "source": [
        "## Dataset Generation\n",
        "We'll create datasets using ChatOpenAI. Before writing the code, let's define the roles of our objects:\n",
        "- Dataset Generator: `generator_llm`\n",
        "- Dataset Critic: `critic_llm`\n",
        "- Document Embeddings: `embeddings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2e208263",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from ragas.testset.transforms import KeyphrasesExtractor\n",
        "from ragas.testset.graph import KnowledgeGraph\n",
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "\n",
        "# Dataset Generator\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "# Dataset Critic\n",
        "critic_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "# Document Embeddings\n",
        "embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb8585c",
      "metadata": {},
      "source": [
        "First, let's initialize the DocumentStore. We'll configure it to use custom LLM and embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "42764448",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure the text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "\n",
        "# Wrap LangChain's ChatOpenAI model with LangchainLLMWrapper to make it compatible with Ragas\n",
        "langchain_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
        "\n",
        "# Initialize the key phrase extractor using the LLM defined above\n",
        "keyphrase_extractor = KeyphrasesExtractor(llm=langchain_llm)\n",
        "\n",
        "# Create ragas_embeddings\n",
        "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "for doc in docs:\n",
        "   kg.nodes.append(\n",
        "       Node(\n",
        "           type=NodeType.DOCUMENT,\n",
        "           properties={\n",
        "               \"page_content\": doc.page_content,\n",
        "               \"document_metadata\": doc.metadata\n",
        "           }\n",
        "       )\n",
        "   )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5323799",
      "metadata": {},
      "source": [
        "### Self Check\n",
        "\n",
        "```python\n",
        "print(len(generator.knowledge_graph.nodes))\n",
        "```\n",
        "Run this code to verify if knowledge graph nodes have been created. If no nodes were created, there may be issues with executing subsequent code.\n",
        "\n",
        "```python\n",
        "for node in generator.knowledge_graph.nodes:\n",
        "    print(node.properties)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "606dec2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(\n",
        "    llm=generator_llm,\n",
        "    embedding_model=ragas_embeddings,\n",
        "    knowledge_graph=kg,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f360948",
      "metadata": {},
      "source": [
        "## Distribution of Question Types\n",
        "Before we begin generating questions, let's first define the distribution (frequency) of questions by type. Using the `SingleHopSpecificQuerySynthesizer`, we aim to create a test set with the following distribution of question types:\n",
        "\n",
        "- `simple`: Basic questions (40%)\n",
        "- `reasoning`: Questions requiring reasoning (20%)\n",
        "- `multi_context`: Questions requiring consideration of multiple contexts (20%)\n",
        "- `conditional`: Conditional questions (20%)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e663ea25",
      "metadata": {},
      "source": [
        "### Role of the synthesizers Module\n",
        "The synthesizers module in Ragas is a core module responsible for Query Synthesis. It provides functionality to generate various types of questions based on documents stored in the Knowledge Graph. This module is used to automatically generate test sets for evaluating RAG (Retrieval-Augmented Generation) systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "45b70a8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers.single_hop.specific import (\n",
        "   SingleHopSpecificQuerySynthesizer,\n",
        ")\n",
        "from ragas.testset.synthesizers.multi_hop.specific import (MultiHopQuerySynthesizer)\n",
        "\n",
        "# Create Synthesizer instances for each question type\n",
        "simple_synthesizer = SingleHopSpecificQuerySynthesizer(llm=generator_llm)  \n",
        "reasoning_synthesizer = SingleHopSpecificQuerySynthesizer(llm=generator_llm)\n",
        "multi_context_synthesizer = SingleHopSpecificQuerySynthesizer(llm=generator_llm)\n",
        "conditional_synthesizer = SingleHopSpecificQuerySynthesizer(llm=generator_llm)\n",
        "\n",
        "# Set distribution by question type\n",
        "distribution = [\n",
        "   (simple_synthesizer, 0.4),        # simple: 40%\n",
        "   (reasoning_synthesizer, 0.2),     # reasoning: 20%  \n",
        "   (multi_context_synthesizer, 0.2), # multi_context: 20%\n",
        "   (conditional_synthesizer, 0.2),   # conditional: 20%\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bd797b14",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f4116132fd34f94882262d1ccacd994",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06d1755d570b450ba4e0c1c290c36f42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1b2e4ad6f1441ddb1589729e16bc0a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fe0f5dd1bfe45da93d8176623df5247",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a16ef6421bba44079118311718005c6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a45e97bc419f45e1b04ec25813d0b26e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67cbbcc48b0340f4afdc12d0f6f44545",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "976551e539324d60a5513ac5f9b77663",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20ab3526bf3f4ee9b82e2046e1de473e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = generator.generate_with_langchain_docs(\n",
        "   documents=docs, # document data\n",
        "   testset_size=10, # number of questions to generate\n",
        "   query_distribution=distribution, # distribution by question type \n",
        "   with_debugging_logs=True # output debugging logs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2e06a5c6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what vertex ai do?</td>\n",
              "      <td>[Cognitive architectures: How agents operate T...</td>\n",
              "      <td>Vertex AI is mentioned in the context of produ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the ReAct framework utilize the 'Flig...</td>\n",
              "      <td>[their training data. Knowledge is extended th...</td>\n",
              "      <td>The ReAct framework allows AI agents to choose...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Howw doo I book a flight to Zurich using an AI...</td>\n",
              "      <td>[data. This means that, in a sense, a language...</td>\n",
              "      <td>To book a flight to Zurich using an AI agent, ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wht is SCaNN used for in AI devlopment?</td>\n",
              "      <td>[that they are meant to offer the developer mu...</td>\n",
              "      <td>SCaNN is used as a matching algorithm to match...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what langchain do?</td>\n",
              "      <td>[Cognitive architectures: How agents operate T...</td>\n",
              "      <td>LangChain is mentioned in the context of agent...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what langchain do?</td>\n",
              "      <td>[their training data. Knowledge is extended th...</td>\n",
              "      <td>LangChain is a pre-built agent framework used ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How does Vertex AI facilitate the deployment o...</td>\n",
              "      <td>[Cognitive architectures: How agents operate T...</td>\n",
              "      <td>Vertex AI agents are utilized in production ap...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Wht is LangChain and how does it fit into agen...</td>\n",
              "      <td>[their training data. Knowledge is extended th...</td>\n",
              "      <td>LangChain is a pre-built agent framework that ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>what vertex ai do?</td>\n",
              "      <td>[Cognitive architectures: How agents operate T...</td>\n",
              "      <td>The context mentions 'Production applications ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>what is ReAct do?</td>\n",
              "      <td>[their training data. Knowledge is extended th...</td>\n",
              "      <td>ReAct is a prompt engineering framework that p...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0                                 what vertex ai do?   \n",
              "1  How does the ReAct framework utilize the 'Flig...   \n",
              "2  Howw doo I book a flight to Zurich using an AI...   \n",
              "3            Wht is SCaNN used for in AI devlopment?   \n",
              "4                                 what langchain do?   \n",
              "5                                 what langchain do?   \n",
              "6  How does Vertex AI facilitate the deployment o...   \n",
              "7  Wht is LangChain and how does it fit into agen...   \n",
              "8                                 what vertex ai do?   \n",
              "9                                  what is ReAct do?   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [Cognitive architectures: How agents operate T...   \n",
              "1  [their training data. Knowledge is extended th...   \n",
              "2  [data. This means that, in a sense, a language...   \n",
              "3  [that they are meant to offer the developer mu...   \n",
              "4  [Cognitive architectures: How agents operate T...   \n",
              "5  [their training data. Knowledge is extended th...   \n",
              "6  [Cognitive architectures: How agents operate T...   \n",
              "7  [their training data. Knowledge is extended th...   \n",
              "8  [Cognitive architectures: How agents operate T...   \n",
              "9  [their training data. Knowledge is extended th...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Vertex AI is mentioned in the context of produ...   \n",
              "1  The ReAct framework allows AI agents to choose...   \n",
              "2  To book a flight to Zurich using an AI agent, ...   \n",
              "3  SCaNN is used as a matching algorithm to match...   \n",
              "4  LangChain is mentioned in the context of agent...   \n",
              "5  LangChain is a pre-built agent framework used ...   \n",
              "6  Vertex AI agents are utilized in production ap...   \n",
              "7  LangChain is a pre-built agent framework that ...   \n",
              "8  The context mentions 'Production applications ...   \n",
              "9  ReAct is a prompt engineering framework that p...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  \n",
              "5  single_hop_specifc_query_synthesizer  \n",
              "6  single_hop_specifc_query_synthesizer  \n",
              "7  single_hop_specifc_query_synthesizer  \n",
              "8  single_hop_specifc_query_synthesizer  \n",
              "9  single_hop_specifc_query_synthesizer  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
